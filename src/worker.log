2024-10-24 16:25:21,556 - main - INFO - Starting application initialization
2024-10-24 16:25:21,557 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:25:21,564 - db - INFO - Database initialization completed.
2024-10-24 16:25:21,575 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010771 seconds, for an average of 0.0004487916666666666 seconds per hash.
2024-10-24 16:25:21,575 - functions - INFO - Checking models directory...
2024-10-24 16:25:21,575 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:25:21,575 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:25:21,576 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:25:21,576 - functions - INFO - Model downloads completed.
2024-10-24 16:25:21,582 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:25:21,583 - main - INFO - Application initialization complete
2024-10-24 16:25:46,165 - main - INFO - Attempting to enqueue job with ID: 48859dea68188bf2388c35dd6c6d6d4d
2024-10-24 16:25:46,169 - main - ERROR - Failed to queue model download: cannot pickle '_thread.lock' object
2024-10-24 16:25:46,171 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 263, in add_new_model
    job = model_worker.model_queue.enqueue(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 971, in enqueue
    return self.enqueue_call(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 719, in enqueue_call
    return self.enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1094, in enqueue_job
    return self._enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1122, in _enqueue_job
    job.save(pipeline=pipe)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1087, in save
    mapping = self.to_dict(include_meta=include_meta, include_result=include_result)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1011, in to_dict
    'data': zlib.compress(self.data),
                          ^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 506, in data
    self._data = self.serializer.dumps(job_tuple)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot pickle '_thread.lock' object

2024-10-24 16:25:57,625 - main - INFO - Attempting to enqueue job with ID: 7e574f764bd7b0e8b9cfd0efff4397aa
2024-10-24 16:25:57,628 - main - ERROR - Failed to queue model download: cannot pickle '_thread.lock' object
2024-10-24 16:25:57,629 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 263, in add_new_model
    job = model_worker.model_queue.enqueue(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 971, in enqueue
    return self.enqueue_call(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 719, in enqueue_call
    return self.enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1094, in enqueue_job
    return self._enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1122, in _enqueue_job
    job.save(pipeline=pipe)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1087, in save
    mapping = self.to_dict(include_meta=include_meta, include_result=include_result)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1011, in to_dict
    'data': zlib.compress(self.data),
                          ^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 506, in data
    self._data = self.serializer.dumps(job_tuple)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot pickle '_thread.lock' object

2024-10-24 16:26:05,787 - main - INFO - Application shutdown initiated
2024-10-24 16:26:15,112 - main - INFO - Starting application initialization
2024-10-24 16:26:15,113 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:26:15,117 - db - INFO - Database initialization completed.
2024-10-24 16:26:15,127 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010404 seconds, for an average of 0.0004335 seconds per hash.
2024-10-24 16:26:15,127 - functions - INFO - Checking models directory...
2024-10-24 16:26:15,128 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:26:15,128 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:26:15,128 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:26:15,128 - functions - INFO - Model downloads completed.
2024-10-24 16:26:15,134 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:26:15,135 - main - INFO - Application initialization complete
2024-10-24 16:26:20,161 - main - INFO - Attempting to enqueue job with ID: 131118d4a472497e16556a232d216976
2024-10-24 16:26:20,168 - main - ERROR - Failed to queue model download: cannot pickle '_thread.lock' object
2024-10-24 16:26:20,172 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 263, in add_new_model
    job = model_worker.model_queue.enqueue(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 971, in enqueue
    return self.enqueue_call(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 719, in enqueue_call
    return self.enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1094, in enqueue_job
    return self._enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1122, in _enqueue_job
    job.save(pipeline=pipe)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1087, in save
    mapping = self.to_dict(include_meta=include_meta, include_result=include_result)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1011, in to_dict
    'data': zlib.compress(self.data),
                          ^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 506, in data
    self._data = self.serializer.dumps(job_tuple)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot pickle '_thread.lock' object

2024-10-24 16:26:29,324 - main - INFO - Attempting to enqueue job with ID: 62470906e6e227de8dbdacdf500d8509
2024-10-24 16:26:29,331 - main - ERROR - Failed to queue model download: cannot pickle '_thread.lock' object
2024-10-24 16:26:29,332 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 263, in add_new_model
    job = model_worker.model_queue.enqueue(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 971, in enqueue
    return self.enqueue_call(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 719, in enqueue_call
    return self.enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1094, in enqueue_job
    return self._enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1122, in _enqueue_job
    job.save(pipeline=pipe)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1087, in save
    mapping = self.to_dict(include_meta=include_meta, include_result=include_result)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1011, in to_dict
    'data': zlib.compress(self.data),
                          ^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 506, in data
    self._data = self.serializer.dumps(job_tuple)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot pickle '_thread.lock' object

2024-10-24 16:27:03,273 - main - INFO - Application shutdown initiated
2024-10-24 16:27:04,968 - main - INFO - Starting application initialization
2024-10-24 16:27:04,969 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:27:04,974 - db - INFO - Database initialization completed.
2024-10-24 16:27:04,985 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010167 seconds, for an average of 0.000423625 seconds per hash.
2024-10-24 16:27:04,985 - functions - INFO - Checking models directory...
2024-10-24 16:27:04,985 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:27:04,985 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:27:04,986 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:27:04,986 - functions - INFO - Model downloads completed.
2024-10-24 16:27:04,992 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:27:04,993 - main - INFO - Application initialization complete
2024-10-24 16:27:13,069 - main - INFO - Attempting to enqueue job with ID: 885cc1e60cd0ac731e545e2b1aee5f70
2024-10-24 16:27:13,075 - main - INFO - Job enqueued successfully. Job ID: 885cc1e60cd0ac731e545e2b1aee5f70
2024-10-24 16:28:56,712 - main - INFO - Application shutdown initiated
2024-10-24 16:28:58,607 - main - INFO - Starting application initialization
2024-10-24 16:28:58,608 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:28:58,613 - db - INFO - Database initialization completed.
2024-10-24 16:28:58,624 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010403 seconds, for an average of 0.0004334583333333334 seconds per hash.
2024-10-24 16:28:58,624 - functions - INFO - Checking models directory...
2024-10-24 16:28:58,624 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:28:58,624 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:28:58,624 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:28:58,624 - functions - INFO - Model downloads completed.
2024-10-24 16:28:58,630 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:28:58,632 - main - INFO - Application initialization complete
2024-10-24 16:29:10,812 - main - INFO - Application shutdown initiated
2024-10-24 16:29:13,210 - main - INFO - Starting application initialization
2024-10-24 16:29:13,211 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:29:13,217 - db - INFO - Database initialization completed.
2024-10-24 16:29:13,230 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.013057 seconds, for an average of 0.0005440416666666666 seconds per hash.
2024-10-24 16:29:13,231 - functions - INFO - Checking models directory...
2024-10-24 16:29:13,231 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:29:13,231 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:29:13,231 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:29:13,231 - functions - INFO - Model downloads completed.
2024-10-24 16:29:13,239 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:29:13,240 - main - INFO - Application initialization complete
2024-10-24 16:29:43,239 - main - INFO - Attempting to enqueue job with ID: bc8961503bcf29d383a1a91d8326ae95
2024-10-24 16:29:43,255 - main - INFO - Job enqueued successfully. Job ID: bc8961503bcf29d383a1a91d8326ae95
2024-10-24 16:29:43,255 - main - ERROR - Failed to queue model download: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte
2024-10-24 16:29:43,260 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 279, in add_new_model
    enqueued_job = Job.fetch(unique_id, connection=model_worker.redis_conn)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 591, in fetch
    job.refresh()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 991, in refresh
    data = self.connection.hgetall(self.key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 4978, in hgetall
    return self.execute_command("HGETALL", name, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 567, in _execute_command
    return conn.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 568, in <lambda>
    lambda: self._send_command_parse_response(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 542, in _send_command_parse_response
    return self.parse_response(conn, command_name, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 584, in parse_response
    response = connection.read_response()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 60, in _read_response
    self._read_response(disable_decoding=disable_decoding)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 67, in _read_response
    response = self.encoder.decode(response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/encoders.py", line 43, in decode
    value = value.decode(self.encoding, self.encoding_errors)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte

2024-10-24 16:30:06,621 - main - INFO - Attempting to enqueue job with ID: 388718df391cc6200f22a036abb52ca5
2024-10-24 16:30:06,633 - main - INFO - Job enqueued successfully. Job ID: 388718df391cc6200f22a036abb52ca5
2024-10-24 16:30:06,633 - main - ERROR - Failed to queue model download: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte
2024-10-24 16:30:06,634 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 279, in add_new_model
    enqueued_job = Job.fetch(unique_id, connection=model_worker.redis_conn)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 591, in fetch
    job.refresh()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 991, in refresh
    data = self.connection.hgetall(self.key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 4978, in hgetall
    return self.execute_command("HGETALL", name, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 567, in _execute_command
    return conn.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 568, in <lambda>
    lambda: self._send_command_parse_response(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 542, in _send_command_parse_response
    return self.parse_response(conn, command_name, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 584, in parse_response
    response = connection.read_response()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 60, in _read_response
    self._read_response(disable_decoding=disable_decoding)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 67, in _read_response
    response = self.encoder.decode(response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/encoders.py", line 43, in decode
    value = value.decode(self.encoding, self.encoding_errors)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte

2024-10-24 16:30:44,301 - main - INFO - Application shutdown initiated
2024-10-24 16:30:46,040 - main - INFO - Starting application initialization
2024-10-24 16:30:46,043 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:30:46,048 - db - INFO - Database initialization completed.
2024-10-24 16:30:46,058 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010432 seconds, for an average of 0.0004346666666666667 seconds per hash.
2024-10-24 16:30:46,059 - functions - INFO - Checking models directory...
2024-10-24 16:30:46,059 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:30:46,059 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:30:46,059 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:30:46,059 - functions - INFO - Model downloads completed.
2024-10-24 16:30:46,065 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:30:46,067 - main - INFO - Application initialization complete
2024-10-24 16:30:57,628 - main - INFO - Attempting to enqueue job with ID: c7450c34e6f57d55709e163b9d2e4406
2024-10-24 16:30:57,638 - main - INFO - Job enqueued successfully. Job ID: c7450c34e6f57d55709e163b9d2e4406
2024-10-24 16:30:57,639 - main - ERROR - Failed to fetch job: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte
2024-10-24 16:30:57,639 - main - ERROR - Failed to queue model download: 500: Failed to fetch job after enqueueing: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte
2024-10-24 16:30:57,643 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 280, in add_new_model
    enqueued_job = Job.fetch(unique_id, connection=model_worker.redis_conn)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 591, in fetch
    job.refresh()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 991, in refresh
    data = self.connection.hgetall(self.key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 4978, in hgetall
    return self.execute_command("HGETALL", name, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 567, in _execute_command
    return conn.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 568, in <lambda>
    lambda: self._send_command_parse_response(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 542, in _send_command_parse_response
    return self.parse_response(conn, command_name, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 584, in parse_response
    response = connection.read_response()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 60, in _read_response
    self._read_response(disable_decoding=disable_decoding)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 67, in _read_response
    response = self.encoder.decode(response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/encoders.py", line 43, in decode
    value = value.decode(self.encoding, self.encoding_errors)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 283, in add_new_model
    raise HTTPException(status_code=500, detail=f"Failed to fetch job after enqueueing: {str(e)}")
fastapi.exceptions.HTTPException: 500: Failed to fetch job after enqueueing: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte

2024-10-24 16:31:10,895 - main - INFO - Application shutdown initiated
2024-10-24 16:31:12,972 - main - INFO - Starting application initialization
2024-10-24 16:31:12,974 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:31:12,979 - db - INFO - Database initialization completed.
2024-10-24 16:31:12,989 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009794 seconds, for an average of 0.0004080833333333334 seconds per hash.
2024-10-24 16:31:12,989 - functions - INFO - Checking models directory...
2024-10-24 16:31:12,989 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:31:12,990 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:31:12,990 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:31:12,990 - functions - INFO - Model downloads completed.
2024-10-24 16:31:12,996 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:31:12,997 - main - INFO - Application initialization complete
2024-10-24 16:31:20,374 - main - INFO - Attempting to enqueue job with ID: 21ef9bb6d48258cb84ae173b7be9ce67
2024-10-24 16:31:20,383 - main - INFO - Job enqueued successfully. Job ID: 21ef9bb6d48258cb84ae173b7be9ce67
2024-10-24 16:31:20,384 - main - ERROR - Failed to fetch job: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte
2024-10-24 16:31:20,384 - main - ERROR - Failed to queue model download: 500: Failed to fetch job after enqueueing: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte
2024-10-24 16:31:20,389 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 280, in add_new_model
    enqueued_job = Job.fetch(unique_id, connection=model_worker.redis_conn)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 591, in fetch
    job.refresh()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 991, in refresh
    data = self.connection.hgetall(self.key)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 4978, in hgetall
    return self.execute_command("HGETALL", name, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 567, in _execute_command
    return conn.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 568, in <lambda>
    lambda: self._send_command_parse_response(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 542, in _send_command_parse_response
    return self.parse_response(conn, command_name, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 584, in parse_response
    response = connection.read_response()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 60, in _read_response
    self._read_response(disable_decoding=disable_decoding)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 67, in _read_response
    response = self.encoder.decode(response)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/encoders.py", line 43, in decode
    value = value.decode(self.encoding, self.encoding_errors)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 283, in add_new_model
    raise HTTPException(status_code=500, detail=f"Failed to fetch job after enqueueing: {str(e)}")
fastapi.exceptions.HTTPException: 500: Failed to fetch job after enqueueing: 'utf-8' codec can't decode byte 0x9c in position 1: invalid start byte

2024-10-24 16:34:08,039 - main - INFO - Application shutdown initiated
2024-10-24 16:34:10,039 - main - INFO - Starting application initialization
2024-10-24 16:34:10,041 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:34:10,046 - db - INFO - Database initialization completed.
2024-10-24 16:34:10,057 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010533 seconds, for an average of 0.00043887500000000003 seconds per hash.
2024-10-24 16:34:10,057 - functions - INFO - Checking models directory...
2024-10-24 16:34:10,057 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:34:10,057 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:34:10,057 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:34:10,057 - functions - INFO - Model downloads completed.
2024-10-24 16:34:10,063 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:34:10,065 - main - INFO - Application initialization complete
2024-10-24 16:34:15,321 - main - INFO - Application shutdown initiated
2024-10-24 16:34:32,138 - main - INFO - Starting application initialization
2024-10-24 16:34:32,140 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:34:32,145 - db - INFO - Database initialization completed.
2024-10-24 16:34:32,157 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011551 seconds, for an average of 0.0004812916666666667 seconds per hash.
2024-10-24 16:34:32,158 - functions - INFO - Checking models directory...
2024-10-24 16:34:32,158 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:34:32,158 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:34:32,158 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:34:32,158 - functions - INFO - Model downloads completed.
2024-10-24 16:34:32,165 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:34:32,167 - main - INFO - Application initialization complete
2024-10-24 16:36:16,089 - main - INFO - Application shutdown initiated
2024-10-24 16:36:18,247 - main - INFO - Starting application initialization
2024-10-24 16:36:18,248 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:36:18,253 - db - INFO - Database initialization completed.
2024-10-24 16:36:18,264 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010401 seconds, for an average of 0.000433375 seconds per hash.
2024-10-24 16:36:18,264 - functions - INFO - Checking models directory...
2024-10-24 16:36:18,264 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:36:18,264 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:36:18,265 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:36:18,265 - functions - INFO - Model downloads completed.
2024-10-24 16:36:18,271 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:36:18,273 - main - INFO - Application initialization complete
2024-10-24 16:36:31,638 - main - INFO - Application shutdown initiated
2024-10-24 16:36:33,294 - main - INFO - Starting application initialization
2024-10-24 16:36:33,300 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:36:33,305 - db - INFO - Database initialization completed.
2024-10-24 16:36:33,314 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009512 seconds, for an average of 0.00039633333333333334 seconds per hash.
2024-10-24 16:36:33,315 - functions - INFO - Checking models directory...
2024-10-24 16:36:33,315 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:36:33,315 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:36:33,315 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:36:33,315 - functions - INFO - Model downloads completed.
2024-10-24 16:36:33,320 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:36:33,321 - main - INFO - Application initialization complete
2024-10-24 16:36:55,328 - main - INFO - Application shutdown initiated
2024-10-24 16:36:57,223 - main - INFO - Starting application initialization
2024-10-24 16:36:57,226 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:36:57,232 - db - INFO - Database initialization completed.
2024-10-24 16:36:57,243 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010698 seconds, for an average of 0.00044574999999999997 seconds per hash.
2024-10-24 16:36:57,243 - functions - INFO - Checking models directory...
2024-10-24 16:36:57,243 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:36:57,243 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:36:57,243 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:36:57,243 - functions - INFO - Model downloads completed.
2024-10-24 16:36:57,250 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:36:57,251 - main - INFO - Application initialization complete
2024-10-24 16:37:10,302 - main - INFO - Application shutdown initiated
2024-10-24 16:37:11,727 - main - INFO - Starting application initialization
2024-10-24 16:37:11,728 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:37:11,733 - db - INFO - Database initialization completed.
2024-10-24 16:37:11,742 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009518 seconds, for an average of 0.00039658333333333337 seconds per hash.
2024-10-24 16:37:11,743 - functions - INFO - Checking models directory...
2024-10-24 16:37:11,743 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:37:11,743 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:37:11,743 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:37:11,743 - functions - INFO - Model downloads completed.
2024-10-24 16:37:11,748 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:37:11,749 - main - INFO - Application initialization complete
2024-10-24 16:37:24,480 - main - INFO - Processing model URL: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-24 16:37:24,483 - main - ERROR - Failed to enqueue job: name 'download_model_task' is not defined
2024-10-24 16:37:24,483 - main - ERROR - Failed to process model download request: 500: Failed to queue model download: name 'download_model_task' is not defined
2024-10-24 16:37:24,488 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 275, in add_new_model
    download_model_task,
    ^^^^^^^^^^^^^^^^^^^
NameError: name 'download_model_task' is not defined. Did you mean: 'download_models'?

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 309, in add_new_model
    raise HTTPException(
fastapi.exceptions.HTTPException: 500: Failed to queue model download: name 'download_model_task' is not defined

2024-10-24 16:38:22,072 - main - INFO - Application shutdown initiated
2024-10-24 16:38:23,963 - main - INFO - Starting application initialization
2024-10-24 16:38:23,964 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-24 16:38:23,969 - db - INFO - Database initialization completed.
2024-10-24 16:38:23,979 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009996 seconds, for an average of 0.0004165 seconds per hash.
2024-10-24 16:38:23,979 - functions - INFO - Checking models directory...
2024-10-24 16:38:23,979 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-24 16:38:23,979 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-24 16:38:23,979 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-24 16:38:23,979 - functions - INFO - Model downloads completed.
2024-10-24 16:38:23,985 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-24 16:38:23,986 - main - INFO - Application initialization complete
2024-10-24 16:38:27,089 - main - INFO - Processing model URL: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-24 16:38:27,095 - main - INFO - Job enqueued successfully. Job ID: d83bdc0e10e98794fea35b1944c7568e
2024-10-24 16:49:28,996 - main - INFO - Processing model URL: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-24 16:49:28,997 - main - INFO - Job enqueued successfully. Job ID: 4ad1ec4884ee71c2cfc851a7103ac1ea
2024-10-25 11:24:43,891 - main - INFO - Application shutdown initiated
2024-10-25 13:47:59,379 - main - INFO - Starting application initialization
2024-10-25 13:47:59,381 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 13:47:59,388 - db - INFO - Database initialization completed.
2024-10-25 13:47:59,399 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010691 seconds, for an average of 0.00044545833333333335 seconds per hash.
2024-10-25 13:47:59,399 - functions - INFO - Checking models directory...
2024-10-25 13:47:59,399 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 13:47:59,399 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 13:47:59,399 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 13:47:59,399 - functions - INFO - Model downloads completed.
2024-10-25 13:47:59,406 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 13:47:59,408 - main - INFO - Application initialization complete
2024-10-25 13:50:03,443 - main - INFO - Processing model URL: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 13:50:03,460 - main - INFO - Job enqueued successfully. Job ID: 28a2b04387314978c3f60e22da540a6d
2024-10-25 13:51:37,067 - main - INFO - Processing model URL: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 13:51:37,069 - main - INFO - Job enqueued successfully. Job ID: 6221224200cbc5f9374b89185ffa842e
2024-10-25 13:51:38,501 - worker - INFO - Starting download task for model URL: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 13:51:38,502 - functions - INFO - Model URL not found in database. Adding https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf now...
2024-10-25 13:51:38,503 - functions - INFO - Model URL added: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 13:51:38,503 - functions - INFO - Checking models directory...
2024-10-25 13:51:38,504 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 13:51:38,504 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 13:51:38,504 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 13:51:38,505 - functions - INFO - Downloading model llava-llama-3-8b-v1_1-int4.gguf from https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf...
2024-10-25 13:53:36,288 - functions - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 13:53:36,288 - functions - INFO - Model downloads completed.
2024-10-25 13:53:36,302 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (6221224200cbc5f9374b89185ffa842e)
2024-10-25 13:53:36,303 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 13:54:16,625 - main - INFO - Application shutdown initiated
2024-10-25 14:37:07,722 - main - INFO - Starting application initialization
2024-10-25 14:37:07,726 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 14:37:07,743 - db - INFO - Database initialization completed.
2024-10-25 14:37:07,759 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.015991 seconds, for an average of 0.0006662916666666666 seconds per hash.
2024-10-25 14:37:07,759 - functions - INFO - Checking models directory...
2024-10-25 14:37:07,759 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 14:37:07,760 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 14:37:07,760 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 14:37:07,760 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 14:37:07,760 - functions - INFO - Model downloads completed.
2024-10-25 14:37:07,768 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 14:37:07,771 - main - INFO - Application initialization complete
2024-10-25 14:39:17,183 - main - INFO - Application shutdown initiated
2024-10-25 15:46:22,855 - main - INFO - Starting application initialization
2024-10-25 15:46:22,856 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:46:22,864 - db - INFO - Database initialization completed.
2024-10-25 15:46:22,874 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010434 seconds, for an average of 0.00043475 seconds per hash.
2024-10-25 15:46:22,875 - functions - INFO - Checking models directory...
2024-10-25 15:46:22,875 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:46:22,875 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:46:22,875 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 15:46:22,875 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 15:46:22,875 - functions - INFO - Model downloads completed.
2024-10-25 15:46:22,881 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 15:46:22,883 - main - INFO - Application initialization complete
2024-10-25 15:46:35,422 - main - INFO - Application shutdown initiated
2024-10-25 15:47:25,674 - main - INFO - Starting application initialization
2024-10-25 15:47:25,675 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:47:25,679 - db - INFO - Database initialization completed.
2024-10-25 15:47:25,688 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009522 seconds, for an average of 0.00039674999999999997 seconds per hash.
2024-10-25 15:47:25,688 - functions - INFO - Checking models directory...
2024-10-25 15:47:25,688 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:47:25,689 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:47:25,689 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 15:47:25,689 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 15:47:25,689 - functions - INFO - Model downloads completed.
2024-10-25 15:47:25,694 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 15:47:25,695 - main - INFO - Application initialization complete
2024-10-25 15:48:00,885 - __main__ - INFO - Initializing worker...
2024-10-25 15:48:00,886 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-25 15:48:00,889 - __main__ - INFO - Worker started successfully
2024-10-25 15:48:00,900 - __main__ - ERROR - Failed to start worker: cannot pickle '_thread.lock' object
2024-10-25 15:48:00,902 - __main__ - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 113, in start_worker
    worker.work(with_scheduler=True)
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 37, in work
    return pool.apply(super().work, args=args, kwds=kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 360, in apply
    return self.apply_async(func, args, kwds).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 774, in get
    raise self._value
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 540, in _handle_tasks
    put(task)
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
TypeError: cannot pickle '_thread.lock' object

2024-10-25 15:48:41,778 - main - INFO - Application shutdown initiated
2024-10-25 15:48:43,233 - main - INFO - Starting application initialization
2024-10-25 15:48:43,234 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:48:43,238 - db - INFO - Database initialization completed.
2024-10-25 15:48:43,247 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009197 seconds, for an average of 0.00038320833333333333 seconds per hash.
2024-10-25 15:48:43,248 - functions - INFO - Checking models directory...
2024-10-25 15:48:43,248 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:48:43,248 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:48:43,248 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 15:48:43,248 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 15:48:43,248 - functions - INFO - Model downloads completed.
2024-10-25 15:48:43,253 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 15:48:43,254 - main - INFO - Application initialization complete
2024-10-25 15:48:46,433 - __main__ - INFO - Initializing worker...
2024-10-25 15:48:46,433 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-25 15:48:46,435 - __main__ - INFO - Worker started successfully
2024-10-25 15:48:47,646 - __main__ - ERROR - Failed to start worker: BaseWorker.__init__() missing 1 required positional argument: 'queues'
2024-10-25 15:48:47,647 - __main__ - ERROR - multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
                    ^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 43, in _work_wrapper
    worker = Worker(*args)
             ^^^^^^^^^^^^^
TypeError: BaseWorker.__init__() missing 1 required positional argument: 'queues'
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 118, in start_worker
    worker.work(with_scheduler=True)
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 37, in work
    return pool.apply(self._work_wrapper, args=(args, kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 360, in apply
    return self.apply_async(func, args, kwds).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 774, in get
    raise self._value
TypeError: BaseWorker.__init__() missing 1 required positional argument: 'queues'

2024-10-25 15:50:06,464 - main - INFO - Application shutdown initiated
2024-10-25 15:50:07,878 - main - INFO - Starting application initialization
2024-10-25 15:50:07,879 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:50:07,883 - db - INFO - Database initialization completed.
2024-10-25 15:50:07,892 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009199 seconds, for an average of 0.0003832916666666667 seconds per hash.
2024-10-25 15:50:07,892 - functions - INFO - Checking models directory...
2024-10-25 15:50:07,893 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:50:07,893 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:50:07,893 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 15:50:07,893 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 15:50:07,893 - functions - INFO - Model downloads completed.
2024-10-25 15:50:07,898 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 15:50:07,899 - main - INFO - Application initialization complete
2024-10-25 15:50:11,867 - __main__ - INFO - Initializing worker...
2024-10-25 15:50:11,867 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-25 15:50:11,869 - __main__ - INFO - Worker started successfully
2024-10-25 15:50:11,880 - __main__ - ERROR - Failed to start worker: cannot pickle '_thread.lock' object
2024-10-25 15:50:11,881 - __main__ - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 119, in start_worker
    worker.work(with_scheduler=True)
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 37, in work
    return pool.apply(self._work_wrapper, args=(self.queues, self.name, self.connection, args, kwargs))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 360, in apply
    return self.apply_async(func, args, kwds).get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 774, in get
    raise self._value
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py", line 540, in _handle_tasks
    put(task)
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/connection.py", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/reduction.py", line 51, in dumps
    cls(buf, protocol).dump(obj)
TypeError: cannot pickle '_thread.lock' object

2024-10-25 15:50:48,248 - main - INFO - Application shutdown initiated
2024-10-25 15:50:49,725 - main - INFO - Starting application initialization
2024-10-25 15:50:49,727 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:50:49,731 - db - INFO - Database initialization completed.
2024-10-25 15:50:49,742 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010163 seconds, for an average of 0.00042345833333333336 seconds per hash.
2024-10-25 15:50:49,742 - functions - INFO - Checking models directory...
2024-10-25 15:50:49,742 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:50:49,742 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:50:49,742 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 15:50:49,742 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 15:50:49,742 - functions - INFO - Model downloads completed.
2024-10-25 15:50:49,748 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 15:50:49,749 - main - INFO - Application initialization complete
2024-10-25 15:50:52,968 - __main__ - INFO - Initializing worker...
2024-10-25 15:50:52,968 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-25 15:50:52,975 - __main__ - INFO - Worker started successfully with PID: 34086
2024-10-25 15:50:54,184 - rq.worker - INFO - Worker rq:worker:worker-34086 started with PID 34086, version 1.16.2
2024-10-25 15:50:54,185 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-34086
2024-10-25 15:50:54,186 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-25 15:50:54,192 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 15:50:54,194 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 15:50:54,195 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 15:50:55,447 - rq.scheduler - INFO - Scheduler for model_downloads, document_scans, file_uploads started with PID 34091
2024-10-25 15:51:58,626 - main - INFO - Processing model URL:   "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf
2024-10-25 15:51:58,631 - main - INFO - Job enqueued successfully. Job ID: 2b9b3b93e564049a34ab5f5bca7fdb55
2024-10-25 15:51:58,633 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('  "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/...)[39;49;00m (2b9b3b93e564049a34ab5f5bca7fdb55)
2024-10-25 15:51:58,656 - rq.worker - ERROR - [Job 2b9b3b93e564049a34ab5f5bca7fdb55]: exception raised while executing (worker.download_model_task)
Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/utils.py", line 118, in import_attribute
    attribute_owner = getattr(module, attribute_owner_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'worker' has no attribute ''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/worker.py", line 1430, in perform_job
    rv = job.perform()
         ^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1280, in perform
    self._result = self._execute()
                   ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1317, in _execute
    result = self.func(*self.args, **self.kwargs)
             ^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 425, in func
    return import_attribute(self.func_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/utils.py", line 120, in import_attribute
    raise ValueError('Invalid attribute name: %s' % attribute_name)
ValueError: Invalid attribute name: download_model_task

2024-10-25 15:53:00,048 - main - INFO - Application shutdown initiated
2024-10-25 15:53:01,446 - main - INFO - Starting application initialization
2024-10-25 15:53:01,447 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:53:01,452 - db - INFO - Database initialization completed.
2024-10-25 15:53:01,461 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009231 seconds, for an average of 0.000384625 seconds per hash.
2024-10-25 15:53:01,461 - functions - INFO - Checking models directory...
2024-10-25 15:53:01,461 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:53:01,461 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:53:01,461 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 15:53:01,462 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 15:53:01,462 - functions - INFO - Model downloads completed.
2024-10-25 15:53:01,466 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 15:53:01,467 - main - INFO - Application initialization complete
2024-10-25 15:53:03,100 - __main__ - INFO - Worker stopped by user
2024-10-25 15:53:03,100 - rq.worker - INFO - Worker worker-34086 [PID 34086]: warm shut down requested
2024-10-25 15:53:03,128 - rq.scheduler - INFO - Scheduler stopping, releasing locks for model_downloads, document_scans, file_uploads...
2024-10-25 15:53:03,129 - rq.scheduler - INFO - Scheduler with PID 34091 has stopped
2024-10-25 15:53:03,346 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-34086
2024-10-25 15:53:05,675 - __main__ - INFO - Initializing worker...
2024-10-25 15:53:05,675 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-25 15:53:05,682 - __main__ - INFO - Worker started successfully with PID: 34268
2024-10-25 15:53:06,872 - rq.worker - INFO - Worker rq:worker:worker-34268 started with PID 34268, version 1.16.2
2024-10-25 15:53:06,872 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-34268
2024-10-25 15:53:06,873 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-25 15:53:06,879 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 15:53:06,881 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 15:53:06,883 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 15:53:08,086 - rq.scheduler - INFO - Scheduler for file_uploads, document_scans, model_downloads started with PID 34282
2024-10-25 15:53:14,535 - main - INFO - Processing model URL:   "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf
2024-10-25 15:53:14,539 - main - INFO - Job enqueued successfully. Job ID: dc8203f04834c6f79471bf38900b6e2d
2024-10-25 15:53:14,541 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('  "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/...)[39;49;00m (dc8203f04834c6f79471bf38900b6e2d)
2024-10-25 15:53:14,556 - worker - INFO - Starting download task for model URL:   "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf
2024-10-25 15:53:14,557 - functions - INFO - Model URL not found in database. Adding   "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf now...
2024-10-25 15:53:14,560 - functions - INFO - Model URL added:   "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf
2024-10-25 15:53:14,560 - functions - INFO - Checking models directory...
2024-10-25 15:53:14,561 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:53:14,562 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:53:14,563 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 15:53:14,563 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 15:53:14,564 - functions - INFO - Downloading model Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf from   "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf...
2024-10-25 15:53:14,576 - worker - ERROR - Error in download task: <urlopen error unknown url type: "https>
2024-10-25 15:53:14,579 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 51, in download_model_task
    _, download_status = download_models()
                         ^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/src/functions.py", line 248, in download_models
    urllib.request.urlretrieve(url, filename)
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 537, in _open
    return self._call_chain(self.handle_open, 'unknown',
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 1420, in unknown_open
    raise URLError('unknown url type: %s' % type)
urllib.error.URLError: <urlopen error unknown url type: "https>

2024-10-25 15:53:14,582 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (dc8203f04834c6f79471bf38900b6e2d)
2024-10-25 15:53:14,582 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 15:53:21,751 - main - INFO - Processing model URL: https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf
2024-10-25 15:53:21,753 - main - INFO - Job enqueued successfully. Job ID: d4fe7664bd1fa9226efdfeddd86f3e57
2024-10-25 15:53:21,755 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/res...)[39;49;00m (d4fe7664bd1fa9226efdfeddd86f3e57)
2024-10-25 15:53:21,771 - worker - INFO - Starting download task for model URL: https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf
2024-10-25 15:53:21,772 - functions - INFO - Model URL not found in database. Adding https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf now...
2024-10-25 15:53:21,773 - functions - INFO - Model URL added: https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf
2024-10-25 15:53:21,773 - functions - INFO - Checking models directory...
2024-10-25 15:53:21,773 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:53:21,774 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:53:21,775 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf
2024-10-25 15:53:21,776 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-25 15:53:21,776 - functions - INFO - Downloading model Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf from   "https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q4.gguf...
2024-10-25 15:53:21,789 - worker - ERROR - Error in download task: <urlopen error unknown url type: "https>
2024-10-25 15:53:21,791 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 51, in download_model_task
    _, download_status = download_models()
                         ^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/src/functions.py", line 248, in download_models
    urllib.request.urlretrieve(url, filename)
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 515, in open
    response = self._open(req, data)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 537, in _open
    return self._call_chain(self.handle_open, 'unknown',
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/urllib/request.py", line 1420, in unknown_open
    raise URLError('unknown url type: %s' % type)
urllib.error.URLError: <urlopen error unknown url type: "https>

2024-10-25 15:53:21,793 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (d4fe7664bd1fa9226efdfeddd86f3e57)
2024-10-25 15:53:21,794 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 15:54:03,213 - main - INFO - Application shutdown initiated
2024-10-25 15:54:15,172 - main - INFO - Starting application initialization
2024-10-25 15:54:15,173 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:54:15,177 - db - INFO - Database initialization completed.
2024-10-25 15:54:15,188 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010607 seconds, for an average of 0.0004419583333333333 seconds per hash.
2024-10-25 15:54:15,188 - functions - INFO - Checking models directory...
2024-10-25 15:54:15,188 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:54:15,188 - functions - INFO - Downloading model nomic-embed-text-v1.5.Q6_K.gguf from https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q6_K.gguf...
2024-10-25 15:54:18,834 - functions - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:54:18,834 - functions - INFO - Downloading model Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf from https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF/resolve/main/Llama-3.1-8B-Lexi-Uncensored_V2_Q5.gguf...
2024-10-25 15:56:43,342 - main - INFO - Starting application initialization
2024-10-25 15:56:43,343 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:56:43,348 - db - INFO - Database initialization completed.
2024-10-25 15:56:43,358 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.00989 seconds, for an average of 0.0004120833333333333 seconds per hash.
2024-10-25 15:56:43,358 - functions - INFO - Checking models directory...
2024-10-25 15:56:43,358 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:56:43,359 - functions - INFO - Downloading model nomic-embed-text-v1.5.Q6_K.gguf from https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q6_K.gguf...
2024-10-25 15:56:46,438 - functions - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:56:46,439 - functions - INFO - Model downloads completed.
2024-10-25 15:56:46,455 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 15:56:46,458 - main - INFO - Application initialization complete
2024-10-25 15:57:32,563 - main - INFO - Processing model URL: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 15:57:32,572 - main - INFO - Job enqueued successfully. Job ID: 2655b25ec62a5df7db540fce957002fd
2024-10-25 15:57:32,573 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/mai...)[39;49;00m (2655b25ec62a5df7db540fce957002fd)
2024-10-25 15:57:32,589 - worker - INFO - Starting download task for model URL: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 15:57:32,590 - functions - INFO - Model URL not found in database. Adding https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf now...
2024-10-25 15:57:32,590 - functions - INFO - Model URL added: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 15:57:32,591 - functions - INFO - Checking models directory...
2024-10-25 15:57:32,591 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:57:32,592 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:57:32,594 - functions - INFO - Downloading model Hermes-3-Llama-3.1-8B.Q4_K_M.gguf from https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf...
2024-10-25 15:57:52,392 - main - ERROR - Error listing jobs: 'MultiQueueWorker' object has no attribute 'model_queue'
2024-10-25 15:57:52,399 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 368, in list_model_jobs
    job_ids = multi_queue_worker.model_queue.get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'MultiQueueWorker' object has no attribute 'model_queue'

2024-10-25 15:59:15,636 - main - INFO - Application shutdown initiated
2024-10-25 15:59:17,508 - main - INFO - Starting application initialization
2024-10-25 15:59:17,510 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 15:59:17,514 - db - INFO - Database initialization completed.
2024-10-25 15:59:17,524 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010179 seconds, for an average of 0.000424125 seconds per hash.
2024-10-25 15:59:17,524 - functions - INFO - Checking models directory...
2024-10-25 15:59:17,524 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 15:59:31,102 - functions - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 15:59:31,105 - functions - INFO - Model downloads completed.
2024-10-25 15:59:31,110 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (2655b25ec62a5df7db540fce957002fd)
2024-10-25 15:59:31,111 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 15:59:31,153 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 15:59:31,153 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 15:59:31,153 - functions - INFO - Model downloads completed.
2024-10-25 15:59:31,164 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 15:59:31,167 - main - INFO - Application initialization complete
2024-10-25 15:59:57,795 - main - INFO - Application shutdown initiated
2024-10-25 16:00:00,590 - main - INFO - Starting application initialization
2024-10-25 16:00:00,591 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 16:00:00,595 - db - INFO - Database initialization completed.
2024-10-25 16:00:00,605 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009691 seconds, for an average of 0.00040379166666666667 seconds per hash.
2024-10-25 16:00:00,605 - functions - INFO - Checking models directory...
2024-10-25 16:00:00,605 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 16:00:00,606 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 16:00:00,606 - functions - INFO - Downloading model Hermes-3-Llama-3.1-8B.Q4_K_M.gguf from https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf...
2024-10-25 16:01:58,901 - functions - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:01:58,903 - functions - INFO - Model downloads completed.
2024-10-25 16:01:58,918 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 16:01:58,921 - main - INFO - Application initialization complete
2024-10-25 16:02:12,333 - main - INFO - Starting application initialization
2024-10-25 16:02:12,334 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 16:02:12,338 - db - INFO - Database initialization completed.
2024-10-25 16:02:12,349 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010932 seconds, for an average of 0.0004555 seconds per hash.
2024-10-25 16:02:12,350 - functions - INFO - Checking models directory...
2024-10-25 16:02:12,350 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 16:02:12,350 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 16:02:12,350 - functions - INFO - Model downloads completed.
2024-10-25 16:02:12,356 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 16:02:12,357 - main - INFO - Application initialization complete
2024-10-25 16:02:23,165 - main - INFO - Processing model URL: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:02:23,170 - main - INFO - Job enqueued successfully. Job ID: 06e9f1723c86f0e2ec17e09de2dba385
2024-10-25 16:02:23,170 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/mai...)[39;49;00m (06e9f1723c86f0e2ec17e09de2dba385)
2024-10-25 16:02:23,186 - worker - INFO - Starting download task for model URL: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:02:23,187 - functions - INFO - Model URL not found in database. Adding https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf now...
2024-10-25 16:02:23,188 - functions - INFO - Model URL added: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:02:23,189 - functions - INFO - Checking models directory...
2024-10-25 16:02:23,189 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 16:02:23,191 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 16:02:23,191 - functions - INFO - Downloading model Hermes-3-Llama-3.1-8B.Q4_K_M.gguf from https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf...
2024-10-25 16:04:20,254 - functions - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:04:20,258 - functions - INFO - Model downloads completed.
2024-10-25 16:04:20,265 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (06e9f1723c86f0e2ec17e09de2dba385)
2024-10-25 16:04:20,266 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 16:04:20,277 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 16:04:20,279 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 16:04:20,281 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 16:17:50,365 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 16:17:50,369 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 16:17:50,372 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 16:20:29,796 - main - INFO - Application shutdown initiated
2024-10-25 16:20:31,843 - main - INFO - Starting application initialization
2024-10-25 16:20:31,844 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 16:20:31,849 - db - INFO - Database initialization completed.
2024-10-25 16:20:31,860 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010327 seconds, for an average of 0.00043029166666666666 seconds per hash.
2024-10-25 16:20:31,860 - functions - INFO - Checking models directory...
2024-10-25 16:20:31,860 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 16:20:31,861 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 16:20:31,861 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:20:31,861 - functions - INFO - Model downloads completed.
2024-10-25 16:20:31,868 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 16:20:31,869 - main - INFO - Application initialization complete
2024-10-25 16:20:36,419 - main - INFO - Application shutdown initiated
2024-10-25 16:20:48,788 - main - INFO - Starting application initialization
2024-10-25 16:20:48,790 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 16:20:48,794 - db - INFO - Database initialization completed.
2024-10-25 16:20:48,805 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010746 seconds, for an average of 0.00044775 seconds per hash.
2024-10-25 16:20:48,805 - functions - INFO - Checking models directory...
2024-10-25 16:20:48,805 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 16:20:48,806 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 16:20:48,806 - functions - INFO - Downloading model Hermes-3-Llama-3.1-8B.Q4_K_M.gguf from https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf...
2024-10-25 16:22:49,505 - functions - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:22:49,505 - functions - INFO - Model downloads completed.
2024-10-25 16:22:49,524 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 16:22:49,526 - main - INFO - Application initialization complete
2024-10-25 16:22:59,098 - main - INFO - Processing model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-25 16:22:59,103 - main - INFO - Job enqueued successfully. Job ID: 477139713712b0374b6cd71a7031ac37
2024-10-25 16:22:59,105 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf')[39;49;00m (477139713712b0374b6cd71a7031ac37)
2024-10-25 16:22:59,124 - worker - INFO - Starting download task for model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-25 16:22:59,126 - functions - INFO - Model URL not found in database. Adding https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf now...
2024-10-25 16:22:59,127 - functions - INFO - Model URL added: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-25 16:22:59,128 - functions - INFO - Checking models directory...
2024-10-25 16:22:59,129 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 16:22:59,130 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 16:22:59,130 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:22:59,131 - functions - INFO - Downloading model bge-m3-q8_0.gguf from https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf...
2024-10-25 16:23:14,627 - functions - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 16:23:14,628 - functions - INFO - Model downloads completed.
2024-10-25 16:23:14,630 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (477139713712b0374b6cd71a7031ac37)
2024-10-25 16:23:14,631 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 16:29:59,713 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 16:29:59,717 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 16:29:59,718 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 16:35:56,621 - main - INFO - Application shutdown initiated
2024-10-25 16:35:58,423 - main - INFO - Starting application initialization
2024-10-25 16:35:58,424 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 16:35:58,430 - db - INFO - Database initialization completed.
2024-10-25 16:35:58,440 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010076 seconds, for an average of 0.0004198333333333333 seconds per hash.
2024-10-25 16:35:58,440 - functions - INFO - Checking models directory...
2024-10-25 16:35:58,440 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 16:35:58,441 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 16:35:58,441 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 16:35:58,441 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 16:35:58,441 - functions - INFO - Model downloads completed.
2024-10-25 16:35:58,447 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 16:35:58,448 - main - INFO - Application initialization complete
2024-10-25 16:36:03,603 - main - INFO - Application shutdown initiated
2024-10-25 16:43:29,844 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 16:43:29,847 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 16:43:29,850 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 16:57:00,011 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 16:57:00,015 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 16:57:00,017 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 17:08:56,002 - main - INFO - Starting application initialization
2024-10-25 17:08:56,003 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:08:56,008 - db - INFO - Database initialization completed.
2024-10-25 17:08:56,019 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010725 seconds, for an average of 0.000446875 seconds per hash.
2024-10-25 17:08:56,019 - functions - INFO - Checking models directory...
2024-10-25 17:08:56,019 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:08:56,019 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:08:56,019 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:08:56,020 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:08:56,020 - functions - INFO - Model downloads completed.
2024-10-25 17:08:56,026 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:08:56,027 - main - INFO - Application initialization complete
2024-10-25 17:10:30,098 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 17:10:30,103 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 17:10:30,105 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 17:11:26,511 - main - INFO - Application shutdown initiated
2024-10-25 17:11:28,285 - main - INFO - Starting application initialization
2024-10-25 17:11:28,286 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:11:28,291 - db - INFO - Database initialization completed.
2024-10-25 17:11:28,302 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010672 seconds, for an average of 0.0004446666666666666 seconds per hash.
2024-10-25 17:11:28,302 - functions - INFO - Checking models directory...
2024-10-25 17:11:28,302 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:11:28,303 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:11:28,303 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:11:28,303 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:11:28,303 - functions - INFO - Model downloads completed.
2024-10-25 17:11:28,309 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:11:28,310 - main - INFO - Application initialization complete
2024-10-25 17:13:02,574 - main - WARNING - Error checking existing jobs: 'NoneType' object has no attribute 'get_job_ids'
2024-10-25 17:13:02,574 - main - ERROR - Failed to enqueue job: 'NoneType' object has no attribute 'enqueue'
2024-10-25 17:13:02,574 - main - ERROR - Failed to process document upload request: 500: Failed to queue document processing: 'NoneType' object has no attribute 'enqueue'
2024-10-25 17:13:02,575 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 780, in upload_and_process_documents
    job = document_processing_queue.enqueue(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'enqueue'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 815, in upload_and_process_documents
    raise HTTPException(
fastapi.exceptions.HTTPException: 500: Failed to queue document processing: 'NoneType' object has no attribute 'enqueue'

2024-10-25 17:14:50,409 - main - WARNING - Error checking existing jobs: 'NoneType' object has no attribute 'get_job_ids'
2024-10-25 17:14:50,409 - main - ERROR - Failed to enqueue job: 'NoneType' object has no attribute 'enqueue'
2024-10-25 17:14:50,409 - main - ERROR - Failed to process document upload request: 500: Failed to queue document processing: 'NoneType' object has no attribute 'enqueue'
2024-10-25 17:14:50,409 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 780, in upload_and_process_documents
    job = document_processing_queue.enqueue(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'enqueue'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 815, in upload_and_process_documents
    raise HTTPException(
fastapi.exceptions.HTTPException: 500: Failed to queue document processing: 'NoneType' object has no attribute 'enqueue'

2024-10-25 17:15:28,661 - main - INFO - Application shutdown initiated
2024-10-25 17:15:30,508 - main - INFO - Starting application initialization
2024-10-25 17:15:30,509 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:15:30,513 - db - INFO - Database initialization completed.
2024-10-25 17:15:30,524 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011176 seconds, for an average of 0.0004656666666666667 seconds per hash.
2024-10-25 17:15:30,525 - functions - INFO - Checking models directory...
2024-10-25 17:15:30,525 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:15:30,525 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:15:30,525 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:15:30,525 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:15:30,525 - functions - INFO - Model downloads completed.
2024-10-25 17:15:30,532 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:15:30,533 - main - INFO - Application initialization complete
2024-10-25 17:15:35,833 - main - INFO - Job enqueued successfully. Job ID: 20ef0580048876219a3f65c18511aeac
2024-10-25 17:15:35,834 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.process_document_task('friendlist.txt', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', None, '127.0.0.1')[39;49;00m (20ef0580048876219a3f65c18511aeac)
2024-10-25 17:15:35,856 - rq.worker - ERROR - [Job 20ef0580048876219a3f65c18511aeac]: exception raised while executing (worker.process_document_task)
Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/utils.py", line 118, in import_attribute
    attribute_owner = getattr(module, attribute_owner_name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'worker' has no attribute ''

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/worker.py", line 1430, in perform_job
    rv = job.perform()
         ^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1280, in perform
    self._result = self._execute()
                   ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1317, in _execute
    result = self.func(*self.args, **self.kwargs)
             ^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 425, in func
    return import_attribute(self.func_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/utils.py", line 120, in import_attribute
    raise ValueError('Invalid attribute name: %s' % attribute_name)
ValueError: Invalid attribute name: process_document_task

2024-10-25 17:18:04,762 - main - INFO - Application shutdown initiated
2024-10-25 17:18:06,731 - main - INFO - Starting application initialization
2024-10-25 17:18:06,733 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:18:06,737 - db - INFO - Database initialization completed.
2024-10-25 17:18:06,748 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010896 seconds, for an average of 0.000454 seconds per hash.
2024-10-25 17:18:06,748 - functions - INFO - Checking models directory...
2024-10-25 17:18:06,749 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:18:06,749 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:18:06,749 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:18:06,749 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:18:06,749 - functions - INFO - Model downloads completed.
2024-10-25 17:18:06,755 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:18:06,756 - main - INFO - Application initialization complete
2024-10-25 17:19:17,525 - main - INFO - Application shutdown initiated
2024-10-25 17:19:48,878 - main - INFO - Starting application initialization
2024-10-25 17:19:48,879 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:19:48,883 - db - INFO - Database initialization completed.
2024-10-25 17:19:48,894 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010459 seconds, for an average of 0.00043579166666666663 seconds per hash.
2024-10-25 17:19:48,894 - functions - INFO - Checking models directory...
2024-10-25 17:19:48,894 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:19:48,895 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:19:48,895 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:19:48,895 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:19:48,895 - functions - INFO - Model downloads completed.
2024-10-25 17:19:48,901 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:19:48,903 - main - INFO - Application initialization complete
2024-10-25 17:20:29,103 - main - INFO - Application shutdown initiated
2024-10-25 17:20:30,991 - main - INFO - Starting application initialization
2024-10-25 17:20:30,992 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:20:30,997 - db - INFO - Database initialization completed.
2024-10-25 17:20:31,007 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010269 seconds, for an average of 0.00042787500000000003 seconds per hash.
2024-10-25 17:20:31,007 - functions - INFO - Checking models directory...
2024-10-25 17:20:31,007 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:20:31,008 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:20:31,008 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:20:31,008 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:20:31,008 - functions - INFO - Model downloads completed.
2024-10-25 17:20:31,014 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:20:31,015 - main - INFO - Application initialization complete
2024-10-25 17:21:18,305 - main - INFO - Application shutdown initiated
2024-10-25 17:21:20,050 - main - INFO - Starting application initialization
2024-10-25 17:21:20,052 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:21:20,057 - db - INFO - Database initialization completed.
2024-10-25 17:21:20,068 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010518 seconds, for an average of 0.00043825 seconds per hash.
2024-10-25 17:21:20,068 - functions - INFO - Checking models directory...
2024-10-25 17:21:20,068 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:21:20,068 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:21:20,068 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:21:20,068 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:21:20,069 - functions - INFO - Model downloads completed.
2024-10-25 17:21:20,076 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:21:20,078 - main - INFO - Application initialization complete
2024-10-25 17:21:22,626 - rq.worker - INFO - Worker worker-34268 [PID 34268]: warm shut down requested
2024-10-25 17:21:22,626 - __main__ - INFO - Worker stopped by user
2024-10-25 17:21:23,267 - rq.scheduler - INFO - Scheduler stopping, releasing locks for file_uploads, document_scans, model_downloads...
2024-10-25 17:21:23,274 - rq.scheduler - INFO - Scheduler with PID 34282 has stopped
2024-10-25 17:21:23,536 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-34268
2024-10-25 17:21:26,659 - __main__ - INFO - Initializing worker...
2024-10-25 17:21:26,659 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-25 17:21:26,667 - __main__ - INFO - Worker started successfully with PID: 38209
2024-10-25 17:21:27,870 - rq.worker - INFO - Worker rq:worker:worker-38209 started with PID 38209, version 1.16.2
2024-10-25 17:21:27,870 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-38209
2024-10-25 17:21:27,871 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-25 17:21:27,878 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 17:21:27,880 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 17:21:27,881 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 17:21:29,302 - rq.scheduler - INFO - Scheduler for file_uploads, model_downloads, document_scans started with PID 38210
2024-10-25 17:21:51,412 - main - INFO - Processing document URL: string
2024-10-25 17:21:51,415 - main - ERROR - Failed to enqueue job: cannot pickle 'BufferedRandom' instances
2024-10-25 17:21:51,415 - main - ERROR - Failed to process document upload request: 500: Failed to queue document processing: cannot pickle 'BufferedRandom' instances
2024-10-25 17:21:51,419 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 769, in upload_and_process_documents
    job = document_processing_queue.enqueue(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 971, in enqueue
    return self.enqueue_call(
           ^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 719, in enqueue_call
    return self.enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1094, in enqueue_job
    return self._enqueue_job(job, pipeline=pipeline, at_front=at_front)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 1122, in _enqueue_job
    job.save(pipeline=pipe)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1087, in save
    mapping = self.to_dict(include_meta=include_meta, include_result=include_result)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1011, in to_dict
    'data': zlib.compress(self.data),
                          ^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 506, in data
    self._data = self.serializer.dumps(job_tuple)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot pickle 'BufferedRandom' instances

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 804, in upload_and_process_documents
    raise HTTPException(
fastapi.exceptions.HTTPException: 500: Failed to queue document processing: cannot pickle 'BufferedRandom' instances

2024-10-25 17:23:22,219 - main - INFO - Application shutdown initiated
2024-10-25 17:23:24,223 - main - INFO - Starting application initialization
2024-10-25 17:23:24,228 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:23:24,232 - db - INFO - Database initialization completed.
2024-10-25 17:23:24,243 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010637 seconds, for an average of 0.0004432083333333334 seconds per hash.
2024-10-25 17:23:24,244 - functions - INFO - Checking models directory...
2024-10-25 17:23:24,244 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:23:24,244 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:23:24,244 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:23:24,244 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:23:24,244 - functions - INFO - Model downloads completed.
2024-10-25 17:23:24,250 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:23:24,252 - main - INFO - Application initialization complete
2024-10-25 17:23:28,038 - main - INFO - Processing document URL: string
2024-10-25 17:23:28,048 - main - INFO - Job enqueued successfully. Job ID: 75eb98f6989a2bac846661d846b875ba
2024-10-25 17:23:28,054 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/75eb98f6989a2bac846661d846b875ba_2019-Sherlock-KDD.pdf', 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', None)[39;49;00m (75eb98f6989a2bac846661d846b875ba)
2024-10-25 17:23:28,071 - worker - INFO - Starting file upload task for file: /tmp/75eb98f6989a2bac846661d846b875ba_2019-Sherlock-KDD.pdf
2024-10-25 17:23:28,075 - worker - ERROR - Error in upload task: module 'magika' has no attribute 'identify_bytes'
2024-10-25 17:23:28,078 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 124, in upload_file_task
    result = magika.identify_bytes(file_content)
             ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'magika' has no attribute 'identify_bytes'

2024-10-25 17:23:28,081 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (75eb98f6989a2bac846661d846b875ba)
2024-10-25 17:23:28,082 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 17:28:39,973 - main - INFO - Received request to retrieve all stored documents
2024-10-25 17:28:39,974 - main - INFO - Retrieving all stored documents from the database
2024-10-25 17:28:39,983 - main - INFO - Retrieved 2 stored documents from the database
2024-10-25 17:36:58,263 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 17:36:58,268 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 17:36:58,270 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 17:38:54,022 - main - INFO - Processing document URL: string
2024-10-25 17:38:54,023 - main - INFO - Job enqueued successfully. Job ID: 9dc9f44088b20e5c276e6b4363b66173
2024-10-25 17:38:54,024 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/9dc9f44088b20e5c276e6b4363b66173_Python - Docs - PostHog.pdf', 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', None)[39;49;00m (9dc9f44088b20e5c276e6b4363b66173)
2024-10-25 17:38:54,039 - worker - INFO - Starting file upload task for file: /tmp/9dc9f44088b20e5c276e6b4363b66173_Python - Docs - PostHog.pdf
2024-10-25 17:38:54,041 - worker - ERROR - Error in upload task: module 'magika' has no attribute 'identify_bytes'
2024-10-25 17:38:54,044 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 124, in upload_file_task
    result = magika.identify_bytes(file_content)
             ^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'magika' has no attribute 'identify_bytes'

2024-10-25 17:38:54,047 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (9dc9f44088b20e5c276e6b4363b66173)
2024-10-25 17:38:54,048 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 17:40:35,583 - main - INFO - Application shutdown initiated
2024-10-25 17:40:37,295 - main - INFO - Starting application initialization
2024-10-25 17:40:37,296 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:40:37,300 - db - INFO - Database initialization completed.
2024-10-25 17:40:37,310 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.009881 seconds, for an average of 0.0004117083333333333 seconds per hash.
2024-10-25 17:40:37,311 - functions - INFO - Checking models directory...
2024-10-25 17:40:37,311 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:40:37,311 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:40:37,311 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:40:37,311 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:40:37,311 - functions - INFO - Model downloads completed.
2024-10-25 17:40:37,318 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:40:37,319 - main - INFO - Application initialization complete
2024-10-25 17:41:55,994 - main - INFO - Processing document URL: string
2024-10-25 17:41:55,996 - main - INFO - Job enqueued successfully. Job ID: 391d94870925eef25f6ba9a52939f448
2024-10-25 17:41:55,997 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/391d94870925eef25f6ba9a52939f448_Python API Reference _ Supabase Docs..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (391d94870925eef25f6ba9a52939f448)
2024-10-25 17:41:56,011 - worker - INFO - Starting file upload task for file: /tmp/391d94870925eef25f6ba9a52939f448_Python API Reference _ Supabase Docs.pdf
2024-10-25 17:41:56,016 - worker - ERROR - Error in upload task: module 'magika' has no attribute 'scan_bytes'
2024-10-25 17:41:56,019 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 124, in upload_file_task
    result = magika.scan_bytes(file_content)
             ^^^^^^^^^^^^^^^^^
AttributeError: module 'magika' has no attribute 'scan_bytes'

2024-10-25 17:41:56,022 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (391d94870925eef25f6ba9a52939f448)
2024-10-25 17:41:56,023 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 17:43:35,592 - main - INFO - Application shutdown initiated
2024-10-25 17:43:37,420 - main - INFO - Starting application initialization
2024-10-25 17:43:37,422 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:43:37,427 - db - INFO - Database initialization completed.
2024-10-25 17:43:37,439 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011719 seconds, for an average of 0.0004882916666666667 seconds per hash.
2024-10-25 17:43:37,439 - functions - INFO - Checking models directory...
2024-10-25 17:43:37,439 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:43:37,439 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:43:37,439 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:43:37,440 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:43:37,440 - functions - INFO - Model downloads completed.
2024-10-25 17:43:37,446 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:43:37,447 - main - INFO - Application initialization complete
2024-10-25 17:43:45,813 - main - INFO - Received request to retrieve all stored documents
2024-10-25 17:43:45,813 - main - INFO - Retrieving all stored documents from the database
2024-10-25 17:43:45,822 - main - INFO - Retrieved 2 stored documents from the database
2024-10-25 17:44:14,584 - main - INFO - Processing document URL: string
2024-10-25 17:44:14,588 - main - INFO - Job enqueued successfully. Job ID: e8355c804058ab30b22e7f451e9f79fd
2024-10-25 17:44:14,588 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/e8355c804058ab30b22e7f451e9f79fd_2019-Sherlock-KDD.pdf', 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (e8355c804058ab30b22e7f451e9f79fd)
2024-10-25 17:44:14,614 - worker - INFO - Starting file upload task for file: /tmp/e8355c804058ab30b22e7f451e9f79fd_2019-Sherlock-KDD.pdf
2024-10-25 17:44:14,634 - worker - ERROR - Error in upload task: cannot unpack non-iterable coroutine object
2024-10-25 17:44:14,636 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 135, in upload_file_task
    sentences, thousands_of_input_words = parse_submitted_document_file_into_sentence_strings_func(file_path_or_url, mime_type)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot unpack non-iterable coroutine object

2024-10-25 17:44:14,638 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (e8355c804058ab30b22e7f451e9f79fd)
2024-10-25 17:44:14,638 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 17:45:03,457 - main - INFO - Application shutdown initiated
2024-10-25 17:45:05,362 - main - INFO - Starting application initialization
2024-10-25 17:45:05,363 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:45:05,369 - db - INFO - Database initialization completed.
2024-10-25 17:45:05,380 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011528 seconds, for an average of 0.0004803333333333333 seconds per hash.
2024-10-25 17:45:05,381 - functions - INFO - Checking models directory...
2024-10-25 17:45:05,381 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:45:05,381 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:45:05,381 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:45:05,381 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:45:05,382 - functions - INFO - Model downloads completed.
2024-10-25 17:45:05,389 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:45:05,390 - main - INFO - Application initialization complete
2024-10-25 17:45:07,167 - main - INFO - Processing document URL: string
2024-10-25 17:45:07,171 - main - INFO - Job enqueued successfully. Job ID: b2f9567140309448378e6d2ac03fec65
2024-10-25 17:45:07,172 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/b2f9567140309448378e6d2ac03fec65_2019-Sherlock-KDD.pdf', 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (b2f9567140309448378e6d2ac03fec65)
2024-10-25 17:45:07,193 - worker - INFO - Starting file upload task for file: /tmp/b2f9567140309448378e6d2ac03fec65_2019-Sherlock-KDD.pdf
2024-10-25 17:45:07,203 - worker - ERROR - Error in upload task: cannot unpack non-iterable coroutine object
2024-10-25 17:45:07,205 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 135, in upload_file_task
    sentences, thousands_of_input_words = parse_submitted_document_file_into_sentence_strings_func(file_path_or_url, mime_type)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: cannot unpack non-iterable coroutine object

2024-10-25 17:45:07,209 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (b2f9567140309448378e6d2ac03fec65)
2024-10-25 17:45:07,210 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 17:45:42,716 - main - INFO - Application shutdown initiated
2024-10-25 17:45:44,286 - main - INFO - Starting application initialization
2024-10-25 17:45:44,287 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-25 17:45:44,293 - db - INFO - Database initialization completed.
2024-10-25 17:45:44,304 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.01114 seconds, for an average of 0.0004641666666666667 seconds per hash.
2024-10-25 17:45:44,304 - functions - INFO - Checking models directory...
2024-10-25 17:45:44,304 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-25 17:45:44,305 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-25 17:45:44,305 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-25 17:45:44,305 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-25 17:45:44,305 - functions - INFO - Model downloads completed.
2024-10-25 17:45:44,311 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-25 17:45:44,312 - main - INFO - Application initialization complete
2024-10-25 17:45:48,146 - main - INFO - Processing document URL: string
2024-10-25 17:45:48,150 - main - INFO - Job enqueued successfully. Job ID: 7155861e6e2dcc5602cad2602b88a959
2024-10-25 17:45:48,150 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/7155861e6e2dcc5602cad2602b88a959_2019-Sherlock-KDD.pdf', 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (7155861e6e2dcc5602cad2602b88a959)
2024-10-25 17:45:48,174 - worker - INFO - Starting file upload task for file: /tmp/7155861e6e2dcc5602cad2602b88a959_2019-Sherlock-KDD.pdf
2024-10-25 17:45:49,747 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (7155861e6e2dcc5602cad2602b88a959)
2024-10-25 17:45:49,747 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-25 17:52:34,800 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 17:52:34,805 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 17:52:34,806 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 18:06:04,921 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 18:06:04,926 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 18:06:04,928 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 18:19:35,087 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 18:19:35,090 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 18:19:35,091 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 18:33:05,229 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 18:33:05,235 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 18:33:05,237 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 18:46:35,293 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 18:46:35,299 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 18:46:35,301 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 19:00:05,451 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 19:00:05,455 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 19:00:05,456 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 19:13:35,568 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 19:13:35,573 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 19:13:35,574 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 19:27:05,617 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 19:27:05,620 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 19:27:05,621 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 19:40:35,666 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 19:40:35,668 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 19:40:35,669 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 19:54:05,790 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 19:54:05,795 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 19:54:05,797 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 20:07:35,986 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 20:07:35,991 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 20:07:35,994 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 20:21:06,156 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 20:21:06,160 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 20:21:06,161 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 20:34:36,285 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 20:34:36,290 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 20:34:36,291 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 20:48:06,436 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 20:48:06,440 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 20:48:06,442 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 21:01:36,534 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 21:01:36,538 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 21:01:36,540 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 21:15:06,646 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 21:15:06,649 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 21:15:06,650 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 21:28:36,699 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 21:28:36,704 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 21:28:36,706 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 21:42:06,797 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 21:42:06,802 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 21:42:06,803 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 21:55:36,910 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 21:55:36,916 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 21:55:36,917 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 22:09:07,042 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 22:09:07,047 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 22:09:07,049 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 22:22:37,180 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 22:22:37,184 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 22:22:37,186 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 22:36:07,363 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-25 22:36:07,367 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-25 22:36:07,368 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-25 22:43:46,241 - main - INFO - Application shutdown initiated
2024-10-27 10:25:40,327 - main - INFO - Starting application initialization
2024-10-27 10:25:40,333 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 10:25:40,362 - db - INFO - Database initialization completed.
2024-10-27 10:25:40,375 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.012886 seconds, for an average of 0.0005369166666666667 seconds per hash.
2024-10-27 10:25:40,376 - functions - INFO - Checking models directory...
2024-10-27 10:25:40,376 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 10:25:40,376 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 10:25:40,377 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 10:25:40,377 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 10:25:40,377 - functions - INFO - Model downloads completed.
2024-10-27 10:25:40,383 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 10:25:40,385 - main - INFO - Application initialization complete
2024-10-27 10:28:33,098 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:28:33,099 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:28:33,125 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:28:33,130 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:28:33,132 - main - INFO - Processed 1 semantic data types
2024-10-27 10:28:33,140 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:28:50,786 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:28:50,787 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:28:50,790 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:28:50,794 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:28:50,794 - main - INFO - Processed 1 semantic data types
2024-10-27 10:28:50,796 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:28:52,425 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:28:52,425 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:28:52,427 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:28:52,429 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:28:52,429 - main - INFO - Processed 1 semantic data types
2024-10-27 10:28:52,429 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:30:53,347 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:30:53,354 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:30:53,354 - main - INFO - Processed 1 semantic data types
2024-10-27 10:31:54,713 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:31:54,720 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:31:54,721 - main - INFO - Processed 1 semantic data types
2024-10-27 10:31:54,724 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:31:54,726 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:31:54,726 - main - INFO - Processed 1 semantic data types
2024-10-27 10:32:06,480 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:32:06,481 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:32:06,488 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:32:06,518 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:32:06,520 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:32:06,520 - main - INFO - Processed 1 semantic data types
2024-10-27 10:32:11,038 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:32:11,038 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:32:11,042 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:32:11,063 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:32:11,065 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:32:11,065 - main - INFO - Processed 1 semantic data types
2024-10-27 10:32:13,994 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:32:13,997 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:32:13,997 - main - INFO - Processed 1 semantic data types
2024-10-27 10:32:13,999 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 10:32:14,001 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 10:32:14,001 - main - INFO - Processed 1 semantic data types
2024-10-27 10:32:18,734 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:32:18,734 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:32:18,736 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:32:18,738 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:32:18,738 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:32:18,741 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:54:05,796 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:54:05,796 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:54:05,809 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:54:05,811 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:54:05,811 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:54:05,814 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:55:05,491 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:55:05,491 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:55:05,495 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:55:05,496 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:55:05,496 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:55:05,498 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:57:55,039 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:57:55,040 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:57:55,047 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:57:55,047 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:57:55,058 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:57:55,062 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:58:51,388 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:58:51,389 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:58:51,392 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:58:51,392 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:58:51,397 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:58:51,398 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:58:58,994 - main - INFO - Processing document URL: None
2024-10-27 10:58:59,003 - main - INFO - Job enqueued successfully. Job ID: 0d0c82b44d063f305d72119c5f1b0580
2024-10-27 10:58:59,007 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:58:59,007 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:58:59,011 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:59:28,396 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:59:28,396 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:59:28,397 - main - INFO - Received request to retrieve all stored documents
2024-10-27 10:59:28,397 - main - INFO - Retrieving all stored documents from the database
2024-10-27 10:59:28,403 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 10:59:28,404 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:00:58,603 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:00:58,604 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:00:58,606 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:00:58,606 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:00:58,612 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:00:58,614 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:02:06,785 - main - INFO - Application shutdown initiated
2024-10-27 11:02:20,826 - main - INFO - Starting application initialization
2024-10-27 11:02:20,827 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:02:20,834 - db - INFO - Database initialization completed.
2024-10-27 11:02:20,845 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010781 seconds, for an average of 0.00044920833333333336 seconds per hash.
2024-10-27 11:02:20,845 - functions - INFO - Checking models directory...
2024-10-27 11:02:20,845 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:02:20,846 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:02:20,847 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:02:20,847 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:02:20,847 - functions - INFO - Model downloads completed.
2024-10-27 11:02:20,853 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:02:20,855 - main - INFO - Application initialization complete
2024-10-27 11:03:01,382 - main - INFO - Application shutdown initiated
2024-10-27 11:03:03,275 - main - INFO - Starting application initialization
2024-10-27 11:03:03,278 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:03:03,283 - db - INFO - Database initialization completed.
2024-10-27 11:03:03,295 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.012534 seconds, for an average of 0.00052225 seconds per hash.
2024-10-27 11:03:03,296 - functions - INFO - Checking models directory...
2024-10-27 11:03:03,296 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:03:03,296 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:03:03,296 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:03:03,296 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:03:03,296 - functions - INFO - Model downloads completed.
2024-10-27 11:03:03,303 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:03:03,305 - main - INFO - Application initialization complete
2024-10-27 11:04:50,520 - main - INFO - Application shutdown initiated
2024-10-27 11:04:52,270 - main - INFO - Starting application initialization
2024-10-27 11:04:52,271 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:04:52,275 - db - INFO - Database initialization completed.
2024-10-27 11:04:52,286 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010742 seconds, for an average of 0.0004475833333333333 seconds per hash.
2024-10-27 11:04:52,287 - functions - INFO - Checking models directory...
2024-10-27 11:04:52,287 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:04:52,287 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:04:52,287 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:04:52,287 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:04:52,287 - functions - INFO - Model downloads completed.
2024-10-27 11:04:52,293 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:04:52,294 - main - INFO - Application initialization complete
2024-10-27 11:05:11,141 - main - INFO - Application shutdown initiated
2024-10-27 11:05:20,528 - main - INFO - Starting application initialization
2024-10-27 11:05:20,528 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:05:20,532 - db - INFO - Database initialization completed.
2024-10-27 11:05:20,543 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010723 seconds, for an average of 0.0004467916666666667 seconds per hash.
2024-10-27 11:05:20,543 - functions - INFO - Checking models directory...
2024-10-27 11:05:20,543 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:05:20,544 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:05:20,544 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:05:20,544 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:05:20,544 - functions - INFO - Model downloads completed.
2024-10-27 11:05:20,550 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:05:20,552 - main - INFO - Application initialization complete
2024-10-27 11:08:13,525 - main - INFO - Application shutdown initiated
2024-10-27 11:08:15,345 - main - INFO - Starting application initialization
2024-10-27 11:08:15,346 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:08:15,351 - db - INFO - Database initialization completed.
2024-10-27 11:08:15,362 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010919 seconds, for an average of 0.0004549583333333333 seconds per hash.
2024-10-27 11:08:15,362 - functions - INFO - Checking models directory...
2024-10-27 11:08:15,362 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:08:15,363 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:08:15,363 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:08:15,363 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:08:15,363 - functions - INFO - Model downloads completed.
2024-10-27 11:08:15,369 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:08:15,371 - main - INFO - Application initialization complete
2024-10-27 11:08:43,408 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:08:43,408 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:08:43,409 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:08:43,409 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:08:43,416 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:08:43,417 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:13:45,227 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:13:45,227 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:13:45,233 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:13:45,233 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:13:45,265 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:13:45,267 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:17:01,550 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:17:01,550 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:17:01,554 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:17:01,554 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:17:01,563 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:17:01,569 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:17:07,940 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:17:07,940 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:17:07,943 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:17:07,943 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:17:07,949 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:17:07,950 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:17:37,116 - main - INFO - Processing document URL: None
2024-10-27 11:17:37,118 - main - INFO - Job enqueued successfully. Job ID: be686fdb14f34656707f2ee159ab8cdd
2024-10-27 11:17:37,122 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:17:37,122 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:17:37,131 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:19:22,677 - main - INFO - Application shutdown initiated
2024-10-27 11:19:24,475 - main - INFO - Starting application initialization
2024-10-27 11:19:24,477 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:19:24,482 - db - INFO - Database initialization completed.
2024-10-27 11:19:24,493 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010389 seconds, for an average of 0.00043287500000000005 seconds per hash.
2024-10-27 11:19:24,493 - functions - INFO - Checking models directory...
2024-10-27 11:19:24,493 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:19:24,493 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:19:24,493 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:19:24,494 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:19:24,494 - functions - INFO - Model downloads completed.
2024-10-27 11:19:24,500 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:19:24,501 - main - INFO - Application initialization complete
2024-10-27 11:25:52,746 - main - INFO - Application shutdown initiated
2024-10-27 11:25:57,227 - main - INFO - Starting application initialization
2024-10-27 11:25:57,228 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:25:57,234 - db - INFO - Database initialization completed.
2024-10-27 11:25:57,246 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011458 seconds, for an average of 0.0004774166666666666 seconds per hash.
2024-10-27 11:25:57,246 - functions - INFO - Checking models directory...
2024-10-27 11:25:57,246 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:25:57,246 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:25:57,247 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:25:57,247 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:25:57,247 - functions - INFO - Model downloads completed.
2024-10-27 11:25:57,253 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:25:57,254 - main - INFO - Application initialization complete
2024-10-27 11:26:12,324 - main - INFO - Processing document URL: None
2024-10-27 11:26:12,329 - main - INFO - Job enqueued successfully. Job ID: 8dd45d89cedff20c2154f947f6d4e6a4
2024-10-27 11:26:12,336 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:26:12,336 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:26:12,344 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:29:43,584 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:29:43,584 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:29:43,593 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:29:43,593 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:29:43,613 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:29:43,617 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:30:53,718 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:30:53,719 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:30:53,726 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:30:53,726 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:30:53,734 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:30:53,735 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:31:50,813 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:31:50,814 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:31:50,817 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:31:50,817 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:31:50,846 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:31:50,849 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:31:56,821 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:31:56,822 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:31:56,825 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:31:56,825 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:31:56,828 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:31:56,829 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:32:46,409 - main - INFO - Processing document URL: string
2024-10-27 11:32:46,416 - main - INFO - Job enqueued successfully. Job ID: 5286c64f0e5a6bdeb48b6c81f25bf28f
2024-10-27 11:34:21,832 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:34:21,845 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1428, in get_connection
    if connection.can_read() and self.cache is None:
       ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 570, in can_read
    return self._parser.can_read(timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/base.py", line 128, in can_read
    return self._buffer and self._buffer.can_read(timeout)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/socket.py", line 95, in can_read
    return bool(self.unread_bytes()) or self._read_from_socket(
                                        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/socket.py", line 68, in _read_from_socket
    raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)
redis.exceptions.ConnectionError: Connection closed by server.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1432, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:34:26,832 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:34:26,835 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:34:31,834 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:34:31,836 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:34:36,833 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:34:36,834 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:34:41,843 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:34:41,845 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:34:46,843 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:34:46,847 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:34:51,839 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:34:51,840 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:34:56,835 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:34:56,837 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:01,835 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:01,837 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:06,843 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:06,848 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:11,833 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:11,834 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:16,840 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:16,842 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:21,994 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:21,998 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:22,000 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:35:22,001 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:35:22,024 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:22,025 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:22,026 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:35:22,026 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:35:22,035 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:35:22,037 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:35:27,006 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:27,007 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:32,004 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:32,006 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:36,997 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:35:36,998 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:35:42,007 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:35:42,010 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:35:46,999 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:35:47,001 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:35:51,995 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:35:51,996 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:35:57,002 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:35:57,003 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:36:02,006 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:02,009 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:07,003 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:07,004 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:12,000 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:12,002 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:17,009 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:17,011 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:21,999 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:22,001 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:27,009 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:27,011 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:32,003 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:32,007 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:37,010 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:37,013 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:41,995 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:41,995 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:47,001 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:47,004 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:52,001 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:52,002 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:36:57,006 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:36:57,008 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:37:02,001 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:37:02,002 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:37:07,009 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:37:07,011 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:37:10,454 - main - INFO - Application shutdown initiated
2024-10-27 11:39:46,682 - main - INFO - Starting application initialization
2024-10-27 11:39:46,683 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:39:46,690 - db - INFO - Database initialization completed.
2024-10-27 11:39:46,702 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011851 seconds, for an average of 0.0004937916666666667 seconds per hash.
2024-10-27 11:39:46,702 - functions - INFO - Checking models directory...
2024-10-27 11:39:46,702 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:39:46,703 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:39:46,703 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:39:46,703 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:39:46,703 - functions - INFO - Model downloads completed.
2024-10-27 11:39:46,709 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:39:46,711 - main - INFO - Application initialization complete
2024-10-27 11:39:46,998 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:39:47,006 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:39:51,991 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:39:51,992 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:39:55,144 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:39:55,145 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:39:55,154 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:39:55,154 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:39:55,155 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:39:55,155 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:39:55,156 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:39:55,156 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:39:55,162 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:39:55,164 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:40:00,150 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:00,152 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:05,153 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:05,155 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:10,151 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:10,152 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:15,156 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:15,158 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:20,157 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:20,159 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:25,165 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:25,166 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:30,152 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:30,153 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:35,151 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:35,153 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:40,154 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:40,157 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:45,153 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:45,156 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:50,152 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:50,155 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:40:55,153 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:40:55,154 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:00,145 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:00,145 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:05,154 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:05,156 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:10,157 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:10,160 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:15,159 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:15,162 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:20,152 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:20,153 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:25,153 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:25,155 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:30,151 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:30,153 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:35,158 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:35,160 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:40,150 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:40,151 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:45,157 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:45,160 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:50,146 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:50,147 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:41:55,153 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:41:55,155 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:00,167 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:00,170 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:05,148 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:05,150 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:10,150 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:10,151 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:15,149 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:15,149 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:20,151 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:20,153 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:25,151 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:25,154 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:30,158 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:30,160 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:35,155 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:35,157 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:40,153 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:40,156 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:45,156 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:45,157 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:50,145 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:50,146 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:42:55,148 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:42:55,148 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:00,150 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:00,153 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:05,149 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:05,150 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:10,153 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:10,154 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:15,164 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:15,165 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:20,154 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:20,159 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:25,154 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:25,157 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:30,154 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:30,156 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:35,150 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:35,151 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:40,153 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:40,157 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:45,150 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:45,151 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:50,152 - main - ERROR - Error listing jobs: Authentication required.
2024-10-27 11:43:50,153 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 369, in connect
    self.on_connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 471, in on_connect
    self.read_response()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 592, in read_response
    response = self._parser.read_response(disable_decoding=disable_decoding)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 15, in read_response
    result = self._read_response(disable_decoding=disable_decoding)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/resp2.py", line 38, in _read_response
    raise error
redis.exceptions.AuthenticationError: Authentication required.

2024-10-27 11:43:55,145 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:43:55,147 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:44:00,150 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:44:00,151 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:44:05,148 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:44:05,149 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:44:20,214 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:44:20,214 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:44:20,230 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:44:20,230 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:44:20,236 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:44:20,237 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:44:43,889 - main - INFO - Processing document URL: string
2024-10-27 11:44:43,900 - main - INFO - Job enqueued successfully. Job ID: 59f0210c92d4727b96b114f46d760377
2024-10-27 11:45:07,459 - main - INFO - Application shutdown initiated
2024-10-27 11:45:09,333 - main - INFO - Starting application initialization
2024-10-27 11:45:09,335 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:45:09,340 - db - INFO - Database initialization completed.
2024-10-27 11:45:09,351 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010853 seconds, for an average of 0.0004522083333333333 seconds per hash.
2024-10-27 11:45:09,351 - functions - INFO - Checking models directory...
2024-10-27 11:45:09,351 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:45:09,352 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:45:09,352 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:45:09,352 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:45:09,352 - functions - INFO - Model downloads completed.
2024-10-27 11:45:09,358 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:45:09,359 - main - INFO - Application initialization complete
2024-10-27 11:47:18,486 - main - INFO - Application shutdown initiated
2024-10-27 11:47:20,282 - main - INFO - Starting application initialization
2024-10-27 11:47:20,283 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:47:20,289 - db - INFO - Database initialization completed.
2024-10-27 11:47:20,300 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010781 seconds, for an average of 0.00044920833333333336 seconds per hash.
2024-10-27 11:47:20,301 - functions - INFO - Checking models directory...
2024-10-27 11:47:20,301 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:47:20,301 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:47:20,301 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:47:20,301 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:47:20,301 - functions - INFO - Model downloads completed.
2024-10-27 11:47:20,307 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:47:20,308 - main - INFO - Application initialization complete
2024-10-27 11:47:38,656 - __main__ - INFO - Initializing worker...
2024-10-27 11:47:38,656 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 11:47:38,664 - __main__ - INFO - Worker started successfully with PID: 94119
2024-10-27 11:50:43,034 - main - INFO - Application shutdown initiated
2024-10-27 11:50:45,000 - main - INFO - Starting application initialization
2024-10-27 11:50:45,002 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 11:50:45,008 - db - INFO - Database initialization completed.
2024-10-27 11:50:45,020 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.01124 seconds, for an average of 0.00046833333333333335 seconds per hash.
2024-10-27 11:50:45,020 - functions - INFO - Checking models directory...
2024-10-27 11:50:45,020 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 11:50:45,020 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 11:50:45,020 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 11:50:45,021 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 11:50:45,021 - functions - INFO - Model downloads completed.
2024-10-27 11:50:45,027 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 11:50:45,028 - main - INFO - Application initialization complete
2024-10-27 11:51:24,152 - __main__ - INFO - Initializing worker...
2024-10-27 11:51:24,152 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 11:51:24,164 - __main__ - INFO - Worker started successfully with PID: 94559
2024-10-27 11:52:01,249 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:52:01,249 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:52:01,252 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:52:01,252 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:52:01,261 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:52:01,263 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:52:12,289 - __main__ - INFO - Initializing worker...
2024-10-27 11:52:12,289 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 11:52:12,299 - __main__ - INFO - Worker started successfully with PID: 94697
2024-10-27 11:52:44,137 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:52:44,137 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:52:44,137 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:52:44,137 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:52:44,142 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:52:44,143 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:54:11,677 - __main__ - INFO - Initializing worker...
2024-10-27 11:54:11,677 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 11:54:11,685 - __main__ - INFO - Worker started successfully with PID: 94887
2024-10-27 11:55:09,140 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:09,149 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1428, in get_connection
    if connection.can_read() and self.cache is None:
       ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 570, in can_read
    return self._parser.can_read(timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/base.py", line 128, in can_read
    return self._buffer and self._buffer.can_read(timeout)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/socket.py", line 95, in can_read
    return bool(self.unread_bytes()) or self._read_from_socket(
                                        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/socket.py", line 68, in _read_from_socket
    raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)
redis.exceptions.ConnectionError: Connection closed by server.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1432, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:14,141 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:14,143 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:19,141 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:19,143 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:24,142 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:24,144 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:29,143 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:29,145 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:34,146 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:34,148 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:39,146 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:39,149 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:44,147 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:44,149 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:49,144 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:49,146 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:54,145 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:54,147 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:55:59,145 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:55:59,148 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:04,147 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:04,149 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:09,148 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:09,150 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:14,144 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:14,147 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:19,144 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:19,146 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:24,142 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:24,144 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:29,146 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:29,148 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:34,145 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:34,147 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:39,142 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:39,145 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:44,141 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:44,142 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:49,142 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:49,144 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:54,144 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:54,147 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:56:59,140 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:56:59,141 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:57:04,146 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 11:57:04,147 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 11:57:17,830 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:57:17,831 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:57:17,833 - main - INFO - Received request to retrieve all stored documents
2024-10-27 11:57:17,833 - main - INFO - Retrieving all stored documents from the database
2024-10-27 11:57:17,839 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:57:17,841 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 11:57:22,951 - main - INFO - Processing document URL: string
2024-10-27 11:57:22,954 - main - INFO - Job enqueued successfully. Job ID: 7685a0bb80e9d7ca75be0b1957ccfac6
2024-10-27 11:57:35,447 - __main__ - INFO - Initializing worker...
2024-10-27 11:57:35,447 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 11:57:35,454 - __main__ - INFO - Worker started successfully with PID: 95679
2024-10-27 12:00:39,642 - main - INFO - Processing document URL: string
2024-10-27 12:00:39,656 - main - INFO - Job enqueued successfully. Job ID: 0ab0f765afd164cfcd75e7d2503ed9ef
2024-10-27 12:06:27,886 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 12:06:27,891 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1428, in get_connection
    if connection.can_read() and self.cache is None:
       ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 570, in can_read
    return self._parser.can_read(timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/base.py", line 128, in can_read
    return self._buffer and self._buffer.can_read(timeout)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/socket.py", line 95, in can_read
    return bool(self.unread_bytes()) or self._read_from_socket(
                                        ^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/_parsers/socket.py", line 68, in _read_from_socket
    raise ConnectionError(SERVER_CLOSED_CONNECTION_ERROR)
redis.exceptions.ConnectionError: Connection closed by server.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1432, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 12:06:32,882 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 12:06:32,884 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 12:06:37,886 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 12:06:37,887 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 12:06:42,880 - main - ERROR - Error listing jobs: Error 61 connecting to localhost:6379. Connection refused.
2024-10-27 12:06:42,881 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 357, in connect
    sock = self.retry.call_with_retry(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/retry.py", line 62, in call_with_retry
    return do()
           ^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 358, in <lambda>
    lambda: self._connect(), lambda error: self.disconnect(error)
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 730, in _connect
    raise err
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 718, in _connect
    sock.connect(socket_address)
ConnectionRefusedError: [Errno 61] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/main.py", line 829, in list_document_jobs
    job_ids = Queue('file_uploads', connection=redis_manager.redis_sync).get_job_ids()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/queue.py", line 376, in get_job_ids
    job_ids = [as_text(job_id) for job_id in self.connection.lrange(self.key, start, end)]
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/commands/core.py", line 2748, in lrange
    return self.execute_command("LRANGE", name, start, end, keys=[name])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 559, in execute_command
    return self._execute_command(*args, **options)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/client.py", line 565, in _execute_command
    conn = self.connection or pool.get_connection(command_name, **options)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 1422, in get_connection
    connection.connect()
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/redis/connection.py", line 363, in connect
    raise ConnectionError(self._error_message(e))
redis.exceptions.ConnectionError: Error 61 connecting to localhost:6379. Connection refused.

2024-10-27 12:07:04,601 - main - INFO - Application shutdown initiated
2024-10-27 12:07:26,965 - main - INFO - Starting application initialization
2024-10-27 12:07:26,967 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 12:07:26,973 - db - INFO - Database initialization completed.
2024-10-27 12:07:26,985 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011504 seconds, for an average of 0.00047933333333333335 seconds per hash.
2024-10-27 12:07:26,985 - functions - INFO - Checking models directory...
2024-10-27 12:07:26,985 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 12:07:26,986 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 12:07:26,986 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 12:07:26,986 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 12:07:26,986 - functions - INFO - Model downloads completed.
2024-10-27 12:07:26,992 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 12:07:26,994 - main - INFO - Application initialization complete
2024-10-27 12:07:53,727 - main - INFO - Processing document URL: string
2024-10-27 12:07:53,737 - main - INFO - Job enqueued successfully. Job ID: a97f3f17992b58da2fa500a521d6c75b
2024-10-27 12:10:14,453 - main - INFO - Application shutdown initiated
2024-10-27 12:10:15,891 - main - INFO - Starting application initialization
2024-10-27 12:10:15,897 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 12:10:15,901 - db - INFO - Database initialization completed.
2024-10-27 12:10:15,912 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010815 seconds, for an average of 0.000450625 seconds per hash.
2024-10-27 12:10:15,913 - functions - INFO - Checking models directory...
2024-10-27 12:10:15,913 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 12:10:15,913 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 12:10:15,913 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 12:10:15,913 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 12:10:15,913 - functions - INFO - Model downloads completed.
2024-10-27 12:10:15,919 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 12:10:15,920 - main - INFO - Application initialization complete
2024-10-27 12:10:23,791 - main - INFO - Processing document URL: string
2024-10-27 12:10:23,798 - main - INFO - Job enqueued successfully. Job ID: d4ac6891c278f6184152411d437e1c1d
2024-10-27 12:10:33,427 - __main__ - INFO - Initializing worker...
2024-10-27 12:10:33,427 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:10:33,435 - __main__ - INFO - Worker started successfully with PID: 97663
2024-10-27 12:14:42,559 - __main__ - INFO - Initializing worker...
2024-10-27 12:14:42,559 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:14:42,567 - __main__ - INFO - Worker started successfully with PID: 97954
2024-10-27 12:17:15,380 - main - INFO - Processing document URL: string
2024-10-27 12:17:15,388 - main - INFO - Job enqueued successfully. Job ID: 17935b8e61282d16c4ec504cabdd5ec6
2024-10-27 12:17:35,208 - __main__ - INFO - Initializing worker...
2024-10-27 12:17:35,209 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:17:35,217 - __main__ - INFO - Worker started successfully with PID: 98125
2024-10-27 12:18:15,426 - __main__ - INFO - Initializing worker...
2024-10-27 12:18:15,427 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:18:15,435 - __main__ - INFO - Worker started successfully with PID: 98184
2024-10-27 12:20:45,684 - main - INFO - Application shutdown initiated
2024-10-27 12:20:47,196 - main - INFO - Starting application initialization
2024-10-27 12:20:47,197 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 12:20:47,202 - db - INFO - Database initialization completed.
2024-10-27 12:20:47,213 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010927 seconds, for an average of 0.0004552916666666666 seconds per hash.
2024-10-27 12:20:47,214 - functions - INFO - Checking models directory...
2024-10-27 12:20:47,214 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 12:20:47,214 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 12:20:47,214 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 12:20:47,214 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 12:20:47,214 - functions - INFO - Model downloads completed.
2024-10-27 12:20:47,220 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 12:20:47,221 - main - INFO - Application initialization complete
2024-10-27 12:20:51,959 - __main__ - INFO - Initializing worker...
2024-10-27 12:20:51,959 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:20:51,966 - __main__ - INFO - Worker started successfully with PID: 98330
2024-10-27 12:20:53,171 - __mp_main__ - INFO - Connecting to Redis at your-redis-host:6379
2024-10-27 12:20:53,174 - __mp_main__ - ERROR - Redis connection error: Error 8 connecting to your-redis-host:6379. nodename nor servname provided, or not known.
2024-10-27 12:21:34,866 - __main__ - INFO - Initializing worker...
2024-10-27 12:21:34,867 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:21:34,874 - __main__ - INFO - Worker started successfully with PID: 98412
2024-10-27 12:21:36,058 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-27 12:21:36,060 - __mp_main__ - ERROR - Redis connection error: AUTH <password> called without any password configured for the default user. Are you sure your configuration is correct?
2024-10-27 12:25:16,383 - __main__ - INFO - Initializing worker...
2024-10-27 12:25:16,383 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:25:16,391 - __main__ - INFO - Worker started successfully with PID: 98679
2024-10-27 12:25:17,586 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-27 12:25:17,588 - __mp_main__ - ERROR - Redis connection error: AUTH <password> called without any password configured for the default user. Are you sure your configuration is correct?
2024-10-27 12:26:29,147 - __main__ - INFO - Initializing worker...
2024-10-27 12:26:29,148 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:26:29,156 - __main__ - INFO - Worker started successfully with PID: 99463
2024-10-27 12:26:30,374 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-27 12:26:30,387 - __mp_main__ - ERROR - Redis connection error: AUTH <password> called without any password configured for the default user. Are you sure your configuration is correct?
2024-10-27 12:29:28,827 - main - INFO - Application shutdown initiated
2024-10-27 12:31:35,925 - main - INFO - Starting application initialization
2024-10-27 12:31:35,926 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 12:31:35,930 - db - INFO - Database initialization completed.
2024-10-27 12:31:35,941 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.010658 seconds, for an average of 0.0004440833333333334 seconds per hash.
2024-10-27 12:31:35,941 - functions - INFO - Checking models directory...
2024-10-27 12:31:35,941 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 12:31:35,942 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 12:31:35,942 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 12:31:35,942 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 12:31:35,942 - functions - INFO - Model downloads completed.
2024-10-27 12:31:35,948 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 12:31:35,949 - main - INFO - Application initialization complete
2024-10-27 12:31:59,490 - main - INFO - Processing document URL: string
2024-10-27 12:31:59,512 - main - INFO - Job enqueued successfully. Job ID: 0a19bf4f74defb7128c626a317bed1a4
2024-10-27 12:32:06,767 - __main__ - INFO - Initializing worker...
2024-10-27 12:32:06,767 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 12:32:06,774 - __main__ - INFO - Worker started successfully with PID: 99877
2024-10-27 12:32:07,975 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-27 12:32:07,977 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 12:32:07,981 - db - INFO - Database initialization completed.
2024-10-27 12:32:07,993 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011758 seconds, for an average of 0.0004899166666666666 seconds per hash.
2024-10-27 12:32:07,993 - functions - INFO - Checking models directory...
2024-10-27 12:32:07,993 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 12:32:07,994 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 12:32:07,994 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 12:32:07,994 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 12:32:07,994 - functions - INFO - Model downloads completed.
2024-10-27 12:32:08,001 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 12:32:08,012 - rq.worker - INFO - Worker rq:worker:worker-99877 started with PID 99877, version 1.16.2
2024-10-27 12:32:08,012 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-99877
2024-10-27 12:32:08,013 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-27 12:32:08,020 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 12:32:08,022 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 12:32:08,023 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 12:32:08,026 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/7685a0bb80e9d7ca75be0b1957ccfac6_Hnsw in hnsw - Rust.pdf', 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (7685a0bb80e9d7ca75be0b1957ccfac6)
2024-10-27 12:32:08,052 - worker - INFO - Starting file upload task for file: /tmp/7685a0bb80e9d7ca75be0b1957ccfac6_Hnsw in hnsw - Rust.pdf
2024-10-27 12:32:08,863 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (7685a0bb80e9d7ca75be0b1957ccfac6)
2024-10-27 12:32:08,863 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:32:08,898 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/0ab0f765afd164cfcd75e7d2503ed9ef_A Junior Quants Guide to EV Options ..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (0ab0f765afd164cfcd75e7d2503ed9ef)
2024-10-27 12:32:08,920 - worker - INFO - Starting file upload task for file: /tmp/0ab0f765afd164cfcd75e7d2503ed9ef_A Junior Quants Guide to EV Options Trading Code Included.pdf
2024-10-27 12:32:09,311 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (0ab0f765afd164cfcd75e7d2503ed9ef)
2024-10-27 12:32:09,312 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:32:09,318 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/a97f3f17992b58da2fa500a521d6c75b_DataFog-Inspird-Mutual-NDA-202410 .p..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (a97f3f17992b58da2fa500a521d6c75b)
2024-10-27 12:32:09,341 - worker - INFO - Starting file upload task for file: /tmp/a97f3f17992b58da2fa500a521d6c75b_DataFog-Inspird-Mutual-NDA-202410 .pdf
2024-10-27 12:32:09,376 - rq.scheduler - INFO - Scheduler for document_scans, file_uploads, model_downloads started with PID 99878
2024-10-27 12:32:09,594 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (a97f3f17992b58da2fa500a521d6c75b)
2024-10-27 12:32:09,595 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:32:09,601 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/d4ac6891c278f6184152411d437e1c1d_DataFog-Inspird-Mutual-NDA-202410 .p..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (d4ac6891c278f6184152411d437e1c1d)
2024-10-27 12:32:09,623 - worker - INFO - Starting file upload task for file: /tmp/d4ac6891c278f6184152411d437e1c1d_DataFog-Inspird-Mutual-NDA-202410 .pdf
2024-10-27 12:32:09,872 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (d4ac6891c278f6184152411d437e1c1d)
2024-10-27 12:32:09,872 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:32:09,879 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/17935b8e61282d16c4ec504cabdd5ec6_DataFog-Inspird-Mutual-NDA-202410 .p..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (17935b8e61282d16c4ec504cabdd5ec6)
2024-10-27 12:32:09,901 - worker - INFO - Starting file upload task for file: /tmp/17935b8e61282d16c4ec504cabdd5ec6_DataFog-Inspird-Mutual-NDA-202410 .pdf
2024-10-27 12:32:10,143 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (17935b8e61282d16c4ec504cabdd5ec6)
2024-10-27 12:32:10,144 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:32:10,150 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/0a19bf4f74defb7128c626a317bed1a4_DataFog-Inspird-Mutual-NDA-202410 .p..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (0a19bf4f74defb7128c626a317bed1a4)
2024-10-27 12:32:10,171 - worker - INFO - Starting file upload task for file: /tmp/0a19bf4f74defb7128c626a317bed1a4_DataFog-Inspird-Mutual-NDA-202410 .pdf
2024-10-27 12:32:10,424 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (0a19bf4f74defb7128c626a317bed1a4)
2024-10-27 12:32:10,425 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:33:58,304 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:33:58,305 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:33:58,337 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 12:33:58,367 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 12:33:58,368 - main - INFO - Processed 1 semantic data types
2024-10-27 12:33:58,372 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:34:05,063 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:34:05,063 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:34:05,070 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:34:05,072 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:34:05,072 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:34:05,074 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:35:04,247 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:35:04,247 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:35:04,270 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:35:04,279 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:35:04,279 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:35:04,292 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:35:13,554 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:35:13,555 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:35:13,630 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:35:13,633 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:35:13,633 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:35:13,637 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:35:50,000 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:35:50,000 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:35:50,009 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:35:50,011 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:35:50,011 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:35:50,014 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:36:05,343 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:36:05,344 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:36:05,348 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:36:05,349 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:36:05,349 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:36:05,352 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:36:50,261 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:36:50,262 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:36:50,273 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:36:50,277 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:36:50,277 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:36:50,281 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:45:40,556 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 12:45:40,562 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 12:45:40,564 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 12:50:23,821 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:50:23,822 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:50:23,833 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:50:23,854 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 12:50:23,856 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 12:50:23,856 - main - INFO - Processed 1 semantic data types
2024-10-27 12:52:24,827 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:52:24,830 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:52:24,855 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 12:52:24,859 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 12:52:24,860 - main - INFO - Processed 1 semantic data types
2024-10-27 12:52:24,863 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:52:26,882 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:52:26,883 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:52:26,886 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:52:26,891 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:52:26,891 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:52:26,902 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:53:11,506 - main - INFO - Processing document URL: None
2024-10-27 12:53:11,513 - main - INFO - Job enqueued successfully. Job ID: 218456c75c90c8f86fbac50eae1d2676
2024-10-27 12:53:11,514 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/218456c75c90c8f86fbac50eae1d2676_A Junior Quants Guide to EV Options ..., None, None, 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 'records', 'zip', None)[39;49;00m (218456c75c90c8f86fbac50eae1d2676)
2024-10-27 12:53:11,531 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:53:11,531 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:53:11,540 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:53:11,659 - worker - INFO - Starting file upload task for file: /tmp/218456c75c90c8f86fbac50eae1d2676_A Junior Quants Guide to EV Options Trading Code Included.pdf
2024-10-27 12:53:12,181 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (218456c75c90c8f86fbac50eae1d2676)
2024-10-27 12:53:12,182 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:53:46,841 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:53:46,843 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:53:46,850 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:53:59,321 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:53:59,322 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:53:59,329 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:54:08,565 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:54:08,566 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:54:08,569 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:54:08,571 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:54:08,571 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:54:08,574 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:56:19,915 - main - INFO - Processing document URL: string
2024-10-27 12:56:19,918 - main - INFO - Job enqueued successfully. Job ID: 8f10f0826b7f0ce0c8520a5be9393806
2024-10-27 12:56:19,919 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/8f10f0826b7f0ce0c8520a5be9393806_A Junior Quants Guide to EV Options ..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (8f10f0826b7f0ce0c8520a5be9393806)
2024-10-27 12:56:19,969 - worker - INFO - Starting file upload task for file: /tmp/8f10f0826b7f0ce0c8520a5be9393806_A Junior Quants Guide to EV Options Trading Code Included.pdf
2024-10-27 12:56:20,458 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (8f10f0826b7f0ce0c8520a5be9393806)
2024-10-27 12:56:20,458 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:56:20,465 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 12:56:20,467 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 12:56:20,468 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 12:56:41,570 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:56:41,570 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:56:41,577 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:57:23,405 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:57:23,405 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:57:23,411 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:57:31,989 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:57:31,989 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:57:31,994 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:57:31,996 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:57:31,996 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:57:31,999 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:57:54,264 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:57:54,264 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:57:54,281 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:57:54,285 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:57:54,285 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:57:54,288 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:59:08,431 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:59:08,432 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:59:08,437 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 12:59:25,360 - main - INFO - Processing document URL: string
2024-10-27 12:59:25,366 - main - INFO - Job enqueued successfully. Job ID: 7f0b9ab060a331df6126f115d7273a90
2024-10-27 12:59:25,368 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/7f0b9ab060a331df6126f115d7273a90_DataFog-Inspird-Mutual-NDA-202410 .p..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (7f0b9ab060a331df6126f115d7273a90)
2024-10-27 12:59:25,409 - worker - INFO - Starting file upload task for file: /tmp/7f0b9ab060a331df6126f115d7273a90_DataFog-Inspird-Mutual-NDA-202410 .pdf
2024-10-27 12:59:25,770 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (7f0b9ab060a331df6126f115d7273a90)
2024-10-27 12:59:25,771 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 12:59:33,976 - main - INFO - Received request to retrieve all stored documents
2024-10-27 12:59:33,976 - main - INFO - Retrieving all stored documents from the database
2024-10-27 12:59:33,982 - main - INFO - Retrieved 2 stored documents from the database
2024-10-27 13:04:13,671 - main - INFO - Application shutdown initiated
2024-10-27 13:04:15,908 - main - INFO - Starting application initialization
2024-10-27 13:04:15,910 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:04:15,915 - db - INFO - Database initialization completed.
2024-10-27 13:04:15,927 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011462 seconds, for an average of 0.00047758333333333333 seconds per hash.
2024-10-27 13:04:15,927 - functions - INFO - Checking models directory...
2024-10-27 13:04:15,927 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:04:15,928 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:04:15,928 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:04:15,928 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:04:15,928 - functions - INFO - Model downloads completed.
2024-10-27 13:04:15,934 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:04:15,936 - main - INFO - Application initialization complete
2024-10-27 13:05:59,377 - main - INFO - Application shutdown initiated
2024-10-27 13:06:01,234 - main - INFO - Starting application initialization
2024-10-27 13:06:01,236 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:06:01,241 - db - INFO - Database initialization completed.
2024-10-27 13:06:01,253 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.012359 seconds, for an average of 0.0005149583333333333 seconds per hash.
2024-10-27 13:06:01,253 - functions - INFO - Checking models directory...
2024-10-27 13:06:01,253 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:06:01,254 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:06:01,254 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:06:01,254 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:06:01,254 - functions - INFO - Model downloads completed.
2024-10-27 13:06:01,261 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:06:01,262 - main - INFO - Application initialization complete
2024-10-27 13:06:49,597 - main - INFO - Application shutdown initiated
2024-10-27 13:06:51,361 - main - INFO - Starting application initialization
2024-10-27 13:06:51,362 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:06:51,367 - db - INFO - Database initialization completed.
2024-10-27 13:06:51,378 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011196 seconds, for an average of 0.00046649999999999996 seconds per hash.
2024-10-27 13:06:51,379 - functions - INFO - Checking models directory...
2024-10-27 13:06:51,379 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:06:51,379 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:06:51,379 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:06:51,379 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:06:51,379 - functions - INFO - Model downloads completed.
2024-10-27 13:06:51,385 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:06:51,387 - main - INFO - Application initialization complete
2024-10-27 13:07:08,653 - main - INFO - Processing document URL: string
2024-10-27 13:07:08,660 - main - INFO - Job enqueued successfully. Job ID: 482d1cc74527d21fcef8075905ac44c3
2024-10-27 13:07:08,661 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/482d1cc74527d21fcef8075905ac44c3_DataFog-Inspird-Mutual-NDA-202410 .p..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (482d1cc74527d21fcef8075905ac44c3)
2024-10-27 13:07:08,692 - worker - INFO - Starting file upload task for file: /tmp/482d1cc74527d21fcef8075905ac44c3_DataFog-Inspird-Mutual-NDA-202410 .pdf
2024-10-27 13:07:08,694 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:07:08,706 - db - INFO - Database initialization completed.
2024-10-27 13:07:08,716 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.00951 seconds, for an average of 0.00039624999999999996 seconds per hash.
2024-10-27 13:07:08,717 - functions - INFO - Checking models directory...
2024-10-27 13:07:08,718 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:07:08,719 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:07:08,719 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:07:08,720 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:07:08,721 - functions - INFO - Model downloads completed.
2024-10-27 13:07:09,061 - worker - ERROR - Database error: 'file_name' is an invalid keyword argument for Document
2024-10-27 13:07:09,062 - worker - ERROR - Error in upload task: 'file_name' is an invalid keyword argument for Document
2024-10-27 13:07:09,065 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 174, in upload_file_task
    document = Document(
               ^^^^^^^^^
  File "<string>", line 4, in __init__
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/sqlalchemy/orm/state.py", line 571, in _initialize_instance
    with util.safe_reraise():
         ^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/sqlalchemy/orm/state.py", line 569, in _initialize_instance
    manager.original_init(*mixed[1:], **kwargs)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py", line 2177, in _declarative_constructor
    raise TypeError(
TypeError: 'file_name' is an invalid keyword argument for Document

2024-10-27 13:07:09,068 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (482d1cc74527d21fcef8075905ac44c3)
2024-10-27 13:07:09,068 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:07:09,075 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 13:07:09,077 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 13:07:09,078 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 13:08:41,512 - main - INFO - Processing document URL: string
2024-10-27 13:08:41,517 - main - INFO - Job enqueued successfully. Job ID: 5d4bb787d5d980456725cd2246f97710
2024-10-27 13:08:41,521 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/5d4bb787d5d980456725cd2246f97710_DataFog-Inspird-Mutual-NDA-202410 .p..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (5d4bb787d5d980456725cd2246f97710)
2024-10-27 13:08:41,567 - worker - INFO - Starting file upload task for file: /tmp/5d4bb787d5d980456725cd2246f97710_DataFog-Inspird-Mutual-NDA-202410 .pdf
2024-10-27 13:08:41,568 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:08:41,575 - db - INFO - Database initialization completed.
2024-10-27 13:08:41,581 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.005556 seconds, for an average of 0.00023150000000000002 seconds per hash.
2024-10-27 13:08:41,582 - functions - INFO - Checking models directory...
2024-10-27 13:08:41,582 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:08:41,583 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:08:41,583 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:08:41,584 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:08:41,584 - functions - INFO - Model downloads completed.
2024-10-27 13:08:41,915 - worker - ERROR - Database error: 'file_name' is an invalid keyword argument for Document
2024-10-27 13:08:41,916 - worker - ERROR - Error in upload task: 'file_name' is an invalid keyword argument for Document
2024-10-27 13:08:41,920 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 174, in upload_file_task
    document = Document(
               ^^^^^^^^^
  File "<string>", line 4, in __init__
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/sqlalchemy/orm/state.py", line 571, in _initialize_instance
    with util.safe_reraise():
         ^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/sqlalchemy/util/langhelpers.py", line 146, in __exit__
    raise exc_value.with_traceback(exc_tb)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/sqlalchemy/orm/state.py", line 569, in _initialize_instance
    manager.original_init(*mixed[1:], **kwargs)
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/sqlalchemy/orm/decl_base.py", line 2177, in _declarative_constructor
    raise TypeError(
TypeError: 'file_name' is an invalid keyword argument for Document

2024-10-27 13:08:41,923 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (5d4bb787d5d980456725cd2246f97710)
2024-10-27 13:08:41,924 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:09:27,691 - main - INFO - Application shutdown initiated
2024-10-27 13:09:29,495 - main - INFO - Starting application initialization
2024-10-27 13:09:29,496 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:09:29,505 - db - INFO - Database initialization completed.
2024-10-27 13:09:29,517 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.012147 seconds, for an average of 0.000506125 seconds per hash.
2024-10-27 13:09:29,518 - functions - INFO - Checking models directory...
2024-10-27 13:09:29,518 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:09:29,518 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:09:29,518 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:09:29,518 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:09:29,518 - functions - INFO - Model downloads completed.
2024-10-27 13:09:29,524 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:09:29,526 - main - INFO - Application initialization complete
2024-10-27 13:09:54,996 - main - INFO - Application shutdown initiated
2024-10-27 13:09:56,803 - main - INFO - Starting application initialization
2024-10-27 13:09:56,804 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:09:56,810 - db - INFO - Database initialization completed.
2024-10-27 13:09:56,822 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.011401 seconds, for an average of 0.00047504166666666664 seconds per hash.
2024-10-27 13:09:56,822 - functions - INFO - Checking models directory...
2024-10-27 13:09:56,822 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:09:56,822 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:09:56,823 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:09:56,823 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:09:56,823 - functions - INFO - Model downloads completed.
2024-10-27 13:09:56,829 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:09:56,830 - main - INFO - Application initialization complete
2024-10-27 13:10:01,222 - main - INFO - Processing document URL: string
2024-10-27 13:10:01,244 - main - INFO - Job enqueued successfully. Job ID: 0feff959dad078422b8dca7566c1e5ac
2024-10-27 13:10:01,247 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/0feff959dad078422b8dca7566c1e5ac_DataFog-Inspird-Mutual-NDA-202410 .p..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (0feff959dad078422b8dca7566c1e5ac)
2024-10-27 13:10:01,274 - worker - INFO - Starting file upload task for file: /tmp/0feff959dad078422b8dca7566c1e5ac_DataFog-Inspird-Mutual-NDA-202410 .pdf
2024-10-27 13:10:01,275 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:10:01,282 - db - INFO - Database initialization completed.
2024-10-27 13:10:01,288 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 24; Took 0.00527 seconds, for an average of 0.00021958333333333335 seconds per hash.
2024-10-27 13:10:01,289 - functions - INFO - Checking models directory...
2024-10-27 13:10:01,289 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:10:01,290 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:10:01,291 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:10:01,292 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:10:01,295 - functions - INFO - Model downloads completed.
2024-10-27 13:10:01,671 - worker - INFO - Successfully saved document and embedding with hash: string
2024-10-27 13:10:01,674 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (0feff959dad078422b8dca7566c1e5ac)
2024-10-27 13:10:01,674 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:10:33,850 - main - INFO - Processing document URL: string
2024-10-27 13:10:33,859 - main - INFO - Job enqueued successfully. Job ID: f5def68097e9d5691d0ffaee10d31e00
2024-10-27 13:10:33,860 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/f5def68097e9d5691d0ffaee10d31e00_topic_AF244-0004_Securing Legacies_ ..., 'string', 0, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', 'string')[39;49;00m (f5def68097e9d5691d0ffaee10d31e00)
2024-10-27 13:10:33,887 - worker - INFO - Starting file upload task for file: /tmp/f5def68097e9d5691d0ffaee10d31e00_topic_AF244-0004_Securing Legacies_ Developing an AI-Enhanced Privacy Architecture for Military Family Knowledge Systems.PDF
2024-10-27 13:10:33,890 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:10:33,898 - db - INFO - Database initialization completed.
2024-10-27 13:10:33,903 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.00495 seconds, for an average of 0.0001903846153846154 seconds per hash.
2024-10-27 13:10:33,904 - functions - INFO - Checking models directory...
2024-10-27 13:10:33,904 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:10:33,905 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:10:33,906 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:10:33,906 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:10:33,906 - functions - INFO - Model downloads completed.
2024-10-27 13:10:34,245 - worker - INFO - Successfully saved document and embedding with hash: string
2024-10-27 13:10:34,250 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (f5def68097e9d5691d0ffaee10d31e00)
2024-10-27 13:10:34,250 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:10:55,799 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:10:55,799 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:10:55,816 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:10:55,818 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:10:55,818 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:10:55,821 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:12:57,219 - main - INFO - Application shutdown initiated
2024-10-27 13:13:27,002 - main - INFO - Starting application initialization
2024-10-27 13:13:27,003 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:13:27,009 - db - INFO - Database initialization completed.
2024-10-27 13:13:27,021 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.011795 seconds, for an average of 0.00045365384615384614 seconds per hash.
2024-10-27 13:13:27,022 - functions - INFO - Checking models directory...
2024-10-27 13:13:27,022 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:13:27,022 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:13:27,022 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:13:27,022 - functions - INFO - Model downloads completed.
2024-10-27 13:13:27,029 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:13:27,030 - main - INFO - Application initialization complete
2024-10-27 13:13:48,633 - main - INFO - Processing model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:13:48,645 - main - INFO - Job enqueued successfully. Job ID: f3b2d4759b5747994fde60a75d014de2
2024-10-27 13:13:48,647 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf')[39;49;00m (f3b2d4759b5747994fde60a75d014de2)
2024-10-27 13:13:48,690 - worker - INFO - Starting download task for model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:13:48,718 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (f3b2d4759b5747994fde60a75d014de2)
2024-10-27 13:13:48,719 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:18:35,580 - main - INFO - Application shutdown initiated
2024-10-27 13:18:37,396 - main - INFO - Starting application initialization
2024-10-27 13:18:37,398 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:18:37,404 - db - INFO - Database initialization completed.
2024-10-27 13:18:37,416 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.011352 seconds, for an average of 0.0004366153846153846 seconds per hash.
2024-10-27 13:18:37,416 - functions - INFO - Checking models directory...
2024-10-27 13:18:37,416 - functions - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:18:37,417 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:18:37,417 - functions - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:18:37,417 - functions - INFO - Model downloads completed.
2024-10-27 13:18:37,424 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:18:37,425 - main - INFO - Application initialization complete
2024-10-27 13:20:30,335 - main - INFO - Application shutdown initiated
2024-10-27 13:20:33,797 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 13:20:33,801 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 13:20:33,802 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 13:23:02,872 - main - INFO - Starting application initialization
2024-10-27 13:23:02,874 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:23:02,879 - db - INFO - Database initialization completed.
2024-10-27 13:23:02,891 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.011891 seconds, for an average of 0.0004573461538461539 seconds per hash.
2024-10-27 13:23:02,892 - utils - INFO - Checking models directory...
2024-10-27 13:23:02,892 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:23:02,892 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:23:02,892 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:23:02,892 - utils - INFO - Model downloads completed.
2024-10-27 13:23:02,898 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:23:02,900 - main - INFO - Application initialization complete
2024-10-27 13:23:13,664 - main - INFO - Processing model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:23:13,683 - main - INFO - Job enqueued successfully. Job ID: 75a365dfc365a860337e933f0c77b0f2
2024-10-27 13:23:13,684 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf')[39;49;00m (75a365dfc365a860337e933f0c77b0f2)
2024-10-27 13:23:13,708 - worker - INFO - Starting download task for model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:23:13,723 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (75a365dfc365a860337e933f0c77b0f2)
2024-10-27 13:23:13,724 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:26:31,829 - main - INFO - Processing model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:26:31,840 - main - INFO - Job enqueued successfully. Job ID: 22c8284e9701cd34670b77fd9131223d
2024-10-27 13:26:31,841 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf')[39;49;00m (22c8284e9701cd34670b77fd9131223d)
2024-10-27 13:26:31,869 - worker - INFO - Starting download task for model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:26:31,871 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (22c8284e9701cd34670b77fd9131223d)
2024-10-27 13:26:31,872 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:28:02,923 - main - INFO - Application shutdown initiated
2024-10-27 13:28:04,710 - main - INFO - Starting application initialization
2024-10-27 13:28:04,711 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:28:04,717 - db - INFO - Database initialization completed.
2024-10-27 13:28:04,728 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.010598 seconds, for an average of 0.00040761538461538463 seconds per hash.
2024-10-27 13:28:04,728 - utils - INFO - Checking models directory...
2024-10-27 13:28:04,729 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:28:04,729 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:28:04,729 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:28:04,729 - utils - INFO - Model downloads completed.
2024-10-27 13:28:04,736 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:28:04,737 - main - INFO - Application initialization complete
2024-10-27 13:28:08,672 - main - INFO - Processing model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:28:08,676 - main - INFO - Job enqueued successfully. Job ID: d0dcc3df19337f0e06e1934ae9003ad8
2024-10-27 13:28:08,678 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf')[39;49;00m (d0dcc3df19337f0e06e1934ae9003ad8)
2024-10-27 13:28:08,713 - worker - INFO - Starting download task for model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:28:08,715 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (d0dcc3df19337f0e06e1934ae9003ad8)
2024-10-27 13:28:08,716 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:28:39,458 - main - INFO - Application shutdown initiated
2024-10-27 13:28:42,379 - main - INFO - Starting application initialization
2024-10-27 13:28:42,380 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:28:42,385 - db - INFO - Database initialization completed.
2024-10-27 13:28:42,397 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.011599 seconds, for an average of 0.0004461153846153846 seconds per hash.
2024-10-27 13:28:42,398 - utils - INFO - Checking models directory...
2024-10-27 13:28:42,398 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:28:42,398 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:28:42,398 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:28:42,398 - utils - INFO - Model downloads completed.
2024-10-27 13:28:42,406 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:28:42,407 - main - INFO - Application initialization complete
2024-10-27 13:28:52,065 - rq.worker - INFO - Worker worker-99877 [PID 99877]: warm shut down requested
2024-10-27 13:28:52,067 - __main__ - INFO - Worker stopped by user
2024-10-27 13:28:52,827 - rq.scheduler - INFO - Scheduler stopping, releasing locks for document_scans, file_uploads, model_downloads...
2024-10-27 13:28:52,833 - rq.scheduler - INFO - Scheduler with PID 99878 has stopped
2024-10-27 13:28:53,099 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-99877
2024-10-27 13:28:55,968 - __main__ - INFO - Initializing worker...
2024-10-27 13:28:55,968 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-27 13:28:55,975 - __main__ - INFO - Worker started successfully with PID: 6971
2024-10-27 13:28:57,144 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-27 13:28:57,146 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:28:57,150 - db - INFO - Database initialization completed.
2024-10-27 13:28:57,160 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.009526 seconds, for an average of 0.00036638461538461537 seconds per hash.
2024-10-27 13:28:57,160 - utils - INFO - Checking models directory...
2024-10-27 13:28:57,160 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:28:57,160 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:28:57,160 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:28:57,160 - utils - INFO - Model downloads completed.
2024-10-27 13:28:57,168 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:28:57,176 - rq.worker - INFO - Worker rq:worker:worker-6971 started with PID 6971, version 1.16.2
2024-10-27 13:28:57,177 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-6971
2024-10-27 13:28:57,178 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-27 13:28:57,185 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 13:28:57,187 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 13:28:57,188 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 13:28:58,528 - rq.scheduler - INFO - Scheduler for document_scans, model_downloads, file_uploads started with PID 6973
2024-10-27 13:29:04,647 - main - INFO - Processing model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:29:04,651 - main - INFO - Job enqueued successfully. Job ID: 9590f03ebf63e416e75a961d8dab3341
2024-10-27 13:29:04,654 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf')[39;49;00m (9590f03ebf63e416e75a961d8dab3341)
2024-10-27 13:29:04,699 - worker - INFO - Starting download task for model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:29:04,700 - utils - INFO - Model URL not found in database. Adding https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf now...
2024-10-27 13:29:04,702 - utils - INFO - Model URL added: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:29:04,702 - utils - INFO - Checking models directory...
2024-10-27 13:29:04,703 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:29:04,704 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:29:04,704 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:29:04,705 - utils - INFO - Downloading model bge-m3-q8_0.gguf from https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf...
2024-10-27 13:29:04,705 - worker - ERROR - Error in download task: name 'urllib' is not defined
2024-10-27 13:29:04,709 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 96, in download_model_task
    _, download_status = download_models()
                         ^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/src/utils.py", line 385, in download_models
    urllib.request.urlretrieve(url, filename)
    ^^^^^^
NameError: name 'urllib' is not defined. Did you forget to import 'urllib'

2024-10-27 13:29:04,711 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (9590f03ebf63e416e75a961d8dab3341)
2024-10-27 13:29:04,711 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:29:32,186 - main - INFO - Application shutdown initiated
2024-10-27 13:29:33,933 - main - INFO - Starting application initialization
2024-10-27 13:29:33,935 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:29:33,939 - db - INFO - Database initialization completed.
2024-10-27 13:29:33,951 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.0111 seconds, for an average of 0.00042692307692307696 seconds per hash.
2024-10-27 13:29:33,951 - utils - INFO - Checking models directory...
2024-10-27 13:29:33,951 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:29:33,951 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:29:33,951 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:29:33,951 - utils - INFO - Downloading model bge-m3-q8_0.gguf from https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf...
2024-10-27 13:30:10,243 - main - INFO - Starting application initialization
2024-10-27 13:30:10,245 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:30:10,249 - db - INFO - Database initialization completed.
2024-10-27 13:30:10,259 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.009908 seconds, for an average of 0.0003810769230769231 seconds per hash.
2024-10-27 13:30:10,259 - utils - INFO - Checking models directory...
2024-10-27 13:30:10,259 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:30:10,259 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:30:10,259 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:30:10,259 - utils - INFO - Downloading model bge-m3-q8_0.gguf from https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf...
2024-10-27 13:30:23,379 - main - INFO - Starting application initialization
2024-10-27 13:30:23,381 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:30:23,386 - db - INFO - Database initialization completed.
2024-10-27 13:30:23,397 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.010289 seconds, for an average of 0.0003957307692307692 seconds per hash.
2024-10-27 13:30:23,397 - utils - INFO - Checking models directory...
2024-10-27 13:30:23,397 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:30:23,397 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:30:23,397 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:30:23,397 - utils - INFO - Downloading model bge-m3-q8_0.gguf from https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf...
2024-10-27 13:30:38,788 - utils - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:30:38,788 - utils - INFO - Model downloads completed.
2024-10-27 13:30:38,803 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:30:38,805 - main - INFO - Application initialization complete
2024-10-27 13:31:35,374 - main - INFO - Application shutdown initiated
2024-10-27 13:32:04,496 - main - INFO - Starting application initialization
2024-10-27 13:32:04,498 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:32:04,504 - db - INFO - Database initialization completed.
2024-10-27 13:32:04,516 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.012042 seconds, for an average of 0.00046315384615384616 seconds per hash.
2024-10-27 13:32:04,516 - utils - INFO - Checking models directory...
2024-10-27 13:32:04,516 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:32:04,516 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:32:04,516 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:32:04,516 - utils - INFO - Model downloads completed.
2024-10-27 13:32:04,523 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:32:04,524 - main - INFO - Application initialization complete
2024-10-27 13:32:19,244 - main - INFO - Processing model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:32:19,249 - main - INFO - Job enqueued successfully. Job ID: 262c48a311a1a65bfd1da073e139e93b
2024-10-27 13:32:19,250 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf')[39;49;00m (262c48a311a1a65bfd1da073e139e93b)
2024-10-27 13:32:19,302 - worker - INFO - Starting download task for model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:32:19,310 - utils - INFO - Model URL not found in database. Adding https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf now...
2024-10-27 13:32:19,311 - utils - INFO - Model URL added: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:32:19,312 - utils - INFO - Checking models directory...
2024-10-27 13:32:19,312 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:32:19,313 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:32:19,314 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:32:19,315 - utils - INFO - Downloading model bge-m3-q8_0.gguf from https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf...
2024-10-27 13:32:19,315 - worker - ERROR - Error in download task: name 'urllib' is not defined
2024-10-27 13:32:19,318 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 96, in download_model_task
    _, download_status = download_models()
                         ^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/src/utils.py", line 385, in download_models
    logger.info(f"Downloading model {model_name_with_extension} from {url}...")
    ^^^^^^
NameError: name 'urllib' is not defined. Did you forget to import 'urllib'

2024-10-27 13:32:19,320 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (262c48a311a1a65bfd1da073e139e93b)
2024-10-27 13:32:19,321 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:34:02,757 - main - INFO - Application shutdown initiated
2024-10-27 13:34:04,629 - main - INFO - Starting application initialization
2024-10-27 13:34:04,631 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 13:34:04,635 - db - INFO - Database initialization completed.
2024-10-27 13:34:04,646 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.010655 seconds, for an average of 0.0004098076923076923 seconds per hash.
2024-10-27 13:34:04,646 - utils - INFO - Checking models directory...
2024-10-27 13:34:04,646 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 13:34:04,646 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 13:34:04,647 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 13:34:04,647 - utils - INFO - Downloading model bge-m3-q8_0.gguf from https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf...
2024-10-27 13:34:19,564 - utils - INFO - Downloaded: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 13:34:19,565 - utils - INFO - Model downloads completed.
2024-10-27 13:34:19,583 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 13:34:19,586 - main - INFO - Application initialization complete
2024-10-27 13:34:19,587 - main - INFO - Processing model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:34:19,594 - main - INFO - Job enqueued successfully. Job ID: 55399fbd4733d60dc8f278f7580eb4a4
2024-10-27 13:34:19,600 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf')[39;49;00m (55399fbd4733d60dc8f278f7580eb4a4)
2024-10-27 13:34:19,648 - worker - INFO - Starting download task for model URL: https://huggingface.co/vonjack/bge-m3-gguf/resolve/main/bge-m3-q8_0.gguf
2024-10-27 13:34:19,650 - utils - INFO - Model URL already exists.
2024-10-27 13:34:19,654 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (55399fbd4733d60dc8f278f7580eb4a4)
2024-10-27 13:34:19,654 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:34:46,440 - main - INFO - Processing model URL: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 13:34:46,444 - main - INFO - Job enqueued successfully. Job ID: f5703c84282ef486a8e27f4c7d1dcefe
2024-10-27 13:34:46,448 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mworker.download_model_task('https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llav...)[39;49;00m (f5703c84282ef486a8e27f4c7d1dcefe)
2024-10-27 13:34:46,476 - worker - INFO - Starting download task for model URL: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 13:34:46,477 - utils - INFO - Model URL not found in database. Adding https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf now...
2024-10-27 13:34:46,478 - utils - INFO - Model URL added: https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 13:34:46,478 - worker - INFO - Downloading model from https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf/resolve/main/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 13:36:44,363 - rq.worker - INFO - [32mmodel_downloads[39;49;00m: [34mJob OK[39;49;00m (f5703c84282ef486a8e27f4c7d1dcefe)
2024-10-27 13:36:44,366 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 13:38:46,383 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:38:46,384 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:38:46,402 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:38:46,411 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:38:46,411 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:38:46,415 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:38:57,127 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:38:57,128 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:38:57,134 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:38:57,156 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 13:38:57,159 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 13:38:57,160 - main - INFO - Processed 1 semantic data types
2024-10-27 13:39:49,134 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:39:49,135 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:39:49,153 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 13:39:49,161 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 13:39:49,161 - main - INFO - Processed 1 semantic data types
2024-10-27 13:39:49,164 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:39:56,178 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 13:39:56,184 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 13:39:56,184 - main - INFO - Processed 1 semantic data types
2024-10-27 13:39:56,187 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 13:39:56,188 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 13:39:56,188 - main - INFO - Processed 1 semantic data types
2024-10-27 13:39:58,684 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:39:58,684 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:39:58,690 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:39:58,697 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:39:58,697 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:39:58,703 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:41:17,593 - main - INFO - Received request to retrieve all stored documents
2024-10-27 13:41:17,594 - main - INFO - Retrieving all stored documents from the database
2024-10-27 13:41:17,605 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 13:41:17,621 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 13:41:17,623 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 13:41:17,623 - main - INFO - Processed 1 semantic data types
2024-10-27 13:43:29,487 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 13:43:29,492 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 13:43:29,493 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 13:56:59,626 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 13:56:59,630 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 13:56:59,631 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 14:10:05,668 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:10:05,669 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:10:05,681 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 14:10:05,693 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 14:10:05,696 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 14:10:05,696 - main - INFO - Processed 1 semantic data types
2024-10-27 14:10:29,714 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 14:10:29,719 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 14:10:29,721 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 14:20:16,639 - main - INFO - Application shutdown initiated
2024-10-27 14:20:18,535 - main - INFO - Starting application initialization
2024-10-27 14:20:18,538 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:20:18,543 - db - INFO - Database initialization completed.
2024-10-27 14:20:18,554 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.011494 seconds, for an average of 0.0004420769230769231 seconds per hash.
2024-10-27 14:20:18,555 - utils - INFO - Checking models directory...
2024-10-27 14:20:18,555 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:20:18,555 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:20:18,555 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:20:18,556 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:20:18,556 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:20:18,556 - utils - INFO - Model downloads completed.
2024-10-27 14:20:18,562 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:20:18,563 - main - INFO - Application initialization complete
2024-10-27 14:20:38,577 - main - INFO - Application shutdown initiated
2024-10-27 14:20:40,260 - main - INFO - Starting application initialization
2024-10-27 14:20:40,262 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:20:40,266 - db - INFO - Database initialization completed.
2024-10-27 14:20:40,277 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.01075 seconds, for an average of 0.00041346153846153844 seconds per hash.
2024-10-27 14:20:40,277 - utils - INFO - Checking models directory...
2024-10-27 14:20:40,277 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:20:40,277 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:20:40,278 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:20:40,278 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:20:40,278 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:20:40,278 - utils - INFO - Model downloads completed.
2024-10-27 14:20:40,284 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:20:40,285 - main - INFO - Application initialization complete
2024-10-27 14:21:03,236 - main - INFO - Application shutdown initiated
2024-10-27 14:21:04,518 - main - INFO - Starting application initialization
2024-10-27 14:21:04,519 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:21:04,523 - db - INFO - Database initialization completed.
2024-10-27 14:21:04,533 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 26; Took 0.009684 seconds, for an average of 0.00037246153846153847 seconds per hash.
2024-10-27 14:21:04,533 - utils - INFO - Checking models directory...
2024-10-27 14:21:04,533 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:21:04,534 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:21:04,534 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:21:04,534 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:21:04,534 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:21:04,534 - utils - INFO - Model downloads completed.
2024-10-27 14:21:04,539 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:21:04,540 - main - INFO - Application initialization complete
2024-10-27 14:21:29,407 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:21:29,408 - main - INFO - Received request to find most similar strings for query text: `my social security number is asdfasdfasdfasd` using model: nomic-embed-text-v1.5.Q6_K
2024-10-27 14:21:29,409 - main - INFO - Computing embedding for input text: my social security number is asdfasdfasdfasd
2024-10-27 14:21:29,409 - functions - INFO - Received request for embedding for 'my social security number is asdfasdfasdfasd' using model 'nomic-embed-text-v1.5.Q6_K' and embedding pooling method 'mean' from IP address '127.0.0.1'
2024-10-27 14:21:29,938 - functions - INFO - Sentence 1 of 1 has 1 embeddings for text 'my social security number is asdfasdfasdfasd...'
2024-10-27 14:21:29,939 - functions - INFO - Calculated 768-dimensional embeddings (relative to the underlying token embedding dimensions of 768) for 1 sentences in a total of 0.5 seconds.
2024-10-27 14:21:29,939 - functions - INFO - That's an average of 484.84 ms per sentence and 2.063 sentences per second (and 0.0908 total characters per ms) using pooling method 'mean'
2024-10-27 14:21:29,939 - functions - INFO - Embedding calculated for 'my social security number is asdfasdfasdfasd' using model 'nomic-embed-text-v1.5.Q6_K' and embedding pooling method 'mean' in 0.00 seconds, or an average of 0.00 seconds per word. Now saving to database...
2024-10-27 14:21:29,940 - main - INFO - Computed embedding for input text: my social security number is asdfasdfasdfasd
2024-10-27 14:21:29,959 - main - INFO - Finished advanced search in 0.558281 seconds. Found 1 results.
2024-10-27 14:23:39,841 - main - INFO - Application shutdown initiated
2024-10-27 14:23:59,785 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 14:23:59,788 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 14:23:59,790 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 14:25:28,909 - main - INFO - Starting application initialization
2024-10-27 14:25:28,912 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:25:28,916 - db - INFO - Database initialization completed.
2024-10-27 14:25:28,927 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 27; Took 0.010884 seconds, for an average of 0.0004031111111111111 seconds per hash.
2024-10-27 14:25:28,927 - utils - INFO - Checking models directory...
2024-10-27 14:25:28,927 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:25:28,927 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:25:28,927 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:25:28,927 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:25:28,928 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:25:28,928 - utils - INFO - Model downloads completed.
2024-10-27 14:25:28,934 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:25:28,935 - main - INFO - Application initialization complete
2024-10-27 14:25:35,198 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:25:35,199 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:25:35,205 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 14:25:35,206 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:25:35,206 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:25:35,209 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 14:25:51,742 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:25:51,743 - main - INFO - Received request to find most similar strings for query text: `my social security number is asdfasdfasdfasd` using model: nomic-embed-text-v1.5.Q6_K
2024-10-27 14:25:51,743 - main - INFO - Computing embedding for input text: my social security number is asdfasdfasdfasd
2024-10-27 14:25:51,743 - functions - INFO - Received request for embedding for 'my social security number is asdfasdfasdfasd' using model 'nomic-embed-text-v1.5.Q6_K' and embedding pooling method 'mean' from IP address '127.0.0.1'
2024-10-27 14:25:51,746 - functions - INFO - Embedding found in database for 'my social security number is asdfasdfasdfasd' using model 'nomic-embed-text-v1.5.Q6_K' and embedding pooling method 'mean'; returning in 0.0026 seconds
2024-10-27 14:25:51,746 - main - INFO - Computed embedding for input text: my social security number is asdfasdfasdfasd
2024-10-27 14:25:51,768 - main - INFO - Finished advanced search in 0.034495 seconds. Found 2 results.
2024-10-27 14:27:39,732 - main - INFO - Application shutdown initiated
2024-10-27 14:27:41,425 - main - INFO - Starting application initialization
2024-10-27 14:27:41,426 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:27:41,431 - db - INFO - Database initialization completed.
2024-10-27 14:27:41,442 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 27; Took 0.011309 seconds, for an average of 0.00041885185185185184 seconds per hash.
2024-10-27 14:27:41,443 - utils - INFO - Checking models directory...
2024-10-27 14:27:41,443 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:27:41,443 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:27:41,443 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:27:41,443 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:27:41,443 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:27:41,443 - utils - INFO - Model downloads completed.
2024-10-27 14:27:41,450 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:27:41,452 - main - INFO - Application initialization complete
2024-10-27 14:33:54,558 - main - INFO - Application shutdown initiated
2024-10-27 14:33:56,246 - main - INFO - Starting application initialization
2024-10-27 14:33:56,247 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:33:56,253 - db - INFO - Database initialization completed.
2024-10-27 14:33:56,263 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 27; Took 0.010254 seconds, for an average of 0.00037977777777777776 seconds per hash.
2024-10-27 14:33:56,263 - utils - INFO - Checking models directory...
2024-10-27 14:33:56,264 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:33:56,264 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:33:56,264 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:33:56,264 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:33:56,264 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:33:56,264 - utils - INFO - Model downloads completed.
2024-10-27 14:33:56,271 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:33:56,273 - main - INFO - Application initialization complete
2024-10-27 14:34:29,227 - main - INFO - Scan job enqueued successfully. Job ID: 8865a9f2760064c83602f8a07232eb6f
2024-10-27 14:34:29,229 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('my passport number is 01123342523524', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'hoeffding_d')[39;49;00m (8865a9f2760064c83602f8a07232eb6f)
2024-10-27 14:34:29,272 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:34:29,275 - worker - ERROR - Error in scan_document_task: 'AdvancedSemanticSearchRequest' object has no attribute 'results'
2024-10-27 14:34:29,279 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 324, in scan_document_task
    results=request.results  # Your search results from the existing implementation
            ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/pydantic/main.py", line 856, in __getattr__
    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')
AttributeError: 'AdvancedSemanticSearchRequest' object has no attribute 'results'

2024-10-27 14:34:29,282 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (8865a9f2760064c83602f8a07232eb6f)
2024-10-27 14:34:29,283 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 14:34:29,289 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 14:34:29,290 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 14:34:29,291 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 14:34:59,300 - main - INFO - Application shutdown initiated
2024-10-27 14:35:00,966 - main - INFO - Starting application initialization
2024-10-27 14:35:00,968 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:35:00,972 - db - INFO - Database initialization completed.
2024-10-27 14:35:00,983 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 27; Took 0.010668 seconds, for an average of 0.0003951111111111111 seconds per hash.
2024-10-27 14:35:00,983 - utils - INFO - Checking models directory...
2024-10-27 14:35:00,983 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:35:00,983 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:35:00,984 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:35:00,984 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:35:00,984 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:35:00,984 - utils - INFO - Model downloads completed.
2024-10-27 14:35:00,991 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:35:00,992 - main - INFO - Application initialization complete
2024-10-27 14:35:08,382 - main - INFO - Scan job enqueued successfully. Job ID: 840da4032432115b620bf10612f5b701
2024-10-27 14:35:08,382 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('my passport number is 01123342523524', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'hoeffding_d')[39;49;00m (840da4032432115b620bf10612f5b701)
2024-10-27 14:35:08,418 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:35:08,421 - worker - ERROR - Error in scan_document_task: 1 validation error for AdvancedSemanticSearchResponse
results
  Field required [type=missing, input_value={'query_text': 'my passpo...pooling_method': 'mean'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing
2024-10-27 14:35:08,424 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 320, in scan_document_task
    return AdvancedSemanticSearchResponse(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/pydantic/main.py", line 212, in __init__
    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pydantic_core._pydantic_core.ValidationError: 1 validation error for AdvancedSemanticSearchResponse
results
  Field required [type=missing, input_value={'query_text': 'my passpo...pooling_method': 'mean'}, input_type=dict]
    For further information visit https://errors.pydantic.dev/2.9/v/missing

2024-10-27 14:35:08,429 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (840da4032432115b620bf10612f5b701)
2024-10-27 14:35:08,430 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 14:35:47,807 - main - INFO - Application shutdown initiated
2024-10-27 14:35:49,482 - main - INFO - Starting application initialization
2024-10-27 14:35:49,485 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:35:49,491 - db - INFO - Database initialization completed.
2024-10-27 14:35:49,502 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 27; Took 0.010949 seconds, for an average of 0.00040551851851851854 seconds per hash.
2024-10-27 14:35:49,502 - utils - INFO - Checking models directory...
2024-10-27 14:35:49,502 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:35:49,502 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:35:49,502 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:35:49,502 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:35:49,503 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:35:49,503 - utils - INFO - Model downloads completed.
2024-10-27 14:35:49,509 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:35:49,510 - main - INFO - Application initialization complete
2024-10-27 14:35:54,825 - main - INFO - Scan job enqueued successfully. Job ID: 6a18b366d13981561c6359059c721f65
2024-10-27 14:35:54,825 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('my passport number is 01123342523524', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'hoeffding_d')[39;49;00m (6a18b366d13981561c6359059c721f65)
2024-10-27 14:35:54,860 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 14:35:54,865 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (6a18b366d13981561c6359059c721f65)
2024-10-27 14:35:54,866 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 14:49:24,991 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 14:49:24,998 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 14:49:25,000 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 14:52:12,860 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:52:12,865 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:52:12,897 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 14:52:12,899 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:52:12,900 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:52:12,911 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 14:52:23,618 - main - INFO - Processing document URL: None
2024-10-27 14:52:23,621 - main - INFO - Job enqueued successfully. Job ID: a5a6ce8d854baeed85e3fdf16b223888
2024-10-27 14:52:23,622 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/a5a6ce8d854baeed85e3fdf16b223888_2019-Sherlock-KDD.pdf', None, None, 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 'records', 'zip', None)[39;49;00m (a5a6ce8d854baeed85e3fdf16b223888)
2024-10-27 14:52:23,658 - worker - INFO - Starting file upload task for file: /tmp/a5a6ce8d854baeed85e3fdf16b223888_2019-Sherlock-KDD.pdf
2024-10-27 14:52:23,660 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:52:23,662 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:52:23,667 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:52:23,673 - main - INFO - Retrieved 4 stored documents from the database
2024-10-27 14:52:23,674 - db - INFO - Database initialization completed.
2024-10-27 14:52:23,685 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 27; Took 0.010577 seconds, for an average of 0.00039174074074074073 seconds per hash.
2024-10-27 14:52:23,686 - utils - INFO - Checking models directory...
2024-10-27 14:52:23,687 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:52:23,694 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:52:23,695 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:52:23,695 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:52:23,696 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:52:23,697 - utils - INFO - Model downloads completed.
2024-10-27 14:52:24,468 - worker - INFO - Successfully saved document and embedding with hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-27 14:52:24,471 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (a5a6ce8d854baeed85e3fdf16b223888)
2024-10-27 14:52:24,471 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 14:52:32,933 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:52:32,933 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:52:32,945 - main - INFO - Retrieved 5 stored documents from the database
2024-10-27 14:52:32,948 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:52:32,948 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:52:32,959 - main - INFO - Retrieved 5 stored documents from the database
2024-10-27 14:54:14,798 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:54:14,799 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:54:14,829 - main - INFO - Retrieved 5 stored documents from the database
2024-10-27 14:54:14,831 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:54:14,831 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:54:14,846 - main - INFO - Retrieved 5 stored documents from the database
2024-10-27 14:54:47,840 - main - INFO - Processing document URL: None
2024-10-27 14:54:47,844 - main - INFO - Job enqueued successfully. Job ID: 43d9c0e1c9466944e0821be41fd3b678
2024-10-27 14:54:47,846 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/43d9c0e1c9466944e0821be41fd3b678_2019-Sherlock-KDD.pdf', None, None, 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 'records', 'zip', None)[39;49;00m (43d9c0e1c9466944e0821be41fd3b678)
2024-10-27 14:54:47,853 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:54:47,857 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:54:47,881 - main - INFO - Retrieved 5 stored documents from the database
2024-10-27 14:54:47,909 - worker - INFO - Starting file upload task for file: /tmp/43d9c0e1c9466944e0821be41fd3b678_2019-Sherlock-KDD.pdf
2024-10-27 14:54:47,911 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:54:47,919 - db - INFO - Database initialization completed.
2024-10-27 14:54:47,931 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 28; Took 0.011271 seconds, for an average of 0.00040253571428571427 seconds per hash.
2024-10-27 14:54:47,932 - utils - INFO - Checking models directory...
2024-10-27 14:54:47,933 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:54:47,940 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:54:47,941 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:54:47,942 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:54:47,943 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:54:47,944 - utils - INFO - Model downloads completed.
2024-10-27 14:54:48,711 - worker - INFO - Successfully saved document and embedding with hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-27 14:54:48,716 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (43d9c0e1c9466944e0821be41fd3b678)
2024-10-27 14:54:48,716 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 14:54:54,219 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:54:54,220 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:54:54,263 - main - INFO - Retrieved 6 stored documents from the database
2024-10-27 14:54:54,264 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:54:54,264 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:54:54,316 - main - INFO - Retrieved 6 stored documents from the database
2024-10-27 14:55:22,679 - main - INFO - Processing document URL: None
2024-10-27 14:55:22,682 - main - INFO - Job enqueued successfully. Job ID: b5443f76cd135daea6ab6ad62c985d32
2024-10-27 14:55:22,683 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/b5443f76cd135daea6ab6ad62c985d32_Hnsw in hnsw - Rust.pdf', None, None, 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 'records', 'zip', None)[39;49;00m (b5443f76cd135daea6ab6ad62c985d32)
2024-10-27 14:55:22,688 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:55:22,692 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:55:22,722 - worker - INFO - Starting file upload task for file: /tmp/b5443f76cd135daea6ab6ad62c985d32_Hnsw in hnsw - Rust.pdf
2024-10-27 14:55:22,723 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 14:55:22,729 - main - INFO - Retrieved 6 stored documents from the database
2024-10-27 14:55:22,731 - db - INFO - Database initialization completed.
2024-10-27 14:55:22,739 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 28; Took 0.007561 seconds, for an average of 0.0002700357142857143 seconds per hash.
2024-10-27 14:55:22,740 - utils - INFO - Checking models directory...
2024-10-27 14:55:22,741 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 14:55:22,742 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 14:55:22,742 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 14:55:22,743 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 14:55:22,744 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 14:55:22,744 - utils - INFO - Model downloads completed.
2024-10-27 14:55:23,478 - worker - INFO - Successfully saved document and embedding with hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-27 14:55:23,481 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (b5443f76cd135daea6ab6ad62c985d32)
2024-10-27 14:55:23,481 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 14:57:30,947 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:57:30,948 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:57:30,994 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 14:57:30,996 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:57:30,997 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:57:31,040 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 14:57:38,665 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-27 14:57:38,673 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-27 14:57:47,350 - main - INFO - Received request to retrieve all stored documents
2024-10-27 14:57:47,350 - main - INFO - Retrieving all stored documents from the database
2024-10-27 14:57:47,382 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 14:57:47,385 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 14:57:47,386 - main - INFO - Processed 1 semantic data types
2024-10-27 14:57:47,387 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 15:02:08,596 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 15:02:08,603 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 15:02:08,606 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 15:08:40,534 - main - INFO - Received request to retrieve all stored documents
2024-10-27 15:08:40,535 - main - INFO - Retrieving all stored documents from the database
2024-10-27 15:08:40,556 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 15:08:40,560 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 15:08:40,561 - main - INFO - Processed 1 semantic data types
2024-10-27 15:08:40,577 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 15:10:34,799 - main - INFO - Received request to retrieve all stored documents
2024-10-27 15:10:34,800 - main - INFO - Retrieving all stored documents from the database
2024-10-27 15:10:34,852 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 15:10:34,854 - main - INFO - Received request to retrieve all stored documents
2024-10-27 15:10:34,854 - main - INFO - Retrieving all stored documents from the database
2024-10-27 15:10:34,889 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 15:11:53,631 - main - INFO - Received request to retrieve all stored documents
2024-10-27 15:11:53,631 - main - INFO - Retrieving all stored documents from the database
2024-10-27 15:11:53,682 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 15:11:53,686 - main - INFO - Received request to retrieve all stored documents
2024-10-27 15:11:53,686 - main - INFO - Retrieving all stored documents from the database
2024-10-27 15:11:53,715 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 15:11:56,535 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-27 15:11:56,537 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-27 15:12:00,999 - main - INFO - Scan job enqueued successfully. Job ID: 11e695176de8c4c8d479a40c43a1a6a6
2024-10-27 15:12:01,001 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'hoeffding_d')[39;49;00m (11e695176de8c4c8d479a40c43a1a6a6)
2024-10-27 15:12:01,065 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-27 15:12:01,088 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (11e695176de8c4c8d479a40c43a1a6a6)
2024-10-27 15:12:01,090 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-27 15:12:16,032 - main - INFO - Received request to retrieve all stored documents
2024-10-27 15:12:16,032 - main - INFO - Retrieving all stored documents from the database
2024-10-27 15:12:16,065 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 15:12:16,070 - main - INFO - Received request to retrieve all stored documents
2024-10-27 15:12:16,071 - main - INFO - Retrieving all stored documents from the database
2024-10-27 15:12:16,102 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 15:18:46,109 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 15:18:46,112 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 15:18:46,113 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 15:32:16,270 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 15:32:16,276 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 15:32:16,279 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 15:45:46,325 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 15:45:46,329 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 15:45:46,331 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 15:59:16,447 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 15:59:16,451 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 15:59:16,453 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 16:12:46,597 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 16:12:46,602 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 16:12:46,604 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 16:26:16,721 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 16:26:16,726 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 16:26:16,728 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 16:39:46,795 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 16:39:46,800 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 16:39:46,802 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 16:53:16,913 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 16:53:16,917 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 16:53:16,920 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 17:06:47,095 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 17:06:47,099 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 17:06:47,101 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 17:20:17,187 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 17:20:17,191 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 17:20:17,193 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 17:33:47,360 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 17:33:47,365 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 17:33:47,367 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 17:47:17,476 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 17:47:17,481 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 17:47:17,483 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 18:00:47,555 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 18:00:47,559 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 18:00:47,562 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 18:14:17,685 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 18:14:17,692 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 18:14:17,694 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 18:27:47,739 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 18:27:47,757 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 18:27:47,759 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 18:41:17,814 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 18:41:17,817 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 18:41:17,819 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 18:48:54,576 - main - INFO - Received request to retrieve all stored documents
2024-10-27 18:48:54,576 - main - INFO - Retrieving all stored documents from the database
2024-10-27 18:48:54,607 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 18:48:54,611 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 18:48:54,612 - main - INFO - Processed 1 semantic data types
2024-10-27 18:48:54,646 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 18:49:00,532 - main - INFO - Received request to retrieve all stored documents
2024-10-27 18:49:00,533 - main - INFO - Retrieving all stored documents from the database
2024-10-27 18:49:00,560 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 18:49:00,562 - main - INFO - Received request to retrieve all stored documents
2024-10-27 18:49:00,562 - main - INFO - Retrieving all stored documents from the database
2024-10-27 18:49:00,586 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 18:49:12,108 - main - INFO - Processing document URL: None
2024-10-27 18:49:12,117 - main - INFO - Job enqueued successfully. Job ID: 3c211b04deb4c9d81a87a59055d9651c
2024-10-27 18:49:12,118 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task(None, None, None, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', None)[39;49;00m (3c211b04deb4c9d81a87a59055d9651c)
2024-10-27 18:49:12,257 - worker - INFO - Starting file upload task for file: None
2024-10-27 18:49:12,258 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-27 18:49:12,266 - db - INFO - Database initialization completed.
2024-10-27 18:49:12,273 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.005748 seconds, for an average of 0.00019820689655172412 seconds per hash.
2024-10-27 18:49:12,273 - utils - INFO - Checking models directory...
2024-10-27 18:49:12,274 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify-api/src/models
2024-10-27 18:49:12,275 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-27 18:49:12,276 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-27 18:49:12,276 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/bge-m3-q8_0.gguf
2024-10-27 18:49:12,276 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify-api/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-27 18:49:12,277 - utils - INFO - Model downloads completed.
2024-10-27 18:49:12,277 - worker - ERROR - Error in upload task: 'NoneType' object has no attribute 'startswith'
2024-10-27 18:49:12,280 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 174, in upload_file_task
    if file_path_or_url.startswith(('http://', 'https://')):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'startswith'

2024-10-27 18:49:12,308 - rq.worker - ERROR - [Job 3c211b04deb4c9d81a87a59055d9651c]: exception raised while executing (worker.upload_file_task)
Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/worker.py", line 1430, in perform_job
    rv = job.perform()
         ^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1280, in perform
    self._result = self._execute()
                   ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/.venv/lib/python3.12/site-packages/rq/job.py", line 1320, in _execute
    coro_result = loop.run_until_complete(result)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/homebrew/Cellar/python@3.12/3.12.7/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py", line 687, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/Users/sidmohan/Desktop/codexify-api/src/worker.py", line 271, in upload_file_task
    if os.path.exists(file_path_or_url) and file_path_or_url.startswith('/tmp/'):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen genericpath>", line 19, in exists
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType

2024-10-27 18:49:18,000 - main - INFO - Received request to retrieve all stored documents
2024-10-27 18:49:18,001 - main - INFO - Retrieving all stored documents from the database
2024-10-27 18:49:18,043 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 18:49:18,045 - main - INFO - Received request to retrieve all stored documents
2024-10-27 18:49:18,045 - main - INFO - Retrieving all stored documents from the database
2024-10-27 18:49:18,093 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 18:52:30,328 - main - INFO - Received request to retrieve all stored documents
2024-10-27 18:52:30,329 - main - INFO - Retrieving all stored documents from the database
2024-10-27 18:52:30,369 - main - INFO - Retrieving all semantic data types from the database
2024-10-27 18:52:30,371 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-27 18:52:30,371 - main - INFO - Processed 1 semantic data types
2024-10-27 18:52:30,379 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 18:52:34,609 - main - INFO - Received request to retrieve all stored documents
2024-10-27 18:52:34,609 - main - INFO - Retrieving all stored documents from the database
2024-10-27 18:52:34,635 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 18:52:34,637 - main - INFO - Received request to retrieve all stored documents
2024-10-27 18:52:34,637 - main - INFO - Retrieving all stored documents from the database
2024-10-27 18:52:34,676 - main - INFO - Retrieved 7 stored documents from the database
2024-10-27 18:55:57,338 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 18:55:57,345 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 18:55:57,348 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 19:09:27,472 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 19:09:27,477 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 19:09:27,479 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 19:22:57,574 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 19:22:57,580 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 19:22:57,584 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 19:36:27,633 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 19:36:27,640 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 19:36:27,642 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 19:49:57,719 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 19:49:57,724 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 19:49:57,725 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 20:03:27,844 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 20:03:27,850 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 20:03:27,852 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 20:16:58,000 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 20:16:58,005 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 20:16:58,006 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 20:30:28,157 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 20:30:28,163 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 20:30:28,164 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 20:43:58,286 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 20:43:58,291 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 20:43:58,292 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 20:57:28,357 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 20:57:28,362 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 20:57:28,364 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 21:10:58,511 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 21:10:58,515 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 21:10:58,517 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 21:24:28,606 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 21:24:28,610 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 21:24:28,611 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 21:37:58,676 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 21:37:58,680 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 21:37:58,682 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 21:51:28,765 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 21:51:28,769 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 21:51:28,770 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 22:04:58,872 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 22:04:58,876 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 22:04:58,877 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 22:18:29,028 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 22:18:29,032 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 22:18:29,034 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 22:31:59,135 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 22:31:59,139 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 22:31:59,141 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 22:45:29,234 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 22:45:29,239 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 22:45:29,240 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 22:58:59,316 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 22:58:59,320 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 22:58:59,322 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 23:12:29,451 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 23:12:29,455 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 23:12:29,457 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 23:25:59,617 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 23:25:59,621 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 23:25:59,624 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 23:39:29,741 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 23:39:29,745 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 23:39:29,747 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-27 23:52:59,828 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-27 23:52:59,831 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-27 23:52:59,832 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 00:06:29,914 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 00:06:29,919 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 00:06:29,923 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 00:20:00,075 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 00:20:00,080 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 00:20:00,084 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 00:33:30,279 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 00:33:30,283 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 00:33:30,285 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 00:47:00,436 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 00:47:00,440 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 00:47:00,442 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 01:00:30,574 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 01:00:30,579 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 01:00:30,581 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 01:14:00,734 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 01:14:00,738 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 01:14:00,739 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 01:27:30,810 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 01:27:30,814 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 01:27:30,816 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 01:41:00,882 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 01:41:00,886 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 01:41:00,888 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 01:54:30,997 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 01:54:31,001 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 01:54:31,003 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 02:08:01,129 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 02:08:01,132 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 02:08:01,135 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 02:21:31,204 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 02:21:31,207 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 02:21:31,209 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 02:35:01,371 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 02:35:01,375 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 02:35:01,377 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 02:48:31,468 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 02:48:31,471 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 02:48:31,472 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 03:02:01,621 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 03:02:01,626 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 03:02:01,627 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 03:15:31,722 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 03:15:31,724 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 03:15:31,725 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 03:29:01,910 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 03:29:01,915 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 03:29:01,917 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 03:42:31,952 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 03:42:31,955 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 03:42:31,956 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 03:56:02,131 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 03:56:02,135 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 03:56:02,137 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 04:09:32,221 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 04:09:32,225 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 04:09:32,227 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 04:23:02,369 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 04:23:02,375 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 04:23:02,378 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 04:36:32,434 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 04:36:32,437 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 04:36:32,439 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 04:50:02,541 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 04:50:02,546 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 04:50:02,548 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 05:03:32,653 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 05:03:32,657 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 05:03:32,659 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 05:17:02,716 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 05:17:02,719 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 05:17:02,721 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 05:30:32,851 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 05:30:32,854 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 05:30:32,856 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 05:44:02,890 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 05:44:02,893 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 05:44:02,895 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 05:57:32,960 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 05:57:32,965 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 05:57:32,966 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 06:11:03,062 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 06:11:03,067 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 06:11:03,072 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 06:24:33,166 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 06:24:33,171 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 06:24:33,173 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 06:38:03,210 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 06:38:03,213 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 06:38:03,214 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 06:51:33,276 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 06:51:33,279 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 06:51:33,281 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 07:05:03,393 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 07:05:03,397 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 07:05:03,400 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 07:18:33,488 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 07:18:33,491 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 07:18:33,492 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 07:32:03,652 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 07:32:03,657 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 07:32:03,659 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 07:45:33,745 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 07:45:33,748 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 07:45:33,749 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 07:59:03,901 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 07:59:03,906 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 07:59:03,909 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 08:12:34,087 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 08:12:34,098 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 08:12:34,100 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 08:26:04,154 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 08:26:04,157 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 08:26:04,162 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 08:39:34,206 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 08:39:34,213 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 08:39:34,216 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 08:53:04,383 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 08:53:04,388 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 08:53:04,390 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 09:00:48,060 - rq.worker - INFO - Worker worker-6971 [PID 6971]: warm shut down requested
2024-10-28 09:00:48,075 - __main__ - INFO - Worker stopped by user
2024-10-28 09:00:48,854 - main - INFO - Application shutdown initiated
2024-10-28 09:00:49,071 - rq.scheduler - INFO - Scheduler stopping, releasing locks for document_scans, model_downloads, file_uploads...
2024-10-28 09:00:49,111 - rq.scheduler - INFO - Scheduler with PID 6973 has stopped
2024-10-28 09:00:49,630 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-6971
2024-10-28 12:34:16,781 - main - INFO - Starting application initialization
2024-10-28 12:34:16,783 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 12:44:11,850 - __main__ - INFO - Initializing worker...
2024-10-28 12:44:11,851 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 12:44:11,864 - __main__ - INFO - Worker started successfully with PID: 46337
2024-10-28 12:44:14,087 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 12:44:14,089 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 12:44:14,089 - __mp_main__ - ERROR - Redis connection error: the greenlet library is required to use this function. No module named 'greenlet'
2024-10-28 12:44:31,830 - __main__ - INFO - Initializing worker...
2024-10-28 12:44:31,830 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 12:44:31,839 - __main__ - INFO - Worker started successfully with PID: 46460
2024-10-28 12:44:34,065 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 12:44:34,067 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 12:44:34,072 - db - INFO - Database initialization completed.
2024-10-28 12:44:34,085 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.013674 seconds, for an average of 0.0004715172413793104 seconds per hash.
2024-10-28 12:44:34,086 - utils - INFO - Checking models directory...
2024-10-28 12:44:34,086 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 12:44:34,086 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 12:44:34,086 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 12:44:34,087 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 12:44:34,087 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 12:44:34,087 - utils - INFO - Model downloads completed.
2024-10-28 12:44:34,093 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 12:44:34,109 - rq.worker - INFO - Worker rq:worker:worker-46460 started with PID 46460, version 1.16.2
2024-10-28 12:44:34,109 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-46460
2024-10-28 12:44:34,114 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 12:44:34,125 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 12:44:34,145 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 12:44:34,146 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 12:44:36,341 - rq.scheduler - INFO - Scheduler for model_downloads, document_scans, file_uploads started with PID 46467
2024-10-28 12:46:58,083 - main - INFO - Starting application initialization
2024-10-28 12:46:58,084 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 12:46:58,090 - db - INFO - Database initialization completed.
2024-10-28 12:46:58,105 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.014955 seconds, for an average of 0.0005156896551724138 seconds per hash.
2024-10-28 12:46:58,106 - utils - INFO - Checking models directory...
2024-10-28 12:46:58,106 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 12:46:58,106 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 12:46:58,106 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 12:46:58,106 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 12:46:58,106 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 12:46:58,106 - utils - INFO - Model downloads completed.
2024-10-28 12:46:58,112 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 12:46:58,113 - main - INFO - Application initialization complete
2024-10-28 12:52:56,147 - main - INFO - Starting application initialization
2024-10-28 12:52:56,148 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 12:52:56,153 - db - INFO - Database initialization completed.
2024-10-28 12:52:56,166 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012349 seconds, for an average of 0.0004258275862068966 seconds per hash.
2024-10-28 12:52:56,166 - utils - INFO - Checking models directory...
2024-10-28 12:52:56,166 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 12:52:56,167 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 12:52:56,167 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 12:52:56,167 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 12:52:56,167 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 12:52:56,167 - utils - INFO - Model downloads completed.
2024-10-28 12:52:56,173 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 12:52:56,174 - main - INFO - Application initialization complete
2024-10-28 12:53:18,756 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:53:18,757 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:53:18,760 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:53:18,761 - main - INFO - Processed 1 semantic data types
2024-10-28 12:53:18,761 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:53:18,761 - main - INFO - Processed 1 semantic data types
2024-10-28 12:53:23,156 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:53:23,157 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:53:23,187 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:53:23,194 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:53:23,196 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:53:23,196 - main - INFO - Processed 1 semantic data types
2024-10-28 12:53:28,576 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:53:28,576 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:53:28,582 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:53:28,582 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:53:28,604 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:53:28,612 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:53:39,601 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 12:53:39,611 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 12:53:43,151 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:53:43,152 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:53:43,153 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:53:43,153 - main - INFO - Processed 1 semantic data types
2024-10-28 12:53:43,154 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:53:43,154 - main - INFO - Processed 1 semantic data types
2024-10-28 12:53:43,759 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:53:43,759 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:53:43,761 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:53:43,761 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:53:43,784 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:53:43,785 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:53:46,497 - main - INFO - Retrieving content for document with document_hash: string
2024-10-28 12:53:46,504 - main - INFO - Retrieved content for document with document_hash: string
2024-10-28 12:53:50,317 - main - INFO - Scan job enqueued successfully. Job ID: fb7bf53e489d8efaa024dd37d11f3c4f
2024-10-28 12:53:50,323 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'hoeffding_d')[39;49;00m (fb7bf53e489d8efaa024dd37d11f3c4f)
2024-10-28 12:53:50,382 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 12:53:50,400 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (fb7bf53e489d8efaa024dd37d11f3c4f)
2024-10-28 12:53:50,401 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 12:54:01,367 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:54:01,367 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:54:01,385 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:54:01,402 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:54:01,404 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:54:01,404 - main - INFO - Processed 1 semantic data types
2024-10-28 12:54:02,731 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:54:02,731 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:54:02,732 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:54:02,732 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:54:02,752 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:54:02,753 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:54:04,323 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:54:04,324 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:54:04,325 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:54:04,325 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:54:04,347 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:54:04,350 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:56:12,934 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:56:12,937 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:56:12,940 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:56:12,941 - main - INFO - Processed 1 semantic data types
2024-10-28 12:56:12,942 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:56:12,942 - main - INFO - Processed 1 semantic data types
2024-10-28 12:56:17,176 - main - INFO - Received request to retrieve all stored documents
2024-10-28 12:56:17,176 - main - INFO - Retrieving all stored documents from the database
2024-10-28 12:56:17,205 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 12:56:17,211 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 12:56:17,214 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 12:56:17,214 - main - INFO - Processed 1 semantic data types
2024-10-28 12:59:23,507 - rq.worker - INFO - Worker worker-46460 [PID 46460]: warm shut down requested
2024-10-28 12:59:23,507 - __main__ - INFO - Worker stopped by user
2024-10-28 12:59:24,079 - rq.scheduler - INFO - Scheduler stopping, releasing locks for model_downloads, document_scans, file_uploads...
2024-10-28 12:59:24,088 - rq.scheduler - INFO - Scheduler with PID 46467 has stopped
2024-10-28 12:59:24,550 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-46460
2024-10-28 13:02:14,204 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:02:14,204 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:02:14,221 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:02:14,224 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:02:14,224 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:02:14,238 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:02:15,589 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:02:15,589 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:02:15,607 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:02:15,617 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:02:15,620 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:02:15,621 - main - INFO - Processed 1 semantic data types
2024-10-28 13:04:06,224 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:04:06,224 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:04:06,245 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:04:06,248 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:04:06,248 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:04:06,262 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:04:12,436 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:04:12,436 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:04:12,452 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:04:12,459 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:04:12,460 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:04:12,460 - main - INFO - Processed 1 semantic data types
2024-10-28 13:09:16,297 - __main__ - INFO - Initializing worker...
2024-10-28 13:09:16,297 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 13:09:16,305 - __main__ - INFO - Worker started successfully with PID: 48495
2024-10-28 13:09:18,448 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 13:09:18,450 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 13:09:18,455 - db - INFO - Database initialization completed.
2024-10-28 13:09:18,467 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012084 seconds, for an average of 0.00041668965517241375 seconds per hash.
2024-10-28 13:09:18,468 - utils - INFO - Checking models directory...
2024-10-28 13:09:18,468 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 13:09:18,468 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 13:09:18,468 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 13:09:18,469 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 13:09:18,469 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 13:09:18,469 - utils - INFO - Model downloads completed.
2024-10-28 13:09:18,474 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 13:09:18,479 - rq.worker - INFO - Worker rq:worker:worker-48495 started with PID 48495, version 1.16.2
2024-10-28 13:09:18,479 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-48495
2024-10-28 13:09:18,481 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 13:09:18,489 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 13:09:18,491 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 13:09:18,492 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 13:09:20,651 - rq.scheduler - INFO - Scheduler for document_scans, model_downloads, file_uploads started with PID 48496
2024-10-28 13:09:28,187 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:09:28,187 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:09:28,195 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:09:28,198 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:09:28,198 - main - INFO - Processed 1 semantic data types
2024-10-28 13:09:28,229 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:10:47,301 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:10:47,301 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:10:47,306 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:10:47,310 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:10:47,310 - main - INFO - Processed 1 semantic data types
2024-10-28 13:10:47,328 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:11:37,610 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:11:37,611 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:11:37,614 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:11:37,616 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:11:37,616 - main - INFO - Processed 1 semantic data types
2024-10-28 13:11:37,637 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:12:47,353 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:12:47,353 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:12:47,364 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:12:47,366 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:12:47,366 - main - INFO - Processed 1 semantic data types
2024-10-28 13:12:47,377 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:12:49,492 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:12:49,495 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:12:49,507 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:12:49,514 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:12:49,514 - main - INFO - Processed 1 semantic data types
2024-10-28 13:12:49,526 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:13:33,816 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:13:33,816 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:13:33,839 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:13:33,841 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:13:33,843 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:13:33,843 - main - INFO - Processed 1 semantic data types
2024-10-28 13:13:35,366 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:13:35,366 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:13:35,380 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:13:35,383 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:13:35,383 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:13:35,399 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:13:38,009 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:13:38,011 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:13:38,011 - main - INFO - Processed 1 semantic data types
2024-10-28 13:13:38,013 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 13:13:38,014 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 13:13:38,014 - main - INFO - Processed 1 semantic data types
2024-10-28 13:17:55,154 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:17:55,155 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:17:55,184 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:17:55,186 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:17:55,186 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:17:55,209 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:22:48,562 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 13:22:48,568 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 13:22:48,569 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 13:36:18,716 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 13:36:18,734 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 13:36:18,736 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 13:42:26,760 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:42:26,761 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:42:26,803 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:42:26,806 - main - INFO - Received request to retrieve all stored documents
2024-10-28 13:42:26,806 - main - INFO - Retrieving all stored documents from the database
2024-10-28 13:42:26,821 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 13:49:48,846 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 13:49:48,849 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 13:49:48,851 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 14:03:18,919 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 14:03:18,923 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 14:03:18,924 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 14:16:49,061 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 14:16:49,064 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 14:16:49,066 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 14:30:19,179 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 14:30:19,183 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 14:30:19,184 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 14:36:04,073 - main - INFO - Processing document URL: None
2024-10-28 14:36:04,076 - main - INFO - Job enqueued successfully. Job ID: 01e2f0cff1333af5c59bc36f54321732
2024-10-28 14:36:04,084 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task(None, None, None, 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 'records', 'zip', None)[39;49;00m (01e2f0cff1333af5c59bc36f54321732)
2024-10-28 14:36:04,170 - worker - INFO - Starting file upload task for file: None
2024-10-28 14:36:04,172 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 14:36:04,191 - db - INFO - Database initialization completed.
2024-10-28 14:36:04,198 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.006425 seconds, for an average of 0.00022155172413793105 seconds per hash.
2024-10-28 14:36:04,199 - utils - INFO - Checking models directory...
2024-10-28 14:36:04,199 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 14:36:04,200 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 14:36:04,201 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 14:36:04,201 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 14:36:04,201 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 14:36:04,202 - utils - INFO - Model downloads completed.
2024-10-28 14:36:04,202 - worker - ERROR - Error in upload task: 'NoneType' object has no attribute 'startswith'
2024-10-28 14:36:04,204 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 174, in upload_file_task
    if file_path_or_url.startswith(('http://', 'https://')):
AttributeError: 'NoneType' object has no attribute 'startswith'

2024-10-28 14:36:04,214 - rq.worker - ERROR - [Job 01e2f0cff1333af5c59bc36f54321732]: exception raised while executing (worker.upload_file_task)
Traceback (most recent call last):
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/worker.py", line 1430, in perform_job
    rv = job.perform()
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/job.py", line 1280, in perform
    self._result = self._execute()
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/job.py", line 1320, in _execute
    coro_result = loop.run_until_complete(result)
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 271, in upload_file_task
    if os.path.exists(file_path_or_url) and file_path_or_url.startswith('/tmp/'):
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/genericpath.py", line 19, in exists
    os.stat(path)
TypeError: stat: path should be string, bytes, os.PathLike or integer, not NoneType

2024-10-28 14:36:14,295 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:36:14,296 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:36:14,322 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 14:36:14,336 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:36:14,336 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:36:14,355 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 14:37:00,422 - main - INFO - Processing document URL: None
2024-10-28 14:37:00,426 - main - INFO - Job enqueued successfully. Job ID: cf0cb8d40b7b5a230f1f269e2ff80001
2024-10-28 14:37:00,427 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mworker.upload_file_task('/tmp/cf0cb8d40b7b5a230f1f269e2ff80001_2019-Sherlock-KDD.pdf', None, None, 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 'records', 'zip', None)[39;49;00m (cf0cb8d40b7b5a230f1f269e2ff80001)
2024-10-28 14:37:00,438 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:37:00,438 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:37:00,466 - worker - INFO - Starting file upload task for file: /tmp/cf0cb8d40b7b5a230f1f269e2ff80001_2019-Sherlock-KDD.pdf
2024-10-28 14:37:00,468 - main - INFO - Retrieved 7 stored documents from the database
2024-10-28 14:37:00,468 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 14:37:00,480 - db - INFO - Database initialization completed.
2024-10-28 14:37:00,488 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.007235 seconds, for an average of 0.0002494827586206897 seconds per hash.
2024-10-28 14:37:00,488 - utils - INFO - Checking models directory...
2024-10-28 14:37:00,489 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 14:37:00,490 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 14:37:00,490 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 14:37:00,490 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 14:37:00,491 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 14:37:00,491 - utils - INFO - Model downloads completed.
2024-10-28 14:37:02,547 - worker - INFO - Successfully saved document and embedding with hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 14:37:02,550 - rq.worker - INFO - [32mfile_uploads[39;49;00m: [34mJob OK[39;49;00m (cf0cb8d40b7b5a230f1f269e2ff80001)
2024-10-28 14:37:02,550 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 14:37:08,788 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:37:08,788 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:37:08,820 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:37:08,823 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:37:08,823 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:37:08,856 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:43:03,129 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:43:03,129 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:43:03,169 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:43:03,172 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:43:03,172 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:43:03,197 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:43:05,136 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 14:43:05,142 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 14:43:14,087 - main - INFO - Retrieving content for document with document_hash: string
2024-10-28 14:43:14,094 - main - INFO - Retrieved content for document with document_hash: string
2024-10-28 14:43:18,022 - main - INFO - Scan job enqueued successfully. Job ID: a9f74e0a3ee656f915b1d8619419262a
2024-10-28 14:43:18,025 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'hoeffding_d')[39;49;00m (a9f74e0a3ee656f915b1d8619419262a)
2024-10-28 14:43:18,069 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 14:43:18,078 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (a9f74e0a3ee656f915b1d8619419262a)
2024-10-28 14:43:18,079 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 14:43:18,084 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 14:43:18,086 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 14:43:18,087 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 14:43:37,693 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:43:37,694 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:43:37,729 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:43:37,741 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:43:37,741 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:43:37,770 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:43:43,147 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 14:43:43,150 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 14:43:43,150 - main - INFO - Processed 1 semantic data types
2024-10-28 14:43:43,152 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 14:43:43,153 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 14:43:43,153 - main - INFO - Processed 1 semantic data types
2024-10-28 14:44:56,458 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:44:56,458 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:44:56,492 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:44:56,494 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:44:56,495 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:44:56,518 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:46:28,156 - main - INFO - Application shutdown initiated
2024-10-28 14:46:31,142 - main - INFO - Starting application initialization
2024-10-28 14:46:31,143 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 14:46:31,147 - db - INFO - Database initialization completed.
2024-10-28 14:46:31,159 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011367 seconds, for an average of 0.0003919655172413793 seconds per hash.
2024-10-28 14:46:31,159 - utils - INFO - Checking models directory...
2024-10-28 14:46:31,159 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 14:46:31,160 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 14:46:31,160 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 14:46:31,160 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 14:46:31,160 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 14:46:31,160 - utils - INFO - Model downloads completed.
2024-10-28 14:46:31,165 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 14:46:31,166 - main - INFO - Application initialization complete
2024-10-28 14:47:19,172 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:47:19,172 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:47:19,209 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 14:47:19,214 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 14:47:19,214 - main - INFO - Processed 1 semantic data types
2024-10-28 14:47:19,219 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:51:45,528 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:51:45,529 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:51:45,561 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:51:45,565 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:51:45,565 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:51:45,602 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:54:06,406 - main - INFO - Retrieving content for document with document_hash: string
2024-10-28 14:54:06,422 - main - INFO - Retrieved content for document with document_hash: string
2024-10-28 14:54:11,924 - main - INFO - Scan job enqueued successfully. Job ID: eb4e55f46f5ace03719221bd04dbcb66
2024-10-28 14:54:11,926 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'hoeffding_d')[39;49;00m (eb4e55f46f5ace03719221bd04dbcb66)
2024-10-28 14:54:11,971 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 14:54:11,978 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (eb4e55f46f5ace03719221bd04dbcb66)
2024-10-28 14:54:11,979 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 14:54:11,984 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 14:54:11,985 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 14:54:11,987 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 14:58:26,066 - main - INFO - Application shutdown initiated
2024-10-28 14:58:29,994 - main - INFO - Starting application initialization
2024-10-28 14:58:29,995 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 14:58:30,010 - db - INFO - Database initialization completed.
2024-10-28 14:58:30,024 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01336 seconds, for an average of 0.0004606896551724138 seconds per hash.
2024-10-28 14:58:30,024 - utils - INFO - Checking models directory...
2024-10-28 14:58:30,024 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 14:58:30,025 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 14:58:30,025 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 14:58:30,025 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 14:58:30,025 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 14:58:30,025 - utils - INFO - Model downloads completed.
2024-10-28 14:58:30,033 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 14:58:30,035 - main - INFO - Application initialization complete
2024-10-28 14:58:30,039 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:58:30,039 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:58:30,109 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:58:30,114 - main - INFO - Received request to retrieve all stored documents
2024-10-28 14:58:30,114 - main - INFO - Retrieving all stored documents from the database
2024-10-28 14:58:30,147 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 14:58:33,923 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 14:58:33,929 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 14:58:38,171 - main - INFO - Scan job enqueued successfully. Job ID: dc01ce2b72def74a12b9248f1aeab6ac
2024-10-28 14:58:38,175 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('', 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'hoeffding_d')[39;49;00m (dc01ce2b72def74a12b9248f1aeab6ac)
2024-10-28 14:58:38,235 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 14:58:38,237 - functions - INFO - Received request for embedding for '' using model 'nomic-embed-text-v1.5.Q6_K' and embedding pooling method 'mean' from IP address 'localhost'
2024-10-28 14:58:38,487 - functions - INFO - Sentence 1 of 1 has 1 embeddings for text '...'
2024-10-28 14:58:38,488 - functions - INFO - Calculated 768-dimensional embeddings (relative to the underlying token embedding dimensions of 768) for 1 sentences in a total of 0.2 seconds.
2024-10-28 14:58:38,489 - functions - INFO - That's an average of 197.07 ms per sentence and 5.074 sentences per second (and 0.0000 total characters per ms) using pooling method 'mean'
2024-10-28 14:58:38,490 - worker - ERROR - Error in scan_document_task: 'NoneType' object has no attribute 'enqueue_write'
2024-10-28 14:58:38,492 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 316, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, None)
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 472, in get_or_compute_embedding
    await db_writer.enqueue_write([embedding_instance])  # Enqueue the write operation using the db_writer instance directly
AttributeError: 'NoneType' object has no attribute 'enqueue_write'

2024-10-28 14:58:38,510 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (dc01ce2b72def74a12b9248f1aeab6ac)
2024-10-28 14:58:38,511 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:00:05,736 - main - INFO - Application shutdown initiated
2024-10-28 15:00:08,845 - main - INFO - Starting application initialization
2024-10-28 15:00:08,846 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:00:08,851 - db - INFO - Database initialization completed.
2024-10-28 15:00:08,862 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.010854 seconds, for an average of 0.00037427586206896556 seconds per hash.
2024-10-28 15:00:08,863 - utils - INFO - Checking models directory...
2024-10-28 15:00:08,863 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:00:08,863 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:00:08,864 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:00:08,864 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:00:08,864 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:00:08,864 - utils - INFO - Model downloads completed.
2024-10-28 15:00:08,869 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:00:08,870 - main - INFO - Application initialization complete
2024-10-28 15:00:11,520 - main - INFO - Starting application initialization
2024-10-28 15:00:11,521 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:00:11,525 - db - INFO - Database initialization completed.
2024-10-28 15:00:11,538 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012017 seconds, for an average of 0.00041437931034482757 seconds per hash.
2024-10-28 15:00:11,538 - utils - INFO - Checking models directory...
2024-10-28 15:00:11,538 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:00:11,538 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:00:11,538 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:00:11,539 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:00:11,539 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:00:11,539 - utils - INFO - Model downloads completed.
2024-10-28 15:00:11,544 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:00:11,546 - main - INFO - Application initialization complete
2024-10-28 15:00:11,553 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:00:11,553 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:00:11,602 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:00:11,604 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:00:11,604 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:00:11,627 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:00:14,394 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:00:14,409 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:02:51,031 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:02:51,032 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:02:51,112 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:02:51,115 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:02:51,116 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:02:51,148 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:02:53,940 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:02:53,955 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:03:01,305 - main - INFO - Scan job enqueued successfully. Job ID: 1c035811a98b004c9b89ff7870298ca4
2024-10-28 15:03:01,307 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (1c035811a98b004c9b89ff7870298ca4)
2024-10-28 15:03:01,363 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:03:01,366 - functions - INFO - Received request for embedding for 'Sherlock: A Deep Learning Approach to
Semantic Data Type Detection

Madelon Hulsebos
MIT Media Lab madelonhulsebos@gmail.com

Arvind Satyanarayan
MIT CSAIL arvindsatya@mit edu

Kevin Hu
MIT Media Lab kzh@mit edu

Tim Kraska
MIT CSAIL kraska@mit edu

Michiel Bakker
MIT Media Lab bakker@mit edu

Çağatay Demiralp
Megagon Labs cagatay@megagon ai

Emanuel Zgraggen
MIT CSAIL emzg@mit edu

César Hidalgo
MIT Media Lab hidalgo@mit edu

Figure 1: Data processing and analysis flow, starting from (1) a corpus of real-world datasets, proceeding to (2) feature extraction, (3) mapping extracted features to ground truth semantic types, and (4) model training and prediction ABSTRACT
Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types We introduce Sherlock, a multi-input deep neural network for detecting semantic types We train Sherlock on 686, 765 data columns retrieved from the VizNet corpus by matching 78 semantic types from DBpedia to column headers We characterize each matched column with 1, 588 features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values Sherlock achieves a support-weighted
F1 score of 0.89, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations CCS CONCEPTS • Computing methodologies → Machine learning; Knowledge representation and reasoning; • Information systems → Data mining Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than the author(s) must be honored Abstracting with credit is permitted To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acm KDD ’19, August 4–8, 2019, Anchorage, AK, USA © 2019 Copyright held by the owner/author(s) Publication rights licensed to ACM ACM ISBN 978-1-4503-6201-6/19/08 $15.00
https://doi org/10.1145/3292500.3330993

KEYWORDS
Tabular data, type detection, semantic types, deep learning

ACM Reference Format:
Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind
Satyanarayan, Tim Kraska, Çağatay Demiralp, and César Hidalgo Sherlock: A Deep Learning Approach to Semantic Data Type Detection In
The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’19), August 4–8, 2019, Anchorage, AK, USA ACM, New York, NY, USA,
9 pages org/10.1145/3292500.3330993

1 INTRODUCTION
Data preparation and analysis systems rely on correctly detecting types of data columns to enable and constrain functionality For example, automated data cleaning facilitates the generation of clean data through validation and transformation rules that depend on data type [15, 26] Schema matching identifies correspondences between data objects, and frequently uses data types to constrain the search space of correspondences [25, 35] Data discovery surfaces data relevant to a given query, often relying on semantic similarities across tables and columns [6, 7] While most systems reliably detect atomic types such as string,
integer, and boolean, semantic types are disproportionately more powerful and in many cases essential Semantic types provide finergrained descriptions of the data by establishing correspondences between columns and real-world concepts and as such, can help with schema matching to determine which columns refer to the same real-world concepts, or data cleaning by determining the conceptual domain of a column In some cases, the detection of a semantic type can be easy For example, an ISBN or credit card number are generated according to strict validation rules, lending themselves to straightforward type detection with just a few rules Semantic Type Detection2 Sampled Dataset and Features1 Training and Testing SetDateCountryNigeria GermanyNameCanadaM Buhari08-18-201806-14-201507-20-2017 A MerkelJ TrudeauUnseenDataModelPredictedTypes andConfidencesLocation: 0.9Name: 0.7Year: 0.8PredictionDateCharacter DistributionsWord EmbeddingsParagraph VectorsGlobal StatisticsCountryName FeatureExtractionJoinJoinTrainingFeature CategoriesTypesNeural NetworkFeaturesExactMatchingColumn Header Column Values Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1500KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al Table 1: Data values sampled from real-world datasets Sampled values

Type location TBA | Chicago, Ill | Nashville, Tenn location UNIVERSITY SUITES | U 27; NA | NORSE HALL location Away | Away | Home | Away | Away date date date name name name

27 Dec 1811 | 1852 | 1855 | - | 1848 | 1871 | 1877
– –, 1922 | – –, 1902 | – –, 1913 | – –, 1919
December 06 | August 23 | None
Svenack | Svendd | Sveneldritch | Svengöran
HOUSE, BRIAN | HSIAO, AMY | HSU, ASTRID
D dunn

But most types, including location, birth date, and name, do not adhere to such structure, as shown in Table 1 Existing open source and commercial systems take matchingbased approaches to semantic type detection For example, regular expression matching captures patterns of data values using predefined character sequences Dictionary approaches use matches between data headers and values with internal look-up tables While sufficient for detecting simple types, these matching-based approaches are often not robust to malformed or dirty data, support only a limited number of types, and under-perform for types without strict validations For example, Figure 2 shows that Tableau detects a column labeled “Continent Name” as string After removing column headers, no semantic types are detected Note that missing headers or incomprehensible headers are not uncommon For example, SAP’s system table T 005 contains country information and column N MF MT is the standard name field, whereas I NTCA refers to the ISO code or X PLZS to zip-code Figure 2: Data types detected by Tableau Desktop 2018.3 for a dataset of country capitals, with and without headers Machine learning models, coupled with large-scale training and benchmarking corpora, have proven effective at predictive tasks across domains Examples include the AlexNet neural network trained on ImageNet for visual recognition and the Google Neural
Machine Translation system pre-trained on WMT parallel corpora for language translation Inspired by these advances, we introduce
Sherlock, a deep learning approach to semantic type detection trained on a large corpus of real-world columns To begin, we consider 78 semantic types described by T2Dv2
Gold Standard,1 which matches properties from the DBpedia ontology with column headers from the WebTables corpus Then, we use exact matching between semantic types and column headers to extract 686, 765 data columns from the VizNet corpus [14], a large-scale repository of real world datasets collected from the web,
popular visualization systems, and open data portals We consider each column as a mapping from column values to a column header We then extract 1, 588 features from each column, describing the distribution of characters, semantic content of words and columns, and global statistics such as cardinality and uniqueness Treating column headers as ground truth labels of the semantic type, we formulate semantic type detection as a multiclass classification problem A multi-input neural network architecture achieves a supportweighted F1-score of 0.89, exceeding that of decision tree and random forest baseline models, two matching-based approaches that represent type detection approaches in practice, and the consensus of crowdsourced annotations We then examine types for which the neural network demonstrates high and low performance, investigate the contribution of each feature category to model performance, extract feature importances from the decision tree baseline,
and present an error-reject curve suggesting the potential of combining learned models with human annotations To conclude, we discuss promising avenues for future research in semantic type detection, such as assessing training data quality at scale, enriching feature extraction processes, and establishing shared benchmarks To support benchmarks for future research and integration into existing systems, we open source our data,
code, and trained model at https://sherlock Key contributions:
(1) Data (§3): Demonstrating a scalable process for matching 686, 675 columns from VizNet corpus for 78 semantic types, then describing with 1, 588 features like word- and paragraph embeddings (2) Model (§4): Formulating type detection as a multiclass classification problem, then contributing a novel multiinput neural network architecture (3) Results (§5): Benchmarking predictive performance against a decision tree and random forest baseline, two matchingbased models, and crowdsourced consensus 2 RELATED WORK
Sherlock is informed by existing commercial and open source systems for data preparation and analysis, as well as prior research work on ontology-based, feature-based, probabilistic, and synthesized approaches to semantic type detection Commercial and open source Semantic type detection enhances the functionality of commercial data preparation and analysis systems such as Microsoft Power BI [20], Trifacta [31], and Google
Data Studio [12] To the best of our knowledge, these commercial tools rely on manually defined regular expression patterns dictionary lookups of column headers and values to detect a limited set of

1http://webdatacommons org/webtables/goldstandardV2 html

Country/RegionStringLatitudeLongitudeCountry/RegionStringStringStringDecimalDecimalStringStringDetected Types Without Column HeadersDetected Types With Column HeadersRemove HeadersResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1501Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

semantic types For instance, Trifacta detects around 10 types (e ,
gender and zip code) and Power BI only supports time-related semantic types (e , date/time and duration) Open source libraries such as messytables [10], datalib [9], and csvkit [13] similarly use heuristics to detect a limited set of types Benchmarking directly against these systems was infeasible due to the small number of supported types and lack of extensibility However, we compare against learned regular expression and dictionary-based benchmarks representative of the approaches taken by these systems Prior research work, with roots in the semantic web and schema matching literature, provide alternative approaches to semantic type detection One body of work leverages existing data on the web, such as WebTables [5], and ontologies (or, knowledge bases) such as DBPedia [2], Wikitology [30], and Freebase [4] [33] construct a database of value-type mappings,
then assign types using a maximum likelihood estimator based on column values [30] use column headers and values to build a Wikitology query, the result of which maps columns to types Informed by these approaches, we looked towards existing ontologies to derive the 275 semantic types considered in this paper Several approaches capture and compare properties of data in a way that is ontology-agnostic [27] use heuristics to first separate numerical and textual types,
then describe those types using the Kolmogorov-Smirnov (K-S)
test and Term Frequency-Inverse Document Frequency (TF-IDF),
respectively [23] use slightly more features, including the Mann-Whitney test for numerical data and Jaccard similarity for textual data, to train logistic regression and random forest models We extend these feature-based approaches with a significantly larger set of features that includes character-level distributions,
word embeddings, and paragraph vectors We leverage orders of magnitude more features and training samples than prior work in order to train a high-capacity machine learning model, a deep neural network We include a decision tree and random forest model as benchmarks to represent these “simpler” machine learning models The third category of prior work employs a probabilistic approach [11] use conditional random fields to predict the semantic type of each value within a column, then combine these predictions into a prediction for the whole column [19] use probabilistic graphical models to annotate values with entities, columns with types, and column pairs with relationships These predictions simultaneously maximize a potential function using a message passing algorithm Probabilistic approaches are complementary to our machine learning-based approach by providing a means for combining column-specific predictions However, as with prior feature-based models, code for retraining these models was not made available for benchmarking Puranik [24] proposes a “specialist approach” combining the predictions of regular expressions, dictionaries, and machine learning models More recently, Yan and He [34] introduced a system that, given a search keyword and set of positive examples, synthesizes type detection logic from open source GitHub repositories This system provides a novel approach to leveraging domain-specific heuristics for parsing, validating, and transforming

semantic data types While both approaches are exciting, the code underlying these systems was not available for benchmarking 3 DATA
We describe the semantic types we consider, how we extracted data columns from a large repository of real-world datasets, and our feature extraction procedure 3.1 Data Collection
Ontologies like WordNet [32] and DBpedia [2] describe semantic concepts, properties of such concepts, and relationships between them To constrain the number of types we consider, we adopt the types described by the T2Dv2 Gold Standard,1 the result of a study matching DBpedia properties [29] with columns from the Web
Tables web crawl corpus [5] These 275 DBpedia properties, such as country, language, and industry, represent semantic types commonly found in datasets scattered throughout the web To expedite the collection of real-world data from diverse sources,
we use the VizNet repository [14], which aggregates and characterizes data from two popular online visualization platforms and open data portals, in addition to the Web Tables corpus For feasibility,
we restricted ourselves to the first 10M Web Tables datasets, but considered the remainder of the repository in its entirety We then match data columns from VizNet that have headers corresponding to our 275 types To accomodate variation in casing and formatting,
single word types matched case-altered modifications (e , name = Name = NAME) and multi-word types included concatenations of constituent words (e , release date = releaseDate) The matching process resulted in 6, 146, 940 columns matching the 275 considered types Manual verification indicated that the majority of columns were plausibly described by the corresponding semantic type, as shown in Table 1 In other words, matching column headers as ground truth labels of the semantic type yielded high quality training data 3.2 Feature Extraction
To create fixed-length representations of variable-length columns,
aid interpretation of results, and provide “hints” to our neural network, we extract features from each column To capture different properties of columns, we extract four categories of features: global statistics (27), aggregated character distributions (960), pretrained word embeddings (200), and self-trained paragraph vectors (400) Global statistics The first category of features describes highlevel statistical characteristics of columns For example, the “column entropy” feature describes how uniformly values are distributed Such a feature helps differentiate between types that contain more repeated values, such as gender, from types that contain many unique values, such as name Other types, like weight and sales,
may consist of many numerical characters, which is captured by the “mean of the number of numerical characters in values ” A complete list of these 27 features can be found in Table 8 in the Appendix Character-level distributions Preliminary analysis indicated that simple statistical features such as the “fraction of values with numerical characters” provide surprising predictive power Motivated

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1502KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al by these results and the prevalence of character-based matching approaches such as regular expressions, we extract features describing the distribution of characters in a column Specifically, we compute the count of all 96 ASCII-printable characters (i , digits, letters,
and punctuation characters, but not whitespace) within each value of a column We then aggregate these counts with 10 statistical functions (i , any, all, mean, variance, min, max, median, sum,
kurtosis, skewness), resulting in 960 features Example features include “whether all values contain a ‘-’ character” and the “mean number of ‘/’ characters ”

Word embeddings For certain semantic types, columns frequently contain commonly occurring words For example, the city type contains values such as New York City, Paris, and London To characterize the semantic content of these values, we used word embeddings that map words to high-dimensional fixed-length numeric vectors In particular, we used a pre-trained GloVe dictionary [22]
containing 50-dimensional representations of 400K English words aggregated from 6B tokens, used for tasks such as text similarity [16] For each value in a column, if the value is a single word,
we look up the word embedding from the GloVe dictionary We omit a term if it does not appear in the GloVe dictionary For values containing multiple words, we looked up each distinct word and represented the value with the mean of the distinct word vectors Then, we computed the mean, mode, median and variance of word vectors across all values in a column Paragraph vectors To represent each column with a fixed-length numerical vector, we implemented the Distributed Bag of Words version of Paragraph Vector (PV-DBOW) [18] Paragraph vectors were originally developed to numerically represent the “topic” of pieces of texts, but have proven effective for more general tasks,
such as document similarity [8] In our implementation, each column is a “paragraph” while values within a column are “words”:
both the entire column and constituent values are represented by one-hot encoded vectors After pooling together all columns across all classes, the training procedure for each column in the same 60% training set used by the main Sherlock model is as follows We randomly select a window of value vectors, concatenate the column vector with the remaining value vectors, then train a single model to predict the former from the latter Using the Gensim library [28], we trained this model for 20 iterations We used the trained model to map each column in both the training and test sets to a 400-dimensional paragraph vector, which provided a balance between predictive power and computational tractability 3.3 Filtering and Preprocessing
Certain types occur more frequently in the VizNet corpus than others For example, description and city are more common than collection and continent To address this heterogeneity,
we limited the number of columns to at most 15K per class and excluded the 10% types containing less than 1K columns Other semantic types, especially those describing numerical concepts, are unlikely to be represented by word embeddings To contend with this issue, we filtered out the types for which at least 15% of the columns did not contain a single word that is present in

the GloVe dictionary This filter resulted in a final total of 686,765
columns corresponding to 78 semantic types, of which a list is included in Table 7 in the Appendix The distribution of number of columns per semantic type is shown in Figure 3 Figure 3: Number of columns per semantic type extracted from VizNet after filtering out the types with more than 15%
of the columns not present in the GloVe dictionary, or with less than 1K columns Before modeling, we preprocess our features by creating an additional binary feature indicating whether word embeddings were successfully extracted for a given column Including this feature results in a total of 1,588 features Then, we impute missing values across all features with the mean of the respective feature 4 METHODS
We describe our deep learning model, random forest baseline, two matching-based benchmarks, and crowdsourced consensus benchmark Then, we explain our training and evaluation procedures 4.1 Sherlock: A Multi-input Neural Network
Prior machine learning approaches to semantic type detection [19,
33] trained simple models, such as logistic regression, on relatively small feature sets We consider a significantly larger number of features and samples, which motivates our use of a feedforward neural network Specifically, given the different number of features and varying noise levels within each feature category, we use a multi-input architecture with hyperparameters shown in Figure 4 At a high-level, we train subnetworks for each feature category except the statistical features, which consist of only 27 features These subnetworks “compress” input features to an output of fixed dimension We chose this dimension to be equal to the number of types in order to evaluate each subnetwork independently Then,
we concatenate the weights of the three output layers with the statistical features to form the input layer of the primary network Each network consists of two hidden layers with rectified linear unit (ReLU) activation functions Experiments with hidden layer sizes between 100 and 1, 000 (i , on the order of the input layer dimension) indicate that hidden layer sizes of 300, 200, and 400
for the character-level, word embedding, and paragraph vector subnetworks, respectively, provides the best results To prevent

DescriptionAlbumWeightRankCityLocationProductGradesPlaysElevationDepthFamilyCollectionBirth placeCapacityContinent02,5005,0007,50010,00012,50015,000Number of SamplesSemantic TypesResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1503Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

overfitting, we included drop out layers and weight decay terms The final class predictions result from the output of the final softmax layer, corresponding to the network’s confidence about a sample belonging to each class, the predicted label then is the class with the highest confidence The neural network, which we refer to as “Sherlock,” is implemented in TensorFlow [1] each type, we collected the 1, 000 most frequently occurring values across all columns, resulting in 78, 000 { value : type } pairs For example, Figure 5 shows examples of entries mapped to the grades type Given an unseen data column at test time, we compare 1, 000
randomly selected column values to each entry of the dictionary,
then classify the column as the most frequently matched type Figure 5: Examples of dictionary entries and a learned regular expression for the grades type Learned regular expressions Regular expressions are frequently used to detect semantic types with common character patterns, such as address, birth date, and year The second matching-based benchmark uses patterns of characters specified by learned regular expressions We learn regular expressions for each type using the evolutionary procedure of Bartoli et al Consistent with the original setup, we randomly sampled 50 “positive values” from each type, and 50 “negative” values from other types An example of a learned regular expression in Java format for the grades type is shown in Figure 5 As with the dictionary benchmark, we match 1, 000 randomly selected values against learned regular expressions,
then use majority vote to determine the final predicted type Crowdsourced annotations To assess the performance of human annotators at predicting semantic type, we conducted a crowdsourced experiment The experiment began by defining the concepts of data and semantic type, then screened out participants unable to select a specified semantic type After the prescreen, participants completed three sets of ten questions separated by two attention checks Each question presented a list of data values, asked “Which one of the following types best describes these data values?”, and required participants to select a single type from a scrolling menu with 78 types Questions were populated from a pool of 780 samples containing 10 randomly selected values from all 78 types We used the Mechanical Turk crowdsourcing platform [17] to recruit 390 participants that were native English speakers and had ≥95% HIT approval rating, ensuring high-quality annotations Participants completed the experiment in 16 minutes and 22 seconds on average and were compensated 2 USD, a rate slightly exceeding the
United States federal minimum wage of 7.25 USD Detailed worker demographics are described in Appendix A.2 Overall, 390 participants annotated 30 samples each, resulting in a total of 11, 700
annotations, or an average of 15 annotations per sample For each sample, we used the most frequent (i , the mode) type from the 15
annotations as the crowdsourced consensus annotation 4.3 Training and Evaluation
To ensure consistent evaluation across benchmarks, we divided the data into 60/20/20 training/validation/testing splits To account for

Figure 4: Architecture of the primary network and its feature-specific subnetworks, and the hyperparameters used for training 4.2 Benchmarks
To measure the relative performance of Sherlock, we compare against four benchmarks Machine learning classifiers The first benchmark is a decision tree, a non-parametric machine learning model with reasonable “out-of-the-box” performance and straightforward interpretation We use the decision tree to represent the simpler models found in prior research, such as the logistic regression used in Pham et al Learning curves indicated that decision tree performance plateaued beyond a depth of 50, which we then used as the maximum depth We also add a random forest classifier we built from 10 such trees, which often yields significantly better performance For all remaining parameters, we used the default settings in the scikit-learn package [21] Dictionaries are commonly used to detect semantic types that contain a finite set of valid values, such as country,
day, and language The first matching-based benchmark is a dictionary that maps column values or headers to semantic types For

Primary NetworkBatch Norm(size=128)ConcatenateReLU (500 units)ReLU (500 units)Output (78 units)SoftmaxInput FeaturesReLU (x units)Batch NormDropoutReLU (x units)Output (78 units)SoftmaxFeature-specificSubnetworkDropout(rate=0.3)OutputCharacterOutputWordOutputParagraphStatisticalMetricAccuracyLoss FunctionCross-EntropyOptimizerAdamEpochs100Early Stopping Patience5HyperparametersLearning Rate1e-4Weight Decay Rate1e-4Dictionary Entries (20 out of 1000)Learned Regular Expression\w\w \-(?: \w\w)*+|[06PK][A-Za-z]*+\-\w|\w\w\w\w\w\w \w\w \w\w\w \w\w9-12K-5PK - 0509 - 12KG - 05PRESCHOOL-56-8KG-0606 - 08PK -PRESCHOOL-8PK - 8KG - 12K-806 - 12K-^KG - 08- 12PK - 12PK - 08Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1504KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al class imbalances, we evaluate model performance using the average
F1-score = 2 × (precision × recall)/(precision + recall), weighted by the number of columns per class in the test set (i To estimate the mean and 95% percentile error of the crowdsourced consensus F1 score, we conducted 105 bootstrap simulations by resampling annotations for each sample with replacement Computational effort and space required at prediction time are also important metrics for models incorporated into user-facing systems We measure the average time in seconds needed to extract features and generate a prediction for a single sample, and report the space required by the models in megabytes 5 RESULTS
We report the performance of our multi-input neural network and compare against benchmarks Then, we examine types for which
Sherlock demonstrated high and low performance, the contribution of each feature category in isolation, decision tree feature importances, and the effect of rejection threshold on performance 5.1 Benchmark Results
We compare Sherlock against decision tree, random forest, dictionarybased, learned regular expression, and crowdsourced consensus benchmarks Table 2 presents the F1 score weighted by support,
runtime in seconds per sample, and size in megabytes of each model Table 2: Support-weighted F1 score, runtime at prediction,
and size of Sherlock and four benchmarks Method

F1 Score Runtime (s)

Size (Mb)

Sherlock
Decision tree
Random forest

Machine Learning 0.89
0.76
0.84

0.42 (±0.01)
0.26 (±0.01)
0.26 (±0.01)

Dictionary
Regular expression

Matching-based 0.16
0.04
Crowdsourced Annotations

0.01 (±0.03)
0.01 (±0.03)

Consensus

0.32 (±0.02)

33.74 (±0.86)

6.2
59.1
760.4

0.5
0.01

−

that annotating semantic types with a large number of types is a challenging and ambiguous task Comparing the machine learning models, Sherlock significantly outperforms the decision tree baseline, while the random forest classifier is competitive For cases in which interpretability of features and predictions are important considerations, the tree-based benchmarks may be a suitable choice of model Despite poor predictive performance, matching-based benchmarks are significantly smaller and faster than both machine learning models For cases in which absolute runtime and model size are critical, optimizing matching-based models may be a worthwhile approach This trade-off also suggests a hybrid approach of combining matching-based models for “easy” types with machine learning models for more ambiguous types 5.2 Performance for Individual Types
Table 3 displays the top and bottom five types, as measured by the
F1 score achieved by Sherlock for that type High performing types such as grades and industry frequently contain a finite set of valid values, as shown in Figure 5 for grades Other types such as birth date and ISBN, often follow consistent character patterns,
as shown in Table 1 Table 3: Top five and bottom five types by F1 score Type

F1 Score Precision

Recall

Support

Grades
ISBN
Birth Date
Industry
Affiliation

Brand
Person
Director
Sales
Ranking

Top 5 Types

0.991
0.986
0.970
0.968
0.961

0.989
0.981
0.965
0.947
0.966

Bottom 5 Types 0.760
0.654
0.700
0.568
0.612

0.685
0.630
0.537
0.514
0.468

0.994
0.992
0.975
0.989
0.956

0.623
0.608
0.436
0.469
0.349

1765
1430
479
2958
1768

574
579
225
322
439

We first note that the machine learning models significantly outperform the matching-based and crowdsourced consensus benchmarks, in terms of F1 score The relatively low performance of crowdsourced consensus is perhaps due to the visual overload of selecting from 78 types, such that performance may increase with a smaller number of candidate types Handling a large number of candidate classes is a benefit of using an ML-based or matching-based model Alternatively, crowdsourced workers may have difficulties differentiating between classes that are unfamiliar or contain many numeric values Lastly, despite our implementing basic training and honeypot questions, crowdsourced workers will likely improve with longer training times and stricter quality control Inspection of the matching-based benchmarks suggests that dictionaries and learned regular expressions are prone to “overfitting”
on the training set Feedback from crowdsourced workers suggests

Table 4: Examples of low precision and low recall types Examples

True type Predicted type

81, 13, 3, 1
316, 481, 426, 1, 223
$, $$, $$$, $$$$, $$$$$

#1, #2, #3, #4, #5, #6
3, 6, 21, 34, 29, 36, 54
1st, 2nd, 3rd, 4th, 5th

Low Precision
Rank
Plays
Symbol

Low Recall

Sales
Sales
Sales

Ranking
Ranking
Ranking

Rank
Plays
Position

To understand types for which Sherlock performs poorly, we include incorrectly predicted examples for the lowest precision

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1505Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

type (sales) and the lowest recall type (ranking) in Table 4 From the three examples incorrectly predicted as sales, we observe that purely numerical values or values appearing in multiple classes (e ,
currency symbols) present a challenge to type detection systems From the three examples of incorrectly predicted ranking columns,
we again note the ambiguity of numerical values 5.3 Contribution by Feature Category
We trained feature-specific subnetworks in isolation and report the F1 scores in Table 5 Word embedding, character distribution,
and paragraph vector feature sets demonstrate roughly equal performance to each other, and significantly above that of the global statistics features, though this may be due to fewer features Each feature set in isolation performs significantly worse than the full model, supporting our combining of each feature set Table 6: Top-10 features for the decision tree model “Score”
denotes normalized gini impurity (a) Top-10 global statistics features (out of 27) Rank Feature Name

Fraction of Cells with Numeric Characters

1 Number of Values 2 Maximum Value Length 3 Mean # Alphabetic Characters in Cells 4
5 Column Entropy 6
7 Number of None Values 8 Mean Length of Values 9
10 Mean # of Numeric Characters in Cells

Proportion of Unique Values

Fraction of Cells with Alphabetical Characters

Score 1.00
0.79
0.43
0.38
0.35
0.33
0.33
0.28
0.22
0.16

Table 5: Performance contribution of isolated feature sets (b) Top-10 character-level distribution features (out of 960) Feature set
Word embeddings
Character distributions
Paragraph vectors
Global statistics

Num Features 201
960
400
27

F1 Score 0.79
0.78
0.73
0.25

5.4 Feature Importances
We measure feature importance by the total reduction of the Gini impurity criterion brought by that feature to the decision tree model The top 10 most important features from the global statistics and character-level distributions sets are shown in Table 6 While word embedding and paragraph vector features are important, they are difficult to interpret and are therefore omitted Inspecting Table 6a, we find that the “number of values” in a column is the most important feature Certain classes like name and requirements tended to contain fewer values, while others like year and family contained significantly more values The second most important feature is the “maximum value length” in characters, which may differentiate classes with long values, such as address and description, from classes with short values, such as gender and year The top character-level distribution features in Table 6b suggest the importance of specific characters for differentiating between types The third most important feature, the “minimum number of ‘-’ characters”, likely helps determine datetime-related types The fifth most important feature, “whether all values have a ‘,’ character” may also distinguish datetime-related or name-related types Further study of feature importances for semantic type detection is a promising direction for future research 5.5 Rejection Curves
Given unseen data values, Sherlock assesses the probability of those values belonging to each type, then predicts the type with the highest probability Interpreting probabilities as a measure of confidence,
we may want to only label samples with high confidence of belonging to a type To understand the effect of confidence threshold on

Rank Feature Name

Skewness of ‘,’

Sum of ‘D’ across values

1
2 Mean number of ‘M’
3 Minimum number of ‘-’
4
5 Whether all values have a ‘,’
6 Maximum number of ‘g’
7
Skewness of ‘]’
8 Mean number of ‘,’
9 Mean number of ‘z’
10

Sum of ‘n’

Score 1.00
0.77
0.69
0.59
0.47
0.45
0.45
0.40
0.37
0.36

Figure 6: Rejection curves showing performance while rejecting all but the top x% highest confidence samples predictive performance, we present the error-rejection curves of
Sherlock and the decision tree model in Figure 6 By introducing a rejection threshold of 10% of the samples, Sherlock reaches an F1 score of ∼0.95 This significant increase in predictive performance suggests a hybrid approach in which low confidence samples are manually annotated Note that the higher rejection threshold, the lower the error we make in predicting labels,
at the cost of needing more expert capacity 0.00.10.20.30.40.50.60.70.80.91.0Fraction of Samples Rejected0.800.850.900.951.00F1 Score Weighted by SupportNeural Network (Sherlock)Random Forest BaselineModelResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1506KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al 6 DISCUSSION
We began by considering a set of semantic types described by prior work that identifies correspondences between DBPedia [2]
and WebTables [5] Then, we constructed a dataset consisting of matches between those types with columns in the VizNet [14]
corpus Inspection of these columns suggests that such an approach yields training samples with few false positives After extracting four categories of features describing the values of each column,
we formulate type detection as a multiclass classification task A multi-input neural network demonstrates high predictive performance at the classification task compared to machine learning,
matching-based, and crowdsourced benchmarks We note that using real-world data provides the examples needed to train models that detect many types, at scale We also observe that the test examples frequently include dirty (e , missing or malformed) values, which suggests that real-world data also affords a degree of robustness Measuring and operationalizing these two benefits, especially with out-of-distribution examples, is a promising direction of research Developers have multiple avenues to incorporating ML-based semantic type detection approaches into systems To support the use of Sherlock “out-of-the-box,” we distribute Sherlock as a Python library3 that can be easily installed and incorporated into existing codebases For developers interested in a different set of semantic types, we open source our training and analysis scripts.2 The repository also supports developers wishing to retrain Sherlock using data from their specific data ecologies, such as enterprise or research settings with domain-specific data To close, we identify four promising avenues for future research:
(1) enhancing the quantity and quality of the training data, (2)
increasing the number of considered types, (3) enriching the set of features extracted from each column, and (4) developing shared benchmarks Enhancing data quantity and quality Machine learning model performance is limited by the number of training examples Sherlock is no exception Though the VizNet corpus aggregates datasets from four sources, there is an opportunity to incorporate training examples from additional sources, such as Kaggle,2 datasets included alongside the R statistical environment,3 and the ClueWeb web crawl of Excel spreadsheets.4 We expect increases in training data diversity to improve the robustness and generalizability of
Sherlock Model predictions quality is further determined by the correspondence between training data and unseen testing data, such as datasets uploaded by analysts to a system Our method of matching semantic types with columns from real-world data repositories affords both the harvesting of training samples at scale and the ability to use aspects of dirty data, such as the number of missing values,
as features While we verified the quality of training data through manual inspection, there is an opportunity to label data quality at scale by combining crowdsourcing with active learning By assessing the quality of each training dataset, such an approach would support training semantic type detection models with completely “clean” data at scale com/datasets 3https://github com/vincentarelbundock/Rdatasets 4http://lemurproject php

Increasing number of semantic types To ground our approach in prior work, this paper considered 78 semantic types described by the T2Dv2 Gold Standard While 78 semantic types is a substantial increase over what is supported in existing systems, it is a small subset of entities from existing knowledge bases: the DBPedia ontology [2] covers 685 classes, WordNet [32] contains 175K synonym sets, and Knowledge Graph5 contains millions of entities The entities within these knowledge bases, and hierarchical relationships between entities, provide an abundance of semantic types In lieu of a relevant ontology, researchers can count frequency of column headers in available data to determine which semantic types to consider Such a data-driven approach would ensure the maximum number of training samples for each semantic type Additionally, these surfaced semantic types are potentially more specific to usecase and data ecology, such as data scientists integrating enterprise databases within a company Enriching feature extraction We incorporate four categories of features that describe different aspects of column values A promising approach is to include features that describe relationships between columns (e , correlation, number of overlapping values,
and name similarity), aspects of the entire dataset (e , number of columns), and source context (e , webpage title for scraped tables) Additionally, while we used features to aid interpretation of results,
neural networks using raw data as input are a promising direction of research For example, a character-level recurrent neural network could classify concatenated column values Developing shared benchmarks Despite rich prior research in semantic type detection, we could not find a benchmark with publicly available code that accommodates a larger set of semantic types We therefore incorporated benchmarks that approximated stateof-the-art data systems, to the best of our knowledge However,
domains such as image classification and language translation have benefited from shared benchmarks and test sets Towards this end,
we hope that open-sourcing the data and code used in this paper can benefit future research 7 CONCLUSION
Correctly detecting semantic types is critical to many important data science tasks Machine learning models coupled with largescale data repositories have demonstrated success across domains,
and suggest a promising approach to semantic type detection Sherlock provides a step forward towards this direction REFERENCES [1] Martín Abadi et al TensorFlow: A system for large-scale machine learning In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16) [2] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
and Zachary Ives DBpedia: A nucleus for a web of open data [3] Alberto Bartoli, Andrea De Lorenzo, Eric Medvet, and Fabiano Tarlao Inference of regular expressions for text extraction from examples IEEE Transactions on Knowledge and Data Engineering 28, 5 (2016), 1217–1230 [4] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor Freebase: A Collaboratively Created Graph Database for Structuring Human
Knowledge In Proceedings of the 2008 ACM SIGMOD International Conference on
Management of Data (SIGMOD ’08) ACM, New York, NY, USA, 1247–1250 5https://developers com/knowledge-graph

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1507Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

[5] Michael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang WebTables: Exploring the Power of Tables on the Web org/10.14778/1453856.1453916
[6] Raul Castro Fernandez, Ziawasch Abedjan, Famien Koko, Gina Yuan, Samuel
Madden, and Michael Stonebraker Aurum: A Data Discovery System [7] Raul Castro Fernandez, Essam Mansour, Abdulhakim Qahtan, Ahmed Elmagarmid, Ihab Ilyas, Samuel Madden, Mourad Ouzzani, Michael Stonebraker, and
Nan Tang Seeping Semantics: Linking Datasets Using Word Embeddings for Data Discovery org/10.1109/ICDE.2018.00093

[8] Andrew M Dai, Christopher Olah, and Quoc V Le Document embedding

[34] Cong Yan and Yeye He Synthesizing type-detection logic for rich semantic data types using open-source code In Proceedings of the 2018 International
Conference on Management of Data [35] Benjamin Zapilko, Matthäus Zloch, and Johann Schaible Utilizing Regular
Expressions for Instance-Based Schema Matching CEUR Workshop Proceedings 946 A APPENDIX
A.1 Supplemental Tables

http://vega Table 7: 78 semantic types included in this study with paragraph vectors arXiv preprint arXiv:1507.07998 (2015) [9] Interactive Data Lab Datalib: JavaScript Data Utilities io/datalib

[10] Open Knowledge Foundation messytables · PyPi org/

project/messytables

[11] Aman Goel, Craig A Knoblock, and Kristina Lerman Exploiting structure within data for accurate labeling using conditional random fields In Proceedings on the International Conference on Artificial Intelligence (ICAI) Google Data Studio https://datastudio google.com [13] Christopher Groskopf and contributors readthedocs.org

[14] Kevin Hu, Neil Gaikwad, Michiel Bakker, Madelon Hulsebos, Emanuel Zgraggen,
César Hidalgo, Tim Kraska, Guoliang Li, Arvind Satyanarayan, and Çağatay
Demiralp VizNet: Towards a large-scale visualization learning and benchmarking repository In Proceedings of the 2019 Conference on Human Factors in
Computing Systems (CHI) [15] Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer Wrangler: Interactive Visual Specification of Data Transformation Scripts In ACM
Human Factors in Computing Systems (CHI) [16] Tom Kenter and Maarten De Rijke Short text similarity with word embeddings In Proceedings of the 24th ACM international on conference on information and knowledge management [17] Aniket Kittur, Ed H Chi, and Bongwon Suh Crowdsourcing User Studies with Mechanical Turk In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’08) ACM, New York, NY, USA, 453–456 [18] Quoc Le and Tomas Mikolov Distributed representations of sentences and documents In International Conference on Machine Learning [19] Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti Annotating and searching web tables using entities, types and relationships Proceedings of the
VLDB Endowment 3, 1-2 (2010), 1338–1347 Power BI | Interactive Data Visualization BI microsoft.com

[21] Fabian Pedregosa et al Scikit-learn: Machine Learning in Python Journal

of Machine Learning Research 12 (2011), 2825–2830 [22] Jeffrey Pennington, Richard Socher, and Christopher Manning Glove:
Global vectors for word representation In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [23] Minh Pham, Suresh Alse, Craig A Knoblock, and Pedro Szekely Semantic labeling: a domain-independent approach In International Semantic Web Conference Springer, 446–462 [24] Nikhil Waman Puranik A Specialist Approach for Classification of Column

Data University of Maryland, Baltimore County [25] Erhard Rahm and Philip A A Survey of Approaches to Automatic

Schema Matching The VLDB Journal 10, 4 (Dec [26] Vijayshankar Raman and Joseph M Potter’s Wheel: An Interactive Data Cleaning System In Proceedings of the 27th International Conference on Very Large Data Bases (VLDB ’01) Morgan Kaufmann Publishers Inc , San
Francisco, CA, USA, 381–390 [27] S Krishnamurthy Ramnandan, Amol Mittal, Craig A Knoblock, and Pedro Szekely Assigning semantic labels to data sources In European Semantic Web
Conference Springer, 403–417 [28] Radim Řehůřek and Petr Sojka Software Framework for Topic Modelling with Large Corpora In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks [29] Dominique Ritze and Christian Bizer Matching web tables to DBpedia – a

feature utility study context 42, 41 (2017), 19 [30] Zareen Syed, Tim Finin, Varish Mulwad, Anupam Joshi, et al Exploiting a web of semantic data for interpreting tables In Proceedings of the Second Web
Science Conference Data Wrangling Tools & Software trifacta.com [32] Princeton University edu

[33] Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paşca, Warren Shen, Fei
Wu, Gengxin Miao, and Chung Wu Recovering semantics of tables on the web Proceedings of the VLDB Endowment 4, 9 (2011), 528–538 Semantic Types

Code
Collection
Command
Company
Component
Continent
Country
County
Creator
Credit
Currency
Day
Depth
Description Manufacturer

Education
Elevation
Family
File size
Format
Gender
Genre
Grades
Industry
ISBN
Jockey
Language
Location

Address
Affiliate
Affiliation
Age
Album
Area
Artist
Birth date
Birth place
Brand
Capacity
Category
City
Class
Classification Director
Duration
Club

Name
Nationality

Requirement
Result
Sales
Service
Sex
Species
State
Status
Symbol
Team
Team name
Type
Weight
Year

Notes
Operator
Order
Organisation
Origin
Owner
Person
Plays
Position
Product
Publisher
Range
Rank
Ranking
Region
Religion

Table 8: Description of the 27 global statistical features Asterisks (*) denote features included in Venetis et al Feature description
Number of values Fraction of values with unique content *
Fraction of values with numerical characters *
Fraction of values with alphabetical characters of the number of numerical characters in values of the number of alphabetical characters in values of the number special characters in values of the number of words in values *
{Percentage, count, only/has-Boolean} of the None values {Stats, sum, min, max, median, mode, kurtosis, skewness,
any/all-Boolean} of length of values A.2 Mechanical Turk Demographics
Of the 390 participants, 57.18% were male and 0.43% female 1.5%
completed some high school without attaining a diploma, while others had associates (10.5%), bachelor’s (61.0%), master’s (13.1%), or doctorate or professional degree (1.8%) in addition to a high school diploma (12.3%) 26.4% of participants worked with data daily, 33.1%
weekly, 17.2% monthly, and 11.0% annually, while 12.3% never work with data In terms of age: 10.0% of participants were between 18-23,
24-34 (60.3%), 35-40 (13.3%), 41-54 (12.6%), and above 55 (3.8%) Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1508 Sherlock: A Deep Learning Approach to
Semantic Data Type Detection

Madelon Hulsebos
MIT Media Lab madelonhulsebos@gmail.com

Arvind Satyanarayan
MIT CSAIL arvindsatya@mit edu

Kevin Hu
MIT Media Lab kzh@mit edu

Tim Kraska
MIT CSAIL kraska@mit edu

Michiel Bakker
MIT Media Lab bakker@mit edu

Çağatay Demiralp
Megagon Labs cagatay@megagon ai

Emanuel Zgraggen
MIT CSAIL emzg@mit edu

César Hidalgo
MIT Media Lab hidalgo@mit edu

Figure 1: Data processing and analysis flow, starting from (1) a corpus of real-world datasets, proceeding to (2) feature extraction, (3) mapping extracted features to ground truth semantic types, and (4) model training and prediction ABSTRACT
Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types We introduce Sherlock, a multi-input deep neural network for detecting semantic types We train Sherlock on 686, 765 data columns retrieved from the VizNet corpus by matching 78 semantic types from DBpedia to column headers We characterize each matched column with 1, 588 features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values Sherlock achieves a support-weighted
F1 score of 0.89, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations CCS CONCEPTS • Computing methodologies → Machine learning; Knowledge representation and reasoning; • Information systems → Data mining Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than the author(s) must be honored Abstracting with credit is permitted To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acm KDD ’19, August 4–8, 2019, Anchorage, AK, USA © 2019 Copyright held by the owner/author(s) Publication rights licensed to ACM ACM ISBN 978-1-4503-6201-6/19/08 $15.00
https://doi org/10.1145/3292500.3330993

KEYWORDS
Tabular data, type detection, semantic types, deep learning

ACM Reference Format:
Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind
Satyanarayan, Tim Kraska, Çağatay Demiralp, and César Hidalgo Sherlock: A Deep Learning Approach to Semantic Data Type Detection In
The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’19), August 4–8, 2019, Anchorage, AK, USA ACM, New York, NY, USA,
9 pages org/10.1145/3292500.3330993

1 INTRODUCTION
Data preparation and analysis systems rely on correctly detecting types of data columns to enable and constrain functionality For example, automated data cleaning facilitates the generation of clean data through validation and transformation rules that depend on data type [15, 26] Schema matching identifies correspondences between data objects, and frequently uses data types to constrain the search space of correspondences [25, 35] Data discovery surfaces data relevant to a given query, often relying on semantic similarities across tables and columns [6, 7] While most systems reliably detect atomic types such as string,
integer, and boolean, semantic types are disproportionately more powerful and in many cases essential Semantic types provide finergrained descriptions of the data by establishing correspondences between columns and real-world concepts and as such, can help with schema matching to determine which columns refer to the same real-world concepts, or data cleaning by determining the conceptual domain of a column In some cases, the detection of a semantic type can be easy For example, an ISBN or credit card number are generated according to strict validation rules, lending themselves to straightforward type detection with just a few rules Semantic Type Detection2 Sampled Dataset and Features1 Training and Testing SetDateCountryNigeria GermanyNameCanadaM Buhari08-18-201806-14-201507-20-2017 A MerkelJ TrudeauUnseenDataModelPredictedTypes andConfidencesLocation: 0.9Name: 0.7Year: 0.8PredictionDateCharacter DistributionsWord EmbeddingsParagraph VectorsGlobal StatisticsCountryName FeatureExtractionJoinJoinTrainingFeature CategoriesTypesNeural NetworkFeaturesExactMatchingColumn Header Column Values Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1500KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al Table 1: Data values sampled from real-world datasets Sampled values

Type location TBA | Chicago, Ill | Nashville, Tenn location UNIVERSITY SUITES | U 27; NA | NORSE HALL location Away | Away | Home | Away | Away date date date name name name

27 Dec 1811 | 1852 | 1855 | - | 1848 | 1871 | 1877
– –, 1922 | – –, 1902 | – –, 1913 | – –, 1919
December 06 | August 23 | None
Svenack | Svendd | Sveneldritch | Svengöran
HOUSE, BRIAN | HSIAO, AMY | HSU, ASTRID
D dunn

But most types, including location, birth date, and name, do not adhere to such structure, as shown in Table 1 Existing open source and commercial systems take matchingbased approaches to semantic type detection For example, regular expression matching captures patterns of data values using predefined character sequences Dictionary approaches use matches between data headers and values with internal look-up tables While sufficient for detecting simple types, these matching-based approaches are often not robust to malformed or dirty data, support only a limited number of types, and under-perform for types without strict validations For example, Figure 2 shows that Tableau detects a column labeled “Continent Name” as string After removing column headers, no semantic types are detected Note that missing headers or incomprehensible headers are not uncommon For example, SAP’s system table T 005 contains country information and column N MF MT is the standard name field, whereas I NTCA refers to the ISO code or X PLZS to zip-code Figure 2: Data types detected by Tableau Desktop 2018.3 for a dataset of country capitals, with and without headers Machine learning models, coupled with large-scale training and benchmarking corpora, have proven effective at predictive tasks across domains Examples include the AlexNet neural network trained on ImageNet for visual recognition and the Google Neural
Machine Translation system pre-trained on WMT parallel corpora for language translation Inspired by these advances, we introduce
Sherlock, a deep learning approach to semantic type detection trained on a large corpus of real-world columns To begin, we consider 78 semantic types described by T2Dv2
Gold Standard,1 which matches properties from the DBpedia ontology with column headers from the WebTables corpus Then, we use exact matching between semantic types and column headers to extract 686, 765 data columns from the VizNet corpus [14], a large-scale repository of real world datasets collected from the web,
popular visualization systems, and open data portals We consider each column as a mapping from column values to a column header We then extract 1, 588 features from each column, describing the distribution of characters, semantic content of words and columns, and global statistics such as cardinality and uniqueness Treating column headers as ground truth labels of the semantic type, we formulate semantic type detection as a multiclass classification problem A multi-input neural network architecture achieves a supportweighted F1-score of 0.89, exceeding that of decision tree and random forest baseline models, two matching-based approaches that represent type detection approaches in practice, and the consensus of crowdsourced annotations We then examine types for which the neural network demonstrates high and low performance, investigate the contribution of each feature category to model performance, extract feature importances from the decision tree baseline,
and present an error-reject curve suggesting the potential of combining learned models with human annotations To conclude, we discuss promising avenues for future research in semantic type detection, such as assessing training data quality at scale, enriching feature extraction processes, and establishing shared benchmarks To support benchmarks for future research and integration into existing systems, we open source our data,
code, and trained model at https://sherlock Key contributions:
(1) Data (§3): Demonstrating a scalable process for matching 686, 675 columns from VizNet corpus for 78 semantic types, then describing with 1, 588 features like word- and paragraph embeddings (2) Model (§4): Formulating type detection as a multiclass classification problem, then contributing a novel multiinput neural network architecture (3) Results (§5): Benchmarking predictive performance against a decision tree and random forest baseline, two matchingbased models, and crowdsourced consensus 2 RELATED WORK
Sherlock is informed by existing commercial and open source systems for data preparation and analysis, as well as prior research work on ontology-based, feature-based, probabilistic, and synthesized approaches to semantic type detection Commercial and open source Semantic type detection enhances the functionality of commercial data preparation and analysis systems such as Microsoft Power BI [20], Trifacta [31], and Google
Data Studio [12] To the best of our knowledge, these commercial tools rely on manually defined regular expression patterns dictionary lookups of column headers and values to detect a limited set of

1http://webdatacommons org/webtables/goldstandardV2 html

Country/RegionStringLatitudeLongitudeCountry/RegionStringStringStringDecimalDecimalStringStringDetected Types Without Column HeadersDetected Types With Column HeadersRemove HeadersResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1501Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

semantic types For instance, Trifacta detects around 10 types (e ,
gender and zip code) and Power BI only supports time-related semantic types (e , date/time and duration) Open source libraries such as messytables [10], datalib [9], and csvkit [13] similarly use heuristics to detect a limited set of types Benchmarking directly against these systems was infeasible due to the small number of supported types and lack of extensibility However, we compare against learned regular expression and dictionary-based benchmarks representative of the approaches taken by these systems Prior research work, with roots in the semantic web and schema matching literature, provide alternative approaches to semantic type detection One body of work leverages existing data on the web, such as WebTables [5], and ontologies (or, knowledge bases) such as DBPedia [2], Wikitology [30], and Freebase [4] [33] construct a database of value-type mappings,
then assign types using a maximum likelihood estimator based on column values [30] use column headers and values to build a Wikitology query, the result of which maps columns to types Informed by these approaches, we looked towards existing ontologies to derive the 275 semantic types considered in this paper Several approaches capture and compare properties of data in a way that is ontology-agnostic [27] use heuristics to first separate numerical and textual types,
then describe those types using the Kolmogorov-Smirnov (K-S)
test and Term Frequency-Inverse Document Frequency (TF-IDF),
respectively [23] use slightly more features, including the Mann-Whitney test for numerical data and Jaccard similarity for textual data, to train logistic regression and random forest models We extend these feature-based approaches with a significantly larger set of features that includes character-level distributions,
word embeddings, and paragraph vectors We leverage orders of magnitude more features and training samples than prior work in order to train a high-capacity machine learning model, a deep neural network We include a decision tree and random forest model as benchmarks to represent these “simpler” machine learning models The third category of prior work employs a probabilistic approach [11] use conditional random fields to predict the semantic type of each value within a column, then combine these predictions into a prediction for the whole column [19] use probabilistic graphical models to annotate values with entities, columns with types, and column pairs with relationships These predictions simultaneously maximize a potential function using a message passing algorithm Probabilistic approaches are complementary to our machine learning-based approach by providing a means for combining column-specific predictions However, as with prior feature-based models, code for retraining these models was not made available for benchmarking Puranik [24] proposes a “specialist approach” combining the predictions of regular expressions, dictionaries, and machine learning models More recently, Yan and He [34] introduced a system that, given a search keyword and set of positive examples, synthesizes type detection logic from open source GitHub repositories This system provides a novel approach to leveraging domain-specific heuristics for parsing, validating, and transforming

semantic data types While both approaches are exciting, the code underlying these systems was not available for benchmarking 3 DATA
We describe the semantic types we consider, how we extracted data columns from a large repository of real-world datasets, and our feature extraction procedure 3.1 Data Collection
Ontologies like WordNet [32] and DBpedia [2] describe semantic concepts, properties of such concepts, and relationships between them To constrain the number of types we consider, we adopt the types described by the T2Dv2 Gold Standard,1 the result of a study matching DBpedia properties [29] with columns from the Web
Tables web crawl corpus [5] These 275 DBpedia properties, such as country, language, and industry, represent semantic types commonly found in datasets scattered throughout the web To expedite the collection of real-world data from diverse sources,
we use the VizNet repository [14], which aggregates and characterizes data from two popular online visualization platforms and open data portals, in addition to the Web Tables corpus For feasibility,
we restricted ourselves to the first 10M Web Tables datasets, but considered the remainder of the repository in its entirety We then match data columns from VizNet that have headers corresponding to our 275 types To accomodate variation in casing and formatting,
single word types matched case-altered modifications (e , name = Name = NAME) and multi-word types included concatenations of constituent words (e , release date = releaseDate) The matching process resulted in 6, 146, 940 columns matching the 275 considered types Manual verification indicated that the majority of columns were plausibly described by the corresponding semantic type, as shown in Table 1 In other words, matching column headers as ground truth labels of the semantic type yielded high quality training data 3.2 Feature Extraction
To create fixed-length representations of variable-length columns,
aid interpretation of results, and provide “hints” to our neural network, we extract features from each column To capture different properties of columns, we extract four categories of features: global statistics (27), aggregated character distributions (960), pretrained word embeddings (200), and self-trained paragraph vectors (400) Global statistics The first category of features describes highlevel statistical characteristics of columns For example, the “column entropy” feature describes how uniformly values are distributed Such a feature helps differentiate between types that contain more repeated values, such as gender, from types that contain many unique values, such as name Other types, like weight and sales,
may consist of many numerical characters, which is captured by the “mean of the number of numerical characters in values ” A complete list of these 27 features can be found in Table 8 in the Appendix Character-level distributions Preliminary analysis indicated that simple statistical features such as the “fraction of values with numerical characters” provide surprising predictive power Motivated

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1502KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al by these results and the prevalence of character-based matching approaches such as regular expressions, we extract features describing the distribution of characters in a column Specifically, we compute the count of all 96 ASCII-printable characters (i , digits, letters,
and punctuation characters, but not whitespace) within each value of a column We then aggregate these counts with 10 statistical functions (i , any, all, mean, variance, min, max, median, sum,
kurtosis, skewness), resulting in 960 features Example features include “whether all values contain a ‘-’ character” and the “mean number of ‘/’ characters ”

Word embeddings For certain semantic types, columns frequently contain commonly occurring words For example, the city type contains values such as New York City, Paris, and London To characterize the semantic content of these values, we used word embeddings that map words to high-dimensional fixed-length numeric vectors In particular, we used a pre-trained GloVe dictionary [22]
containing 50-dimensional representations of 400K English words aggregated from 6B tokens, used for tasks such as text similarity [16] For each value in a column, if the value is a single word,
we look up the word embedding from the GloVe dictionary We omit a term if it does not appear in the GloVe dictionary For values containing multiple words, we looked up each distinct word and represented the value with the mean of the distinct word vectors Then, we computed the mean, mode, median and variance of word vectors across all values in a column Paragraph vectors To represent each column with a fixed-length numerical vector, we implemented the Distributed Bag of Words version of Paragraph Vector (PV-DBOW) [18] Paragraph vectors were originally developed to numerically represent the “topic” of pieces of texts, but have proven effective for more general tasks,
such as document similarity [8] In our implementation, each column is a “paragraph” while values within a column are “words”:
both the entire column and constituent values are represented by one-hot encoded vectors After pooling together all columns across all classes, the training procedure for each column in the same 60% training set used by the main Sherlock model is as follows We randomly select a window of value vectors, concatenate the column vector with the remaining value vectors, then train a single model to predict the former from the latter Using the Gensim library [28], we trained this model for 20 iterations We used the trained model to map each column in both the training and test sets to a 400-dimensional paragraph vector, which provided a balance between predictive power and computational tractability 3.3 Filtering and Preprocessing
Certain types occur more frequently in the VizNet corpus than others For example, description and city are more common than collection and continent To address this heterogeneity,
we limited the number of columns to at most 15K per class and excluded the 10% types containing less than 1K columns Other semantic types, especially those describing numerical concepts, are unlikely to be represented by word embeddings To contend with this issue, we filtered out the types for which at least 15% of the columns did not contain a single word that is present in

the GloVe dictionary This filter resulted in a final total of 686,765
columns corresponding to 78 semantic types, of which a list is included in Table 7 in the Appendix The distribution of number of columns per semantic type is shown in Figure 3 Figure 3: Number of columns per semantic type extracted from VizNet after filtering out the types with more than 15%
of the columns not present in the GloVe dictionary, or with less than 1K columns Before modeling, we preprocess our features by creating an additional binary feature indicating whether word embeddings were successfully extracted for a given column Including this feature results in a total of 1,588 features Then, we impute missing values across all features with the mean of the respective feature 4 METHODS
We describe our deep learning model, random forest baseline, two matching-based benchmarks, and crowdsourced consensus benchmark Then, we explain our training and evaluation procedures 4.1 Sherlock: A Multi-input Neural Network
Prior machine learning approaches to semantic type detection [19,
33] trained simple models, such as logistic regression, on relatively small feature sets We consider a significantly larger number of features and samples, which motivates our use of a feedforward neural network Specifically, given the different number of features and varying noise levels within each feature category, we use a multi-input architecture with hyperparameters shown in Figure 4 At a high-level, we train subnetworks for each feature category except the statistical features, which consist of only 27 features These subnetworks “compress” input features to an output of fixed dimension We chose this dimension to be equal to the number of types in order to evaluate each subnetwork independently Then,
we concatenate the weights of the three output layers with the statistical features to form the input layer of the primary network Each network consists of two hidden layers with rectified linear unit (ReLU) activation functions Experiments with hidden layer sizes between 100 and 1, 000 (i , on the order of the input layer dimension) indicate that hidden layer sizes of 300, 200, and 400
for the character-level, word embedding, and paragraph vector subnetworks, respectively, provides the best results To prevent

DescriptionAlbumWeightRankCityLocationProductGradesPlaysElevationDepthFamilyCollectionBirth placeCapacityContinent02,5005,0007,50010,00012,50015,000Number of SamplesSemantic TypesResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1503Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

overfitting, we included drop out layers and weight decay terms The final class predictions result from the output of the final softmax layer, corresponding to the network’s confidence about a sample belonging to each class, the predicted label then is the class with the highest confidence The neural network, which we refer to as “Sherlock,” is implemented in TensorFlow [1] each type, we collected the 1, 000 most frequently occurring values across all columns, resulting in 78, 000 { value : type } pairs For example, Figure 5 shows examples of entries mapped to the grades type Given an unseen data column at test time, we compare 1, 000
randomly selected column values to each entry of the dictionary,
then classify the column as the most frequently matched type Figure 5: Examples of dictionary entries and a learned regular expression for the grades type Learned regular expressions Regular expressions are frequently used to detect semantic types with common character patterns, such as address, birth date, and year The second matching-based benchmark uses patterns of characters specified by learned regular expressions We learn regular expressions for each type using the evolutionary procedure of Bartoli et al Consistent with the original setup, we randomly sampled 50 “positive values” from each type, and 50 “negative” values from other types An example of a learned regular expression in Java format for the grades type is shown in Figure 5 As with the dictionary benchmark, we match 1, 000 randomly selected values against learned regular expressions,
then use majority vote to determine the final predicted type Crowdsourced annotations To assess the performance of human annotators at predicting semantic type, we conducted a crowdsourced experiment The experiment began by defining the concepts of data and semantic type, then screened out participants unable to select a specified semantic type After the prescreen, participants completed three sets of ten questions separated by two attention checks Each question presented a list of data values, asked “Which one of the following types best describes these data values?”, and required participants to select a single type from a scrolling menu with 78 types Questions were populated from a pool of 780 samples containing 10 randomly selected values from all 78 types We used the Mechanical Turk crowdsourcing platform [17] to recruit 390 participants that were native English speakers and had ≥95% HIT approval rating, ensuring high-quality annotations Participants completed the experiment in 16 minutes and 22 seconds on average and were compensated 2 USD, a rate slightly exceeding the
United States federal minimum wage of 7.25 USD Detailed worker demographics are described in Appendix A.2 Overall, 390 participants annotated 30 samples each, resulting in a total of 11, 700
annotations, or an average of 15 annotations per sample For each sample, we used the most frequent (i , the mode) type from the 15
annotations as the crowdsourced consensus annotation 4.3 Training and Evaluation
To ensure consistent evaluation across benchmarks, we divided the data into 60/20/20 training/validation/testing splits To account for

Figure 4: Architecture of the primary network and its feature-specific subnetworks, and the hyperparameters used for training 4.2 Benchmarks
To measure the relative performance of Sherlock, we compare against four benchmarks Machine learning classifiers The first benchmark is a decision tree, a non-parametric machine learning model with reasonable “out-of-the-box” performance and straightforward interpretation We use the decision tree to represent the simpler models found in prior research, such as the logistic regression used in Pham et al Learning curves indicated that decision tree performance plateaued beyond a depth of 50, which we then used as the maximum depth We also add a random forest classifier we built from 10 such trees, which often yields significantly better performance For all remaining parameters, we used the default settings in the scikit-learn package [21] Dictionaries are commonly used to detect semantic types that contain a finite set of valid values, such as country,
day, and language The first matching-based benchmark is a dictionary that maps column values or headers to semantic types For

Primary NetworkBatch Norm(size=128)ConcatenateReLU (500 units)ReLU (500 units)Output (78 units)SoftmaxInput FeaturesReLU (x units)Batch NormDropoutReLU (x units)Output (78 units)SoftmaxFeature-specificSubnetworkDropout(rate=0.3)OutputCharacterOutputWordOutputParagraphStatisticalMetricAccuracyLoss FunctionCross-EntropyOptimizerAdamEpochs100Early Stopping Patience5HyperparametersLearning Rate1e-4Weight Decay Rate1e-4Dictionary Entries (20 out of 1000)Learned Regular Expression\w\w \-(?: \w\w)*+|[06PK][A-Za-z]*+\-\w|\w\w\w\w\w\w \w\w \w\w\w \w\w9-12K-5PK - 0509 - 12KG - 05PRESCHOOL-56-8KG-0606 - 08PK -PRESCHOOL-8PK - 8KG - 12K-806 - 12K-^KG - 08- 12PK - 12PK - 08Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1504KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al class imbalances, we evaluate model performance using the average
F1-score = 2 × (precision × recall)/(precision + recall), weighted by the number of columns per class in the test set (i To estimate the mean and 95% percentile error of the crowdsourced consensus F1 score, we conducted 105 bootstrap simulations by resampling annotations for each sample with replacement Computational effort and space required at prediction time are also important metrics for models incorporated into user-facing systems We measure the average time in seconds needed to extract features and generate a prediction for a single sample, and report the space required by the models in megabytes 5 RESULTS
We report the performance of our multi-input neural network and compare against benchmarks Then, we examine types for which
Sherlock demonstrated high and low performance, the contribution of each feature category in isolation, decision tree feature importances, and the effect of rejection threshold on performance 5.1 Benchmark Results
We compare Sherlock against decision tree, random forest, dictionarybased, learned regular expression, and crowdsourced consensus benchmarks Table 2 presents the F1 score weighted by support,
runtime in seconds per sample, and size in megabytes of each model Table 2: Support-weighted F1 score, runtime at prediction,
and size of Sherlock and four benchmarks Method

F1 Score Runtime (s)

Size (Mb)

Sherlock
Decision tree
Random forest

Machine Learning 0.89
0.76
0.84

0.42 (±0.01)
0.26 (±0.01)
0.26 (±0.01)

Dictionary
Regular expression

Matching-based 0.16
0.04
Crowdsourced Annotations

0.01 (±0.03)
0.01 (±0.03)

Consensus

0.32 (±0.02)

33.74 (±0.86)

6.2
59.1
760.4

0.5
0.01

−

that annotating semantic types with a large number of types is a challenging and ambiguous task Comparing the machine learning models, Sherlock significantly outperforms the decision tree baseline, while the random forest classifier is competitive For cases in which interpretability of features and predictions are important considerations, the tree-based benchmarks may be a suitable choice of model Despite poor predictive performance, matching-based benchmarks are significantly smaller and faster than both machine learning models For cases in which absolute runtime and model size are critical, optimizing matching-based models may be a worthwhile approach This trade-off also suggests a hybrid approach of combining matching-based models for “easy” types with machine learning models for more ambiguous types 5.2 Performance for Individual Types
Table 3 displays the top and bottom five types, as measured by the
F1 score achieved by Sherlock for that type High performing types such as grades and industry frequently contain a finite set of valid values, as shown in Figure 5 for grades Other types such as birth date and ISBN, often follow consistent character patterns,
as shown in Table 1 Table 3: Top five and bottom five types by F1 score Type

F1 Score Precision

Recall

Support

Grades
ISBN
Birth Date
Industry
Affiliation

Brand
Person
Director
Sales
Ranking

Top 5 Types

0.991
0.986
0.970
0.968
0.961

0.989
0.981
0.965
0.947
0.966

Bottom 5 Types 0.760
0.654
0.700
0.568
0.612

0.685
0.630
0.537
0.514
0.468

0.994
0.992
0.975
0.989
0.956

0.623
0.608
0.436
0.469
0.349

1765
1430
479
2958
1768

574
579
225
322
439

We first note that the machine learning models significantly outperform the matching-based and crowdsourced consensus benchmarks, in terms of F1 score The relatively low performance of crowdsourced consensus is perhaps due to the visual overload of selecting from 78 types, such that performance may increase with a smaller number of candidate types Handling a large number of candidate classes is a benefit of using an ML-based or matching-based model Alternatively, crowdsourced workers may have difficulties differentiating between classes that are unfamiliar or contain many numeric values Lastly, despite our implementing basic training and honeypot questions, crowdsourced workers will likely improve with longer training times and stricter quality control Inspection of the matching-based benchmarks suggests that dictionaries and learned regular expressions are prone to “overfitting”
on the training set Feedback from crowdsourced workers suggests

Table 4: Examples of low precision and low recall types Examples

True type Predicted type

81, 13, 3, 1
316, 481, 426, 1, 223
$, $$, $$$, $$$$, $$$$$

#1, #2, #3, #4, #5, #6
3, 6, 21, 34, 29, 36, 54
1st, 2nd, 3rd, 4th, 5th

Low Precision
Rank
Plays
Symbol

Low Recall

Sales
Sales
Sales

Ranking
Ranking
Ranking

Rank
Plays
Position

To understand types for which Sherlock performs poorly, we include incorrectly predicted examples for the lowest precision

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1505Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

type (sales) and the lowest recall type (ranking) in Table 4 From the three examples incorrectly predicted as sales, we observe that purely numerical values or values appearing in multiple classes (e ,
currency symbols) present a challenge to type detection systems From the three examples of incorrectly predicted ranking columns,
we again note the ambiguity of numerical values 5.3 Contribution by Feature Category
We trained feature-specific subnetworks in isolation and report the F1 scores in Table 5 Word embedding, character distribution,
and paragraph vector feature sets demonstrate roughly equal performance to each other, and significantly above that of the global statistics features, though this may be due to fewer features Each feature set in isolation performs significantly worse than the full model, supporting our combining of each feature set Table 6: Top-10 features for the decision tree model “Score”
denotes normalized gini impurity (a) Top-10 global statistics features (out of 27) Rank Feature Name

Fraction of Cells with Numeric Characters

1 Number of Values 2 Maximum Value Length 3 Mean # Alphabetic Characters in Cells 4
5 Column Entropy 6
7 Number of None Values 8 Mean Length of Values 9
10 Mean # of Numeric Characters in Cells

Proportion of Unique Values

Fraction of Cells with Alphabetical Characters

Score 1.00
0.79
0.43
0.38
0.35
0.33
0.33
0.28
0.22
0.16

Table 5: Performance contribution of isolated feature sets (b) Top-10 character-level distribution features (out of 960) Feature set
Word embeddings
Character distributions
Paragraph vectors
Global statistics

Num Features 201
960
400
27

F1 Score 0.79
0.78
0.73
0.25

5.4 Feature Importances
We measure feature importance by the total reduction of the Gini impurity criterion brought by that feature to the decision tree model The top 10 most important features from the global statistics and character-level distributions sets are shown in Table 6 While word embedding and paragraph vector features are important, they are difficult to interpret and are therefore omitted Inspecting Table 6a, we find that the “number of values” in a column is the most important feature Certain classes like name and requirements tended to contain fewer values, while others like year and family contained significantly more values The second most important feature is the “maximum value length” in characters, which may differentiate classes with long values, such as address and description, from classes with short values, such as gender and year The top character-level distribution features in Table 6b suggest the importance of specific characters for differentiating between types The third most important feature, the “minimum number of ‘-’ characters”, likely helps determine datetime-related types The fifth most important feature, “whether all values have a ‘,’ character” may also distinguish datetime-related or name-related types Further study of feature importances for semantic type detection is a promising direction for future research 5.5 Rejection Curves
Given unseen data values, Sherlock assesses the probability of those values belonging to each type, then predicts the type with the highest probability Interpreting probabilities as a measure of confidence,
we may want to only label samples with high confidence of belonging to a type To understand the effect of confidence threshold on

Rank Feature Name

Skewness of ‘,’

Sum of ‘D’ across values

1
2 Mean number of ‘M’
3 Minimum number of ‘-’
4
5 Whether all values have a ‘,’
6 Maximum number of ‘g’
7
Skewness of ‘]’
8 Mean number of ‘,’
9 Mean number of ‘z’
10

Sum of ‘n’

Score 1.00
0.77
0.69
0.59
0.47
0.45
0.45
0.40
0.37
0.36

Figure 6: Rejection curves showing performance while rejecting all but the top x% highest confidence samples predictive performance, we present the error-rejection curves of
Sherlock and the decision tree model in Figure 6 By introducing a rejection threshold of 10% of the samples, Sherlock reaches an F1 score of ∼0.95 This significant increase in predictive performance suggests a hybrid approach in which low confidence samples are manually annotated Note that the higher rejection threshold, the lower the error we make in predicting labels,
at the cost of needing more expert capacity 0.00.10.20.30.40.50.60.70.80.91.0Fraction of Samples Rejected0.800.850.900.951.00F1 Score Weighted by SupportNeural Network (Sherlock)Random Forest BaselineModelResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1506KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al 6 DISCUSSION
We began by considering a set of semantic types described by prior work that identifies correspondences between DBPedia [2]
and WebTables [5] Then, we constructed a dataset consisting of matches between those types with columns in the VizNet [14]
corpus Inspection of these columns suggests that such an approach yields training samples with few false positives After extracting four categories of features describing the values of each column,
we formulate type detection as a multiclass classification task A multi-input neural network demonstrates high predictive performance at the classification task compared to machine learning,
matching-based, and crowdsourced benchmarks We note that using real-world data provides the examples needed to train models that detect many types, at scale We also observe that the test examples frequently include dirty (e , missing or malformed) values, which suggests that real-world data also affords a degree of robustness Measuring and operationalizing these two benefits, especially with out-of-distribution examples, is a promising direction of research Developers have multiple avenues to incorporating ML-based semantic type detection approaches into systems To support the use of Sherlock “out-of-the-box,” we distribute Sherlock as a Python library3 that can be easily installed and incorporated into existing codebases For developers interested in a different set of semantic types, we open source our training and analysis scripts.2 The repository also supports developers wishing to retrain Sherlock using data from their specific data ecologies, such as enterprise or research settings with domain-specific data To close, we identify four promising avenues for future research:
(1) enhancing the quantity and quality of the training data, (2)
increasing the number of considered types, (3) enriching the set of features extracted from each column, and (4) developing shared benchmarks Enhancing data quantity and quality Machine learning model performance is limited by the number of training examples Sherlock is no exception Though the VizNet corpus aggregates datasets from four sources, there is an opportunity to incorporate training examples from additional sources, such as Kaggle,2 datasets included alongside the R statistical environment,3 and the ClueWeb web crawl of Excel spreadsheets.4 We expect increases in training data diversity to improve the robustness and generalizability of
Sherlock Model predictions quality is further determined by the correspondence between training data and unseen testing data, such as datasets uploaded by analysts to a system Our method of matching semantic types with columns from real-world data repositories affords both the harvesting of training samples at scale and the ability to use aspects of dirty data, such as the number of missing values,
as features While we verified the quality of training data through manual inspection, there is an opportunity to label data quality at scale by combining crowdsourcing with active learning By assessing the quality of each training dataset, such an approach would support training semantic type detection models with completely “clean” data at scale com/datasets 3https://github com/vincentarelbundock/Rdatasets 4http://lemurproject php

Increasing number of semantic types To ground our approach in prior work, this paper considered 78 semantic types described by the T2Dv2 Gold Standard While 78 semantic types is a substantial increase over what is supported in existing systems, it is a small subset of entities from existing knowledge bases: the DBPedia ontology [2] covers 685 classes, WordNet [32] contains 175K synonym sets, and Knowledge Graph5 contains millions of entities The entities within these knowledge bases, and hierarchical relationships between entities, provide an abundance of semantic types In lieu of a relevant ontology, researchers can count frequency of column headers in available data to determine which semantic types to consider Such a data-driven approach would ensure the maximum number of training samples for each semantic type Additionally, these surfaced semantic types are potentially more specific to usecase and data ecology, such as data scientists integrating enterprise databases within a company Enriching feature extraction We incorporate four categories of features that describe different aspects of column values A promising approach is to include features that describe relationships between columns (e , correlation, number of overlapping values,
and name similarity), aspects of the entire dataset (e , number of columns), and source context (e , webpage title for scraped tables) Additionally, while we used features to aid interpretation of results,
neural networks using raw data as input are a promising direction of research For example, a character-level recurrent neural network could classify concatenated column values Developing shared benchmarks Despite rich prior research in semantic type detection, we could not find a benchmark with publicly available code that accommodates a larger set of semantic types We therefore incorporated benchmarks that approximated stateof-the-art data systems, to the best of our knowledge However,
domains such as image classification and language translation have benefited from shared benchmarks and test sets Towards this end,
we hope that open-sourcing the data and code used in this paper can benefit future research 7 CONCLUSION
Correctly detecting semantic types is critical to many important data science tasks Machine learning models coupled with largescale data repositories have demonstrated success across domains,
and suggest a promising approach to semantic type detection Sherlock provides a step forward towards this direction REFERENCES [1] Martín Abadi et al TensorFlow: A system for large-scale machine learning In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16) [2] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
and Zachary Ives DBpedia: A nucleus for a web of open data [3] Alberto Bartoli, Andrea De Lorenzo, Eric Medvet, and Fabiano Tarlao Inference of regular expressions for text extraction from examples IEEE Transactions on Knowledge and Data Engineering 28, 5 (2016), 1217–1230 [4] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor Freebase: A Collaboratively Created Graph Database for Structuring Human
Knowledge In Proceedings of the 2008 ACM SIGMOD International Conference on
Management of Data (SIGMOD ’08) ACM, New York, NY, USA, 1247–1250 5https://developers com/knowledge-graph

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1507Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

[5] Michael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang WebTables: Exploring the Power of Tables on the Web org/10.14778/1453856.1453916
[6] Raul Castro Fernandez, Ziawasch Abedjan, Famien Koko, Gina Yuan, Samuel
Madden, and Michael Stonebraker Aurum: A Data Discovery System [7] Raul Castro Fernandez, Essam Mansour, Abdulhakim Qahtan, Ahmed Elmagarmid, Ihab Ilyas, Samuel Madden, Mourad Ouzzani, Michael Stonebraker, and
Nan Tang Seeping Semantics: Linking Datasets Using Word Embeddings for Data Discovery org/10.1109/ICDE.2018.00093

[8] Andrew M Dai, Christopher Olah, and Quoc V Le Document embedding

[34] Cong Yan and Yeye He Synthesizing type-detection logic for rich semantic data types using open-source code In Proceedings of the 2018 International
Conference on Management of Data [35] Benjamin Zapilko, Matthäus Zloch, and Johann Schaible Utilizing Regular
Expressions for Instance-Based Schema Matching CEUR Workshop Proceedings 946 A APPENDIX
A.1 Supplemental Tables

http://vega Table 7: 78 semantic types included in this study with paragraph vectors arXiv preprint arXiv:1507.07998 (2015) [9] Interactive Data Lab Datalib: JavaScript Data Utilities io/datalib

[10] Open Knowledge Foundation messytables · PyPi org/

project/messytables

[11] Aman Goel, Craig A Knoblock, and Kristina Lerman Exploiting structure within data for accurate labeling using conditional random fields In Proceedings on the International Conference on Artificial Intelligence (ICAI) Google Data Studio https://datastudio google.com [13] Christopher Groskopf and contributors readthedocs.org

[14] Kevin Hu, Neil Gaikwad, Michiel Bakker, Madelon Hulsebos, Emanuel Zgraggen,
César Hidalgo, Tim Kraska, Guoliang Li, Arvind Satyanarayan, and Çağatay
Demiralp VizNet: Towards a large-scale visualization learning and benchmarking repository In Proceedings of the 2019 Conference on Human Factors in
Computing Systems (CHI) [15] Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer Wrangler: Interactive Visual Specification of Data Transformation Scripts In ACM
Human Factors in Computing Systems (CHI) [16] Tom Kenter and Maarten De Rijke Short text similarity with word embeddings In Proceedings of the 24th ACM international on conference on information and knowledge management [17] Aniket Kittur, Ed H Chi, and Bongwon Suh Crowdsourcing User Studies with Mechanical Turk In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’08) ACM, New York, NY, USA, 453–456 [18] Quoc Le and Tomas Mikolov Distributed representations of sentences and documents In International Conference on Machine Learning [19] Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti Annotating and searching web tables using entities, types and relationships Proceedings of the
VLDB Endowment 3, 1-2 (2010), 1338–1347 Power BI | Interactive Data Visualization BI microsoft.com

[21] Fabian Pedregosa et al Scikit-learn: Machine Learning in Python Journal

of Machine Learning Research 12 (2011), 2825–2830 [22] Jeffrey Pennington, Richard Socher, and Christopher Manning Glove:
Global vectors for word representation In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [23] Minh Pham, Suresh Alse, Craig A Knoblock, and Pedro Szekely Semantic labeling: a domain-independent approach In International Semantic Web Conference Springer, 446–462 [24] Nikhil Waman Puranik A Specialist Approach for Classification of Column

Data University of Maryland, Baltimore County [25] Erhard Rahm and Philip A A Survey of Approaches to Automatic

Schema Matching The VLDB Journal 10, 4 (Dec [26] Vijayshankar Raman and Joseph M Potter’s Wheel: An Interactive Data Cleaning System In Proceedings of the 27th International Conference on Very Large Data Bases (VLDB ’01) Morgan Kaufmann Publishers Inc , San
Francisco, CA, USA, 381–390 [27] S Krishnamurthy Ramnandan, Amol Mittal, Craig A Knoblock, and Pedro Szekely Assigning semantic labels to data sources In European Semantic Web
Conference Springer, 403–417 [28] Radim Řehůřek and Petr Sojka Software Framework for Topic Modelling with Large Corpora In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks [29] Dominique Ritze and Christian Bizer Matching web tables to DBpedia – a

feature utility study context 42, 41 (2017), 19 [30] Zareen Syed, Tim Finin, Varish Mulwad, Anupam Joshi, et al Exploiting a web of semantic data for interpreting tables In Proceedings of the Second Web
Science Conference Data Wrangling Tools & Software trifacta.com [32] Princeton University edu

[33] Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paşca, Warren Shen, Fei
Wu, Gengxin Miao, and Chung Wu Recovering semantics of tables on the web Proceedings of the VLDB Endowment 4, 9 (2011), 528–538 Semantic Types

Code
Collection
Command
Company
Component
Continent
Country
County
Creator
Credit
Currency
Day
Depth
Description Manufacturer

Education
Elevation
Family
File size
Format
Gender
Genre
Grades
Industry
ISBN
Jockey
Language
Location

Address
Affiliate
Affiliation
Age
Album
Area
Artist
Birth date
Birth place
Brand
Capacity
Category
City
Class
Classification Director
Duration
Club

Name
Nationality

Requirement
Result
Sales
Service
Sex
Species
State
Status
Symbol
Team
Team name
Type
Weight
Year

Notes
Operator
Order
Organisation
Origin
Owner
Person
Plays
Position
Product
Publisher
Range
Rank
Ranking
Region
Religion

Table 8: Description of the 27 global statistical features Asterisks (*) denote features included in Venetis et al Feature description
Number of values Fraction of values with unique content *
Fraction of values with numerical characters *
Fraction of values with alphabetical characters of the number of numerical characters in values of the number of alphabetical characters in values of the number special characters in values of the number of words in values *
{Percentage, count, only/has-Boolean} of the None values {Stats, sum, min, max, median, mode, kurtosis, skewness,
any/all-Boolean} of length of values A.2 Mechanical Turk Demographics
Of the 390 participants, 57.18% were male and 0.43% female 1.5%
completed some high school without attaining a diploma, while others had associates (10.5%), bachelor’s (61.0%), master’s (13.1%), or doctorate or professional degree (1.8%) in addition to a high school diploma (12.3%) 26.4% of participants worked with data daily, 33.1%
weekly, 17.2% monthly, and 11.0% annually, while 12.3% never work with data In terms of age: 10.0% of participants were between 18-23,
24-34 (60.3%), 35-40 (13.3%), 41-54 (12.6%), and above 55 (3.8%) Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1508 Sherlock: A Deep Learning Approach to
Semantic Data Type Detection

Madelon Hulsebos
MIT Media Lab madelonhulsebos@gmail.com

Arvind Satyanarayan
MIT CSAIL arvindsatya@mit edu

Kevin Hu
MIT Media Lab kzh@mit edu

Tim Kraska
MIT CSAIL kraska@mit edu

Michiel Bakker
MIT Media Lab bakker@mit edu

Çağatay Demiralp
Megagon Labs cagatay@megagon ai

Emanuel Zgraggen
MIT CSAIL emzg@mit edu

César Hidalgo
MIT Media Lab hidalgo@mit edu

Figure 1: Data processing and analysis flow, starting from (1) a corpus of real-world datasets, proceeding to (2) feature extraction, (3) mapping extracted features to ground truth semantic types, and (4) model training and prediction ABSTRACT
Correctly detecting the semantic type of data columns is crucial for data science tasks such as automated data cleaning, schema matching, and data discovery Existing data preparation and analysis systems rely on dictionary lookups and regular expression matching to detect semantic types However, these matching-based approaches often are not robust to dirty data and only detect a limited number of types We introduce Sherlock, a multi-input deep neural network for detecting semantic types We train Sherlock on 686, 765 data columns retrieved from the VizNet corpus by matching 78 semantic types from DBpedia to column headers We characterize each matched column with 1, 588 features describing the statistical properties, character distributions, word embeddings, and paragraph vectors of column values Sherlock achieves a support-weighted
F1 score of 0.89, exceeding that of machine learning baselines, dictionary and regular expression benchmarks, and the consensus of crowdsourced annotations CCS CONCEPTS • Computing methodologies → Machine learning; Knowledge representation and reasoning; • Information systems → Data mining Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than the author(s) must be honored Abstracting with credit is permitted To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acm KDD ’19, August 4–8, 2019, Anchorage, AK, USA © 2019 Copyright held by the owner/author(s) Publication rights licensed to ACM ACM ISBN 978-1-4503-6201-6/19/08 $15.00
https://doi org/10.1145/3292500.3330993

KEYWORDS
Tabular data, type detection, semantic types, deep learning

ACM Reference Format:
Madelon Hulsebos, Kevin Hu, Michiel Bakker, Emanuel Zgraggen, Arvind
Satyanarayan, Tim Kraska, Çağatay Demiralp, and César Hidalgo Sherlock: A Deep Learning Approach to Semantic Data Type Detection In
The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’19), August 4–8, 2019, Anchorage, AK, USA ACM, New York, NY, USA,
9 pages org/10.1145/3292500.3330993

1 INTRODUCTION
Data preparation and analysis systems rely on correctly detecting types of data columns to enable and constrain functionality For example, automated data cleaning facilitates the generation of clean data through validation and transformation rules that depend on data type [15, 26] Schema matching identifies correspondences between data objects, and frequently uses data types to constrain the search space of correspondences [25, 35] Data discovery surfaces data relevant to a given query, often relying on semantic similarities across tables and columns [6, 7] While most systems reliably detect atomic types such as string,
integer, and boolean, semantic types are disproportionately more powerful and in many cases essential Semantic types provide finergrained descriptions of the data by establishing correspondences between columns and real-world concepts and as such, can help with schema matching to determine which columns refer to the same real-world concepts, or data cleaning by determining the conceptual domain of a column In some cases, the detection of a semantic type can be easy For example, an ISBN or credit card number are generated according to strict validation rules, lending themselves to straightforward type detection with just a few rules Semantic Type Detection2 Sampled Dataset and Features1 Training and Testing SetDateCountryNigeria GermanyNameCanadaM Buhari08-18-201806-14-201507-20-2017 A MerkelJ TrudeauUnseenDataModelPredictedTypes andConfidencesLocation: 0.9Name: 0.7Year: 0.8PredictionDateCharacter DistributionsWord EmbeddingsParagraph VectorsGlobal StatisticsCountryName FeatureExtractionJoinJoinTrainingFeature CategoriesTypesNeural NetworkFeaturesExactMatchingColumn Header Column Values Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1500KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al Table 1: Data values sampled from real-world datasets Sampled values

Type location TBA | Chicago, Ill | Nashville, Tenn location UNIVERSITY SUITES | U 27; NA | NORSE HALL location Away | Away | Home | Away | Away date date date name name name

27 Dec 1811 | 1852 | 1855 | - | 1848 | 1871 | 1877
– –, 1922 | – –, 1902 | – –, 1913 | – –, 1919
December 06 | August 23 | None
Svenack | Svendd | Sveneldritch | Svengöran
HOUSE, BRIAN | HSIAO, AMY | HSU, ASTRID
D dunn

But most types, including location, birth date, and name, do not adhere to such structure, as shown in Table 1 Existing open source and commercial systems take matchingbased approaches to semantic type detection For example, regular expression matching captures patterns of data values using predefined character sequences Dictionary approaches use matches between data headers and values with internal look-up tables While sufficient for detecting simple types, these matching-based approaches are often not robust to malformed or dirty data, support only a limited number of types, and under-perform for types without strict validations For example, Figure 2 shows that Tableau detects a column labeled “Continent Name” as string After removing column headers, no semantic types are detected Note that missing headers or incomprehensible headers are not uncommon For example, SAP’s system table T 005 contains country information and column N MF MT is the standard name field, whereas I NTCA refers to the ISO code or X PLZS to zip-code Figure 2: Data types detected by Tableau Desktop 2018.3 for a dataset of country capitals, with and without headers Machine learning models, coupled with large-scale training and benchmarking corpora, have proven effective at predictive tasks across domains Examples include the AlexNet neural network trained on ImageNet for visual recognition and the Google Neural
Machine Translation system pre-trained on WMT parallel corpora for language translation Inspired by these advances, we introduce
Sherlock, a deep learning approach to semantic type detection trained on a large corpus of real-world columns To begin, we consider 78 semantic types described by T2Dv2
Gold Standard,1 which matches properties from the DBpedia ontology with column headers from the WebTables corpus Then, we use exact matching between semantic types and column headers to extract 686, 765 data columns from the VizNet corpus [14], a large-scale repository of real world datasets collected from the web,
popular visualization systems, and open data portals We consider each column as a mapping from column values to a column header We then extract 1, 588 features from each column, describing the distribution of characters, semantic content of words and columns, and global statistics such as cardinality and uniqueness Treating column headers as ground truth labels of the semantic type, we formulate semantic type detection as a multiclass classification problem A multi-input neural network architecture achieves a supportweighted F1-score of 0.89, exceeding that of decision tree and random forest baseline models, two matching-based approaches that represent type detection approaches in practice, and the consensus of crowdsourced annotations We then examine types for which the neural network demonstrates high and low performance, investigate the contribution of each feature category to model performance, extract feature importances from the decision tree baseline,
and present an error-reject curve suggesting the potential of combining learned models with human annotations To conclude, we discuss promising avenues for future research in semantic type detection, such as assessing training data quality at scale, enriching feature extraction processes, and establishing shared benchmarks To support benchmarks for future research and integration into existing systems, we open source our data,
code, and trained model at https://sherlock Key contributions:
(1) Data (§3): Demonstrating a scalable process for matching 686, 675 columns from VizNet corpus for 78 semantic types, then describing with 1, 588 features like word- and paragraph embeddings (2) Model (§4): Formulating type detection as a multiclass classification problem, then contributing a novel multiinput neural network architecture (3) Results (§5): Benchmarking predictive performance against a decision tree and random forest baseline, two matchingbased models, and crowdsourced consensus 2 RELATED WORK
Sherlock is informed by existing commercial and open source systems for data preparation and analysis, as well as prior research work on ontology-based, feature-based, probabilistic, and synthesized approaches to semantic type detection Commercial and open source Semantic type detection enhances the functionality of commercial data preparation and analysis systems such as Microsoft Power BI [20], Trifacta [31], and Google
Data Studio [12] To the best of our knowledge, these commercial tools rely on manually defined regular expression patterns dictionary lookups of column headers and values to detect a limited set of

1http://webdatacommons org/webtables/goldstandardV2 html

Country/RegionStringLatitudeLongitudeCountry/RegionStringStringStringDecimalDecimalStringStringDetected Types Without Column HeadersDetected Types With Column HeadersRemove HeadersResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1501Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

semantic types For instance, Trifacta detects around 10 types (e ,
gender and zip code) and Power BI only supports time-related semantic types (e , date/time and duration) Open source libraries such as messytables [10], datalib [9], and csvkit [13] similarly use heuristics to detect a limited set of types Benchmarking directly against these systems was infeasible due to the small number of supported types and lack of extensibility However, we compare against learned regular expression and dictionary-based benchmarks representative of the approaches taken by these systems Prior research work, with roots in the semantic web and schema matching literature, provide alternative approaches to semantic type detection One body of work leverages existing data on the web, such as WebTables [5], and ontologies (or, knowledge bases) such as DBPedia [2], Wikitology [30], and Freebase [4] [33] construct a database of value-type mappings,
then assign types using a maximum likelihood estimator based on column values [30] use column headers and values to build a Wikitology query, the result of which maps columns to types Informed by these approaches, we looked towards existing ontologies to derive the 275 semantic types considered in this paper Several approaches capture and compare properties of data in a way that is ontology-agnostic [27] use heuristics to first separate numerical and textual types,
then describe those types using the Kolmogorov-Smirnov (K-S)
test and Term Frequency-Inverse Document Frequency (TF-IDF),
respectively [23] use slightly more features, including the Mann-Whitney test for numerical data and Jaccard similarity for textual data, to train logistic regression and random forest models We extend these feature-based approaches with a significantly larger set of features that includes character-level distributions,
word embeddings, and paragraph vectors We leverage orders of magnitude more features and training samples than prior work in order to train a high-capacity machine learning model, a deep neural network We include a decision tree and random forest model as benchmarks to represent these “simpler” machine learning models The third category of prior work employs a probabilistic approach [11] use conditional random fields to predict the semantic type of each value within a column, then combine these predictions into a prediction for the whole column [19] use probabilistic graphical models to annotate values with entities, columns with types, and column pairs with relationships These predictions simultaneously maximize a potential function using a message passing algorithm Probabilistic approaches are complementary to our machine learning-based approach by providing a means for combining column-specific predictions However, as with prior feature-based models, code for retraining these models was not made available for benchmarking Puranik [24] proposes a “specialist approach” combining the predictions of regular expressions, dictionaries, and machine learning models More recently, Yan and He [34] introduced a system that, given a search keyword and set of positive examples, synthesizes type detection logic from open source GitHub repositories This system provides a novel approach to leveraging domain-specific heuristics for parsing, validating, and transforming

semantic data types While both approaches are exciting, the code underlying these systems was not available for benchmarking 3 DATA
We describe the semantic types we consider, how we extracted data columns from a large repository of real-world datasets, and our feature extraction procedure 3.1 Data Collection
Ontologies like WordNet [32] and DBpedia [2] describe semantic concepts, properties of such concepts, and relationships between them To constrain the number of types we consider, we adopt the types described by the T2Dv2 Gold Standard,1 the result of a study matching DBpedia properties [29] with columns from the Web
Tables web crawl corpus [5] These 275 DBpedia properties, such as country, language, and industry, represent semantic types commonly found in datasets scattered throughout the web To expedite the collection of real-world data from diverse sources,
we use the VizNet repository [14], which aggregates and characterizes data from two popular online visualization platforms and open data portals, in addition to the Web Tables corpus For feasibility,
we restricted ourselves to the first 10M Web Tables datasets, but considered the remainder of the repository in its entirety We then match data columns from VizNet that have headers corresponding to our 275 types To accomodate variation in casing and formatting,
single word types matched case-altered modifications (e , name = Name = NAME) and multi-word types included concatenations of constituent words (e , release date = releaseDate) The matching process resulted in 6, 146, 940 columns matching the 275 considered types Manual verification indicated that the majority of columns were plausibly described by the corresponding semantic type, as shown in Table 1 In other words, matching column headers as ground truth labels of the semantic type yielded high quality training data 3.2 Feature Extraction
To create fixed-length representations of variable-length columns,
aid interpretation of results, and provide “hints” to our neural network, we extract features from each column To capture different properties of columns, we extract four categories of features: global statistics (27), aggregated character distributions (960), pretrained word embeddings (200), and self-trained paragraph vectors (400) Global statistics The first category of features describes highlevel statistical characteristics of columns For example, the “column entropy” feature describes how uniformly values are distributed Such a feature helps differentiate between types that contain more repeated values, such as gender, from types that contain many unique values, such as name Other types, like weight and sales,
may consist of many numerical characters, which is captured by the “mean of the number of numerical characters in values ” A complete list of these 27 features can be found in Table 8 in the Appendix Character-level distributions Preliminary analysis indicated that simple statistical features such as the “fraction of values with numerical characters” provide surprising predictive power Motivated

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1502KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al by these results and the prevalence of character-based matching approaches such as regular expressions, we extract features describing the distribution of characters in a column Specifically, we compute the count of all 96 ASCII-printable characters (i , digits, letters,
and punctuation characters, but not whitespace) within each value of a column We then aggregate these counts with 10 statistical functions (i , any, all, mean, variance, min, max, median, sum,
kurtosis, skewness), resulting in 960 features Example features include “whether all values contain a ‘-’ character” and the “mean number of ‘/’ characters ”

Word embeddings For certain semantic types, columns frequently contain commonly occurring words For example, the city type contains values such as New York City, Paris, and London To characterize the semantic content of these values, we used word embeddings that map words to high-dimensional fixed-length numeric vectors In particular, we used a pre-trained GloVe dictionary [22]
containing 50-dimensional representations of 400K English words aggregated from 6B tokens, used for tasks such as text similarity [16] For each value in a column, if the value is a single word,
we look up the word embedding from the GloVe dictionary We omit a term if it does not appear in the GloVe dictionary For values containing multiple words, we looked up each distinct word and represented the value with the mean of the distinct word vectors Then, we computed the mean, mode, median and variance of word vectors across all values in a column Paragraph vectors To represent each column with a fixed-length numerical vector, we implemented the Distributed Bag of Words version of Paragraph Vector (PV-DBOW) [18] Paragraph vectors were originally developed to numerically represent the “topic” of pieces of texts, but have proven effective for more general tasks,
such as document similarity [8] In our implementation, each column is a “paragraph” while values within a column are “words”:
both the entire column and constituent values are represented by one-hot encoded vectors After pooling together all columns across all classes, the training procedure for each column in the same 60% training set used by the main Sherlock model is as follows We randomly select a window of value vectors, concatenate the column vector with the remaining value vectors, then train a single model to predict the former from the latter Using the Gensim library [28], we trained this model for 20 iterations We used the trained model to map each column in both the training and test sets to a 400-dimensional paragraph vector, which provided a balance between predictive power and computational tractability 3.3 Filtering and Preprocessing
Certain types occur more frequently in the VizNet corpus than others For example, description and city are more common than collection and continent To address this heterogeneity,
we limited the number of columns to at most 15K per class and excluded the 10% types containing less than 1K columns Other semantic types, especially those describing numerical concepts, are unlikely to be represented by word embeddings To contend with this issue, we filtered out the types for which at least 15% of the columns did not contain a single word that is present in

the GloVe dictionary This filter resulted in a final total of 686,765
columns corresponding to 78 semantic types, of which a list is included in Table 7 in the Appendix The distribution of number of columns per semantic type is shown in Figure 3 Figure 3: Number of columns per semantic type extracted from VizNet after filtering out the types with more than 15%
of the columns not present in the GloVe dictionary, or with less than 1K columns Before modeling, we preprocess our features by creating an additional binary feature indicating whether word embeddings were successfully extracted for a given column Including this feature results in a total of 1,588 features Then, we impute missing values across all features with the mean of the respective feature 4 METHODS
We describe our deep learning model, random forest baseline, two matching-based benchmarks, and crowdsourced consensus benchmark Then, we explain our training and evaluation procedures 4.1 Sherlock: A Multi-input Neural Network
Prior machine learning approaches to semantic type detection [19,
33] trained simple models, such as logistic regression, on relatively small feature sets We consider a significantly larger number of features and samples, which motivates our use of a feedforward neural network Specifically, given the different number of features and varying noise levels within each feature category, we use a multi-input architecture with hyperparameters shown in Figure 4 At a high-level, we train subnetworks for each feature category except the statistical features, which consist of only 27 features These subnetworks “compress” input features to an output of fixed dimension We chose this dimension to be equal to the number of types in order to evaluate each subnetwork independently Then,
we concatenate the weights of the three output layers with the statistical features to form the input layer of the primary network Each network consists of two hidden layers with rectified linear unit (ReLU) activation functions Experiments with hidden layer sizes between 100 and 1, 000 (i , on the order of the input layer dimension) indicate that hidden layer sizes of 300, 200, and 400
for the character-level, word embedding, and paragraph vector subnetworks, respectively, provides the best results To prevent

DescriptionAlbumWeightRankCityLocationProductGradesPlaysElevationDepthFamilyCollectionBirth placeCapacityContinent02,5005,0007,50010,00012,50015,000Number of SamplesSemantic TypesResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1503Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

overfitting, we included drop out layers and weight decay terms The final class predictions result from the output of the final softmax layer, corresponding to the network’s confidence about a sample belonging to each class, the predicted label then is the class with the highest confidence The neural network, which we refer to as “Sherlock,” is implemented in TensorFlow [1] each type, we collected the 1, 000 most frequently occurring values across all columns, resulting in 78, 000 { value : type } pairs For example, Figure 5 shows examples of entries mapped to the grades type Given an unseen data column at test time, we compare 1, 000
randomly selected column values to each entry of the dictionary,
then classify the column as the most frequently matched type Figure 5: Examples of dictionary entries and a learned regular expression for the grades type Learned regular expressions Regular expressions are frequently used to detect semantic types with common character patterns, such as address, birth date, and year The second matching-based benchmark uses patterns of characters specified by learned regular expressions We learn regular expressions for each type using the evolutionary procedure of Bartoli et al Consistent with the original setup, we randomly sampled 50 “positive values” from each type, and 50 “negative” values from other types An example of a learned regular expression in Java format for the grades type is shown in Figure 5 As with the dictionary benchmark, we match 1, 000 randomly selected values against learned regular expressions,
then use majority vote to determine the final predicted type Crowdsourced annotations To assess the performance of human annotators at predicting semantic type, we conducted a crowdsourced experiment The experiment began by defining the concepts of data and semantic type, then screened out participants unable to select a specified semantic type After the prescreen, participants completed three sets of ten questions separated by two attention checks Each question presented a list of data values, asked “Which one of the following types best describes these data values?”, and required participants to select a single type from a scrolling menu with 78 types Questions were populated from a pool of 780 samples containing 10 randomly selected values from all 78 types We used the Mechanical Turk crowdsourcing platform [17] to recruit 390 participants that were native English speakers and had ≥95% HIT approval rating, ensuring high-quality annotations Participants completed the experiment in 16 minutes and 22 seconds on average and were compensated 2 USD, a rate slightly exceeding the
United States federal minimum wage of 7.25 USD Detailed worker demographics are described in Appendix A.2 Overall, 390 participants annotated 30 samples each, resulting in a total of 11, 700
annotations, or an average of 15 annotations per sample For each sample, we used the most frequent (i , the mode) type from the 15
annotations as the crowdsourced consensus annotation 4.3 Training and Evaluation
To ensure consistent evaluation across benchmarks, we divided the data into 60/20/20 training/validation/testing splits To account for

Figure 4: Architecture of the primary network and its feature-specific subnetworks, and the hyperparameters used for training 4.2 Benchmarks
To measure the relative performance of Sherlock, we compare against four benchmarks Machine learning classifiers The first benchmark is a decision tree, a non-parametric machine learning model with reasonable “out-of-the-box” performance and straightforward interpretation We use the decision tree to represent the simpler models found in prior research, such as the logistic regression used in Pham et al Learning curves indicated that decision tree performance plateaued beyond a depth of 50, which we then used as the maximum depth We also add a random forest classifier we built from 10 such trees, which often yields significantly better performance For all remaining parameters, we used the default settings in the scikit-learn package [21] Dictionaries are commonly used to detect semantic types that contain a finite set of valid values, such as country,
day, and language The first matching-based benchmark is a dictionary that maps column values or headers to semantic types For

Primary NetworkBatch Norm(size=128)ConcatenateReLU (500 units)ReLU (500 units)Output (78 units)SoftmaxInput FeaturesReLU (x units)Batch NormDropoutReLU (x units)Output (78 units)SoftmaxFeature-specificSubnetworkDropout(rate=0.3)OutputCharacterOutputWordOutputParagraphStatisticalMetricAccuracyLoss FunctionCross-EntropyOptimizerAdamEpochs100Early Stopping Patience5HyperparametersLearning Rate1e-4Weight Decay Rate1e-4Dictionary Entries (20 out of 1000)Learned Regular Expression\w\w \-(?: \w\w)*+|[06PK][A-Za-z]*+\-\w|\w\w\w\w\w\w \w\w \w\w\w \w\w9-12K-5PK - 0509 - 12KG - 05PRESCHOOL-56-8KG-0606 - 08PK -PRESCHOOL-8PK - 8KG - 12K-806 - 12K-^KG - 08- 12PK - 12PK - 08Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1504KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al class imbalances, we evaluate model performance using the average
F1-score = 2 × (precision × recall)/(precision + recall), weighted by the number of columns per class in the test set (i To estimate the mean and 95% percentile error of the crowdsourced consensus F1 score, we conducted 105 bootstrap simulations by resampling annotations for each sample with replacement Computational effort and space required at prediction time are also important metrics for models incorporated into user-facing systems We measure the average time in seconds needed to extract features and generate a prediction for a single sample, and report the space required by the models in megabytes 5 RESULTS
We report the performance of our multi-input neural network and compare against benchmarks Then, we examine types for which
Sherlock demonstrated high and low performance, the contribution of each feature category in isolation, decision tree feature importances, and the effect of rejection threshold on performance 5.1 Benchmark Results
We compare Sherlock against decision tree, random forest, dictionarybased, learned regular expression, and crowdsourced consensus benchmarks Table 2 presents the F1 score weighted by support,
runtime in seconds per sample, and size in megabytes of each model Table 2: Support-weighted F1 score, runtime at prediction,
and size of Sherlock and four benchmarks Method

F1 Score Runtime (s)

Size (Mb)

Sherlock
Decision tree
Random forest

Machine Learning 0.89
0.76
0.84

0.42 (±0.01)
0.26 (±0.01)
0.26 (±0.01)

Dictionary
Regular expression

Matching-based 0.16
0.04
Crowdsourced Annotations

0.01 (±0.03)
0.01 (±0.03)

Consensus

0.32 (±0.02)

33.74 (±0.86)

6.2
59.1
760.4

0.5
0.01

−

that annotating semantic types with a large number of types is a challenging and ambiguous task Comparing the machine learning models, Sherlock significantly outperforms the decision tree baseline, while the random forest classifier is competitive For cases in which interpretability of features and predictions are important considerations, the tree-based benchmarks may be a suitable choice of model Despite poor predictive performance, matching-based benchmarks are significantly smaller and faster than both machine learning models For cases in which absolute runtime and model size are critical, optimizing matching-based models may be a worthwhile approach This trade-off also suggests a hybrid approach of combining matching-based models for “easy” types with machine learning models for more ambiguous types 5.2 Performance for Individual Types
Table 3 displays the top and bottom five types, as measured by the
F1 score achieved by Sherlock for that type High performing types such as grades and industry frequently contain a finite set of valid values, as shown in Figure 5 for grades Other types such as birth date and ISBN, often follow consistent character patterns,
as shown in Table 1 Table 3: Top five and bottom five types by F1 score Type

F1 Score Precision

Recall

Support

Grades
ISBN
Birth Date
Industry
Affiliation

Brand
Person
Director
Sales
Ranking

Top 5 Types

0.991
0.986
0.970
0.968
0.961

0.989
0.981
0.965
0.947
0.966

Bottom 5 Types 0.760
0.654
0.700
0.568
0.612

0.685
0.630
0.537
0.514
0.468

0.994
0.992
0.975
0.989
0.956

0.623
0.608
0.436
0.469
0.349

1765
1430
479
2958
1768

574
579
225
322
439

We first note that the machine learning models significantly outperform the matching-based and crowdsourced consensus benchmarks, in terms of F1 score The relatively low performance of crowdsourced consensus is perhaps due to the visual overload of selecting from 78 types, such that performance may increase with a smaller number of candidate types Handling a large number of candidate classes is a benefit of using an ML-based or matching-based model Alternatively, crowdsourced workers may have difficulties differentiating between classes that are unfamiliar or contain many numeric values Lastly, despite our implementing basic training and honeypot questions, crowdsourced workers will likely improve with longer training times and stricter quality control Inspection of the matching-based benchmarks suggests that dictionaries and learned regular expressions are prone to “overfitting”
on the training set Feedback from crowdsourced workers suggests

Table 4: Examples of low precision and low recall types Examples

True type Predicted type

81, 13, 3, 1
316, 481, 426, 1, 223
$, $$, $$$, $$$$, $$$$$

#1, #2, #3, #4, #5, #6
3, 6, 21, 34, 29, 36, 54
1st, 2nd, 3rd, 4th, 5th

Low Precision
Rank
Plays
Symbol

Low Recall

Sales
Sales
Sales

Ranking
Ranking
Ranking

Rank
Plays
Position

To understand types for which Sherlock performs poorly, we include incorrectly predicted examples for the lowest precision

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1505Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

type (sales) and the lowest recall type (ranking) in Table 4 From the three examples incorrectly predicted as sales, we observe that purely numerical values or values appearing in multiple classes (e ,
currency symbols) present a challenge to type detection systems From the three examples of incorrectly predicted ranking columns,
we again note the ambiguity of numerical values 5.3 Contribution by Feature Category
We trained feature-specific subnetworks in isolation and report the F1 scores in Table 5 Word embedding, character distribution,
and paragraph vector feature sets demonstrate roughly equal performance to each other, and significantly above that of the global statistics features, though this may be due to fewer features Each feature set in isolation performs significantly worse than the full model, supporting our combining of each feature set Table 6: Top-10 features for the decision tree model “Score”
denotes normalized gini impurity (a) Top-10 global statistics features (out of 27) Rank Feature Name

Fraction of Cells with Numeric Characters

1 Number of Values 2 Maximum Value Length 3 Mean # Alphabetic Characters in Cells 4
5 Column Entropy 6
7 Number of None Values 8 Mean Length of Values 9
10 Mean # of Numeric Characters in Cells

Proportion of Unique Values

Fraction of Cells with Alphabetical Characters

Score 1.00
0.79
0.43
0.38
0.35
0.33
0.33
0.28
0.22
0.16

Table 5: Performance contribution of isolated feature sets (b) Top-10 character-level distribution features (out of 960) Feature set
Word embeddings
Character distributions
Paragraph vectors
Global statistics

Num Features 201
960
400
27

F1 Score 0.79
0.78
0.73
0.25

5.4 Feature Importances
We measure feature importance by the total reduction of the Gini impurity criterion brought by that feature to the decision tree model The top 10 most important features from the global statistics and character-level distributions sets are shown in Table 6 While word embedding and paragraph vector features are important, they are difficult to interpret and are therefore omitted Inspecting Table 6a, we find that the “number of values” in a column is the most important feature Certain classes like name and requirements tended to contain fewer values, while others like year and family contained significantly more values The second most important feature is the “maximum value length” in characters, which may differentiate classes with long values, such as address and description, from classes with short values, such as gender and year The top character-level distribution features in Table 6b suggest the importance of specific characters for differentiating between types The third most important feature, the “minimum number of ‘-’ characters”, likely helps determine datetime-related types The fifth most important feature, “whether all values have a ‘,’ character” may also distinguish datetime-related or name-related types Further study of feature importances for semantic type detection is a promising direction for future research 5.5 Rejection Curves
Given unseen data values, Sherlock assesses the probability of those values belonging to each type, then predicts the type with the highest probability Interpreting probabilities as a measure of confidence,
we may want to only label samples with high confidence of belonging to a type To understand the effect of confidence threshold on

Rank Feature Name

Skewness of ‘,’

Sum of ‘D’ across values

1
2 Mean number of ‘M’
3 Minimum number of ‘-’
4
5 Whether all values have a ‘,’
6 Maximum number of ‘g’
7
Skewness of ‘]’
8 Mean number of ‘,’
9 Mean number of ‘z’
10

Sum of ‘n’

Score 1.00
0.77
0.69
0.59
0.47
0.45
0.45
0.40
0.37
0.36

Figure 6: Rejection curves showing performance while rejecting all but the top x% highest confidence samples predictive performance, we present the error-rejection curves of
Sherlock and the decision tree model in Figure 6 By introducing a rejection threshold of 10% of the samples, Sherlock reaches an F1 score of ∼0.95 This significant increase in predictive performance suggests a hybrid approach in which low confidence samples are manually annotated Note that the higher rejection threshold, the lower the error we make in predicting labels,
at the cost of needing more expert capacity 0.00.10.20.30.40.50.60.70.80.91.0Fraction of Samples Rejected0.800.850.900.951.00F1 Score Weighted by SupportNeural Network (Sherlock)Random Forest BaselineModelResearch Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1506KDD ’19, August 4–8, 2019, Anchorage, AK, USA

Hulsebos et al 6 DISCUSSION
We began by considering a set of semantic types described by prior work that identifies correspondences between DBPedia [2]
and WebTables [5] Then, we constructed a dataset consisting of matches between those types with columns in the VizNet [14]
corpus Inspection of these columns suggests that such an approach yields training samples with few false positives After extracting four categories of features describing the values of each column,
we formulate type detection as a multiclass classification task A multi-input neural network demonstrates high predictive performance at the classification task compared to machine learning,
matching-based, and crowdsourced benchmarks We note that using real-world data provides the examples needed to train models that detect many types, at scale We also observe that the test examples frequently include dirty (e , missing or malformed) values, which suggests that real-world data also affords a degree of robustness Measuring and operationalizing these two benefits, especially with out-of-distribution examples, is a promising direction of research Developers have multiple avenues to incorporating ML-based semantic type detection approaches into systems To support the use of Sherlock “out-of-the-box,” we distribute Sherlock as a Python library3 that can be easily installed and incorporated into existing codebases For developers interested in a different set of semantic types, we open source our training and analysis scripts.2 The repository also supports developers wishing to retrain Sherlock using data from their specific data ecologies, such as enterprise or research settings with domain-specific data To close, we identify four promising avenues for future research:
(1) enhancing the quantity and quality of the training data, (2)
increasing the number of considered types, (3) enriching the set of features extracted from each column, and (4) developing shared benchmarks Enhancing data quantity and quality Machine learning model performance is limited by the number of training examples Sherlock is no exception Though the VizNet corpus aggregates datasets from four sources, there is an opportunity to incorporate training examples from additional sources, such as Kaggle,2 datasets included alongside the R statistical environment,3 and the ClueWeb web crawl of Excel spreadsheets.4 We expect increases in training data diversity to improve the robustness and generalizability of
Sherlock Model predictions quality is further determined by the correspondence between training data and unseen testing data, such as datasets uploaded by analysts to a system Our method of matching semantic types with columns from real-world data repositories affords both the harvesting of training samples at scale and the ability to use aspects of dirty data, such as the number of missing values,
as features While we verified the quality of training data through manual inspection, there is an opportunity to label data quality at scale by combining crowdsourcing with active learning By assessing the quality of each training dataset, such an approach would support training semantic type detection models with completely “clean” data at scale com/datasets 3https://github com/vincentarelbundock/Rdatasets 4http://lemurproject php

Increasing number of semantic types To ground our approach in prior work, this paper considered 78 semantic types described by the T2Dv2 Gold Standard While 78 semantic types is a substantial increase over what is supported in existing systems, it is a small subset of entities from existing knowledge bases: the DBPedia ontology [2] covers 685 classes, WordNet [32] contains 175K synonym sets, and Knowledge Graph5 contains millions of entities The entities within these knowledge bases, and hierarchical relationships between entities, provide an abundance of semantic types In lieu of a relevant ontology, researchers can count frequency of column headers in available data to determine which semantic types to consider Such a data-driven approach would ensure the maximum number of training samples for each semantic type Additionally, these surfaced semantic types are potentially more specific to usecase and data ecology, such as data scientists integrating enterprise databases within a company Enriching feature extraction We incorporate four categories of features that describe different aspects of column values A promising approach is to include features that describe relationships between columns (e , correlation, number of overlapping values,
and name similarity), aspects of the entire dataset (e , number of columns), and source context (e , webpage title for scraped tables) Additionally, while we used features to aid interpretation of results,
neural networks using raw data as input are a promising direction of research For example, a character-level recurrent neural network could classify concatenated column values Developing shared benchmarks Despite rich prior research in semantic type detection, we could not find a benchmark with publicly available code that accommodates a larger set of semantic types We therefore incorporated benchmarks that approximated stateof-the-art data systems, to the best of our knowledge However,
domains such as image classification and language translation have benefited from shared benchmarks and test sets Towards this end,
we hope that open-sourcing the data and code used in this paper can benefit future research 7 CONCLUSION
Correctly detecting semantic types is critical to many important data science tasks Machine learning models coupled with largescale data repositories have demonstrated success across domains,
and suggest a promising approach to semantic type detection Sherlock provides a step forward towards this direction REFERENCES [1] Martín Abadi et al TensorFlow: A system for large-scale machine learning In 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16) [2] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
and Zachary Ives DBpedia: A nucleus for a web of open data [3] Alberto Bartoli, Andrea De Lorenzo, Eric Medvet, and Fabiano Tarlao Inference of regular expressions for text extraction from examples IEEE Transactions on Knowledge and Data Engineering 28, 5 (2016), 1217–1230 [4] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor Freebase: A Collaboratively Created Graph Database for Structuring Human
Knowledge In Proceedings of the 2008 ACM SIGMOD International Conference on
Management of Data (SIGMOD ’08) ACM, New York, NY, USA, 1247–1250 5https://developers com/knowledge-graph

Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1507Sherlock: A Deep Learning Approach to Semantic Data Type Detection

KDD ’19, August 4–8, 2019, Anchorage, AK, USA

[5] Michael J Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang WebTables: Exploring the Power of Tables on the Web org/10.14778/1453856.1453916
[6] Raul Castro Fernandez, Ziawasch Abedjan, Famien Koko, Gina Yuan, Samuel
Madden, and Michael Stonebraker Aurum: A Data Discovery System [7] Raul Castro Fernandez, Essam Mansour, Abdulhakim Qahtan, Ahmed Elmagarmid, Ihab Ilyas, Samuel Madden, Mourad Ouzzani, Michael Stonebraker, and
Nan Tang Seeping Semantics: Linking Datasets Using Word Embeddings for Data Discovery org/10.1109/ICDE.2018.00093

[8] Andrew M Dai, Christopher Olah, and Quoc V Le Document embedding

[34] Cong Yan and Yeye He Synthesizing type-detection logic for rich semantic data types using open-source code In Proceedings of the 2018 International
Conference on Management of Data [35] Benjamin Zapilko, Matthäus Zloch, and Johann Schaible Utilizing Regular
Expressions for Instance-Based Schema Matching CEUR Workshop Proceedings 946 A APPENDIX
A.1 Supplemental Tables

http://vega Table 7: 78 semantic types included in this study with paragraph vectors arXiv preprint arXiv:1507.07998 (2015) [9] Interactive Data Lab Datalib: JavaScript Data Utilities io/datalib

[10] Open Knowledge Foundation messytables · PyPi org/

project/messytables

[11] Aman Goel, Craig A Knoblock, and Kristina Lerman Exploiting structure within data for accurate labeling using conditional random fields In Proceedings on the International Conference on Artificial Intelligence (ICAI) Google Data Studio https://datastudio google.com [13] Christopher Groskopf and contributors readthedocs.org

[14] Kevin Hu, Neil Gaikwad, Michiel Bakker, Madelon Hulsebos, Emanuel Zgraggen,
César Hidalgo, Tim Kraska, Guoliang Li, Arvind Satyanarayan, and Çağatay
Demiralp VizNet: Towards a large-scale visualization learning and benchmarking repository In Proceedings of the 2019 Conference on Human Factors in
Computing Systems (CHI) [15] Sean Kandel, Andreas Paepcke, Joseph Hellerstein, and Jeffrey Heer Wrangler: Interactive Visual Specification of Data Transformation Scripts In ACM
Human Factors in Computing Systems (CHI) [16] Tom Kenter and Maarten De Rijke Short text similarity with word embeddings In Proceedings of the 24th ACM international on conference on information and knowledge management [17] Aniket Kittur, Ed H Chi, and Bongwon Suh Crowdsourcing User Studies with Mechanical Turk In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI ’08) ACM, New York, NY, USA, 453–456 [18] Quoc Le and Tomas Mikolov Distributed representations of sentences and documents In International Conference on Machine Learning [19] Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti Annotating and searching web tables using entities, types and relationships Proceedings of the
VLDB Endowment 3, 1-2 (2010), 1338–1347 Power BI | Interactive Data Visualization BI microsoft.com

[21] Fabian Pedregosa et al Scikit-learn: Machine Learning in Python Journal

of Machine Learning Research 12 (2011), 2825–2830 [22] Jeffrey Pennington, Richard Socher, and Christopher Manning Glove:
Global vectors for word representation In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) [23] Minh Pham, Suresh Alse, Craig A Knoblock, and Pedro Szekely Semantic labeling: a domain-independent approach In International Semantic Web Conference Springer, 446–462 [24] Nikhil Waman Puranik A Specialist Approach for Classification of Column

Data University of Maryland, Baltimore County [25] Erhard Rahm and Philip A A Survey of Approaches to Automatic

Schema Matching The VLDB Journal 10, 4 (Dec [26] Vijayshankar Raman and Joseph M Potter’s Wheel: An Interactive Data Cleaning System In Proceedings of the 27th International Conference on Very Large Data Bases (VLDB ’01) Morgan Kaufmann Publishers Inc , San
Francisco, CA, USA, 381–390 [27] S Krishnamurthy Ramnandan, Amol Mittal, Craig A Knoblock, and Pedro Szekely Assigning semantic labels to data sources In European Semantic Web
Conference Springer, 403–417 [28] Radim Řehůřek and Petr Sojka Software Framework for Topic Modelling with Large Corpora In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks [29] Dominique Ritze and Christian Bizer Matching web tables to DBpedia – a

feature utility study context 42, 41 (2017), 19 [30] Zareen Syed, Tim Finin, Varish Mulwad, Anupam Joshi, et al Exploiting a web of semantic data for interpreting tables In Proceedings of the Second Web
Science Conference Data Wrangling Tools & Software trifacta.com [32] Princeton University edu

[33] Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paşca, Warren Shen, Fei
Wu, Gengxin Miao, and Chung Wu Recovering semantics of tables on the web Proceedings of the VLDB Endowment 4, 9 (2011), 528–538 Semantic Types

Code
Collection
Command
Company
Component
Continent
Country
County
Creator
Credit
Currency
Day
Depth
Description Manufacturer

Education
Elevation
Family
File size
Format
Gender
Genre
Grades
Industry
ISBN
Jockey
Language
Location

Address
Affiliate
Affiliation
Age
Album
Area
Artist
Birth date
Birth place
Brand
Capacity
Category
City
Class
Classification Director
Duration
Club

Name
Nationality

Requirement
Result
Sales
Service
Sex
Species
State
Status
Symbol
Team
Team name
Type
Weight
Year

Notes
Operator
Order
Organisation
Origin
Owner
Person
Plays
Position
Product
Publisher
Range
Rank
Ranking
Region
Religion

Table 8: Description of the 27 global statistical features Asterisks (*) denote features included in Venetis et al Feature description
Number of values Fraction of values with unique content *
Fraction of values with numerical characters *
Fraction of values with alphabetical characters of the number of numerical characters in values of the number of alphabetical characters in values of the number special characters in values of the number of words in values *
{Percentage, count, only/has-Boolean} of the None values {Stats, sum, min, max, median, mode, kurtosis, skewness,
any/all-Boolean} of length of values A.2 Mechanical Turk Demographics
Of the 390 participants, 57.18% were male and 0.43% female 1.5%
completed some high school without attaining a diploma, while others had associates (10.5%), bachelor’s (61.0%), master’s (13.1%), or doctorate or professional degree (1.8%) in addition to a high school diploma (12.3%) 26.4% of participants worked with data daily, 33.1%
weekly, 17.2% monthly, and 11.0% annually, while 12.3% never work with data In terms of age: 10.0% of participants were between 18-23,
24-34 (60.3%), 35-40 (13.3%), 41-54 (12.6%), and above 55 (3.8%) Research Track PaperKDD ’19, August 4–8, 2019, Anchorage, AK, USA1508' using model 'nomic-embed-text-v1.5.Q6_K' and embedding pooling method 'mean' from IP address 'localhost'
2024-10-28 15:03:01,661 - rq.worker - WARNING - Moving job to FailedJobRegistry (Work-horse terminated unexpectedly; waitpid returned 11 (signal 11); )
2024-10-28 15:05:19,050 - main - INFO - Application shutdown initiated
2024-10-28 15:05:22,065 - main - INFO - Starting application initialization
2024-10-28 15:05:22,067 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:05:22,072 - db - INFO - Database initialization completed.
2024-10-28 15:05:22,083 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011594 seconds, for an average of 0.0003997931034482759 seconds per hash.
2024-10-28 15:05:22,084 - utils - INFO - Checking models directory...
2024-10-28 15:05:22,084 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:05:22,084 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:05:22,084 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:05:22,085 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:05:22,085 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:05:22,085 - utils - INFO - Model downloads completed.
2024-10-28 15:05:22,090 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:05:22,092 - main - INFO - Application initialization complete
2024-10-28 15:05:45,740 - rq.worker - INFO - Worker worker-48495 [PID 48495]: warm shut down requested
2024-10-28 15:05:45,751 - __main__ - INFO - Worker stopped by user
2024-10-28 15:05:46,409 - rq.scheduler - INFO - Scheduler stopping, releasing locks for document_scans, model_downloads, file_uploads...
2024-10-28 15:05:46,458 - rq.scheduler - INFO - Scheduler with PID 48496 has stopped
2024-10-28 15:05:47,151 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-48495
2024-10-28 15:05:55,314 - __main__ - INFO - Initializing worker...
2024-10-28 15:05:55,314 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 15:05:55,322 - __main__ - INFO - Worker started successfully with PID: 55558
2024-10-28 15:05:57,937 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 15:05:57,940 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:05:57,956 - db - INFO - Database initialization completed.
2024-10-28 15:05:57,977 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.019974 seconds, for an average of 0.0006887586206896551 seconds per hash.
2024-10-28 15:05:57,977 - utils - INFO - Checking models directory...
2024-10-28 15:05:57,977 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:05:57,977 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:05:57,978 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:05:57,978 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:05:57,978 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:05:57,978 - utils - INFO - Model downloads completed.
2024-10-28 15:05:57,987 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:05:57,994 - rq.worker - INFO - Worker rq:worker:worker-55558 started with PID 55558, version 1.16.2
2024-10-28 15:05:57,994 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-55558
2024-10-28 15:05:57,996 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 15:05:58,020 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 15:05:58,024 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 15:05:58,025 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 15:06:00,668 - rq.scheduler - INFO - Scheduler for file_uploads, model_downloads, document_scans started with PID 55562
2024-10-28 15:06:09,249 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:06:09,249 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:06:09,293 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:06:09,297 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:06:09,297 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:06:09,320 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:06:11,534 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:06:11,549 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:06:16,761 - main - INFO - Scan job enqueued successfully. Job ID: c68b3332e7058af9d22a39ad0d215621
2024-10-28 15:06:16,767 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (c68b3332e7058af9d22a39ad0d215621)
2024-10-28 15:06:16,802 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:06:16,803 - worker - INFO - Successfully built FAISS indexes
2024-10-28 15:06:16,803 - worker - INFO - Retrieved FAISS index for nomic-embed-text-v1.5.Q6_K and mean
2024-10-28 15:06:16,804 - worker - INFO - Created embedding request
2024-10-28 15:06:16,806 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:06:16,823 - db - INFO - Database initialization completed.
2024-10-28 15:06:16,836 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012 seconds, for an average of 0.00041379310344827585 seconds per hash.
2024-10-28 15:06:16,837 - utils - INFO - Checking models directory...
2024-10-28 15:06:16,837 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:06:16,838 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:06:16,839 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:06:16,839 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:06:16,839 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:06:16,840 - utils - INFO - Model downloads completed.
2024-10-28 15:06:16,840 - functions - ERROR - Error in get_or_compute_embedding: name 'hashlib' is not defined
2024-10-28 15:06:16,843 - functions - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 423, in get_or_compute_embedding
    cache_key = f"embedding:{embedding_request.llm_model_name}:{embedding_request.embedding_pooling_method}:{hashlib.md5(embedding_request.text.encode()).hexdigest()}"
NameError: name 'hashlib' is not defined

2024-10-28 15:06:16,843 - worker - ERROR - Error processing embeddings: name 'hashlib' is not defined
2024-10-28 15:06:16,844 - worker - ERROR - Error in scan_document_task: name 'hashlib' is not defined
2024-10-28 15:06:16,846 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 326, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, None)
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 423, in get_or_compute_embedding
    cache_key = f"embedding:{embedding_request.llm_model_name}:{embedding_request.embedding_pooling_method}:{hashlib.md5(embedding_request.text.encode()).hexdigest()}"
NameError: name 'hashlib' is not defined

2024-10-28 15:06:16,854 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (c68b3332e7058af9d22a39ad0d215621)
2024-10-28 15:06:16,855 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:07:26,519 - main - INFO - Application shutdown initiated
2024-10-28 15:07:29,454 - main - INFO - Starting application initialization
2024-10-28 15:07:29,455 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:07:29,460 - db - INFO - Database initialization completed.
2024-10-28 15:07:29,472 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011621 seconds, for an average of 0.00040072413793103447 seconds per hash.
2024-10-28 15:07:29,472 - utils - INFO - Checking models directory...
2024-10-28 15:07:29,472 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:07:29,473 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:07:29,473 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:07:29,473 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:07:29,473 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:07:29,473 - utils - INFO - Model downloads completed.
2024-10-28 15:07:29,479 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:07:29,480 - main - INFO - Application initialization complete
2024-10-28 15:08:05,560 - main - INFO - Application shutdown initiated
2024-10-28 15:08:08,497 - main - INFO - Starting application initialization
2024-10-28 15:08:08,499 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:08:08,503 - db - INFO - Database initialization completed.
2024-10-28 15:08:08,514 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011089 seconds, for an average of 0.0003823793103448276 seconds per hash.
2024-10-28 15:08:08,515 - utils - INFO - Checking models directory...
2024-10-28 15:08:08,515 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:08:08,515 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:08:08,516 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:08:08,516 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:08:08,516 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:08:08,516 - utils - INFO - Model downloads completed.
2024-10-28 15:08:08,521 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:08:08,522 - main - INFO - Application initialization complete
2024-10-28 15:09:16,655 - main - INFO - Application shutdown initiated
2024-10-28 15:09:19,403 - main - INFO - Starting application initialization
2024-10-28 15:09:19,404 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:09:19,409 - db - INFO - Database initialization completed.
2024-10-28 15:09:19,421 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01234 seconds, for an average of 0.00042551724137931035 seconds per hash.
2024-10-28 15:09:19,422 - utils - INFO - Checking models directory...
2024-10-28 15:09:19,422 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:09:19,422 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:09:19,422 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:09:19,422 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:09:19,422 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:09:19,422 - utils - INFO - Model downloads completed.
2024-10-28 15:09:19,428 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:09:19,429 - main - INFO - Application initialization complete
2024-10-28 15:12:13,102 - main - INFO - Application shutdown initiated
2024-10-28 15:12:15,905 - main - INFO - Starting application initialization
2024-10-28 15:12:15,906 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:12:15,913 - db - INFO - Database initialization completed.
2024-10-28 15:12:15,927 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.013695 seconds, for an average of 0.00047224137931034483 seconds per hash.
2024-10-28 15:12:15,928 - utils - INFO - Checking models directory...
2024-10-28 15:12:15,928 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:12:15,928 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:12:15,928 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:12:15,929 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:12:15,929 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:12:15,929 - utils - INFO - Model downloads completed.
2024-10-28 15:12:15,935 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:12:15,936 - main - INFO - Application initialization complete
2024-10-28 15:12:29,248 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:12:29,248 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:12:29,292 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:12:29,294 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:12:29,295 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:12:29,322 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:12:32,731 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 15:12:32,740 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 15:12:38,394 - main - INFO - Scan job enqueued successfully. Job ID: 58cc6a1a721bc3ef7d05dce6b80a77c1
2024-10-28 15:12:38,397 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task("RS\n\n\uf1b2 hnsw-0.11.0  ▼\n\n\uf085 Platform  ▼\n\n🏴 Feature ﬂags\n\nRus..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (58cc6a1a721bc3ef7d05dce6b80a77c1)
2024-10-28 15:12:38,438 - worker - INFO - Starting scan task with parameters: query_text_length=8664, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:12:38,439 - worker - INFO - Successfully built FAISS indexes
2024-10-28 15:12:38,440 - worker - INFO - Retrieved FAISS index for nomic-embed-text-v1.5.Q6_K and mean
2024-10-28 15:12:38,440 - worker - INFO - Created embedding request
2024-10-28 15:12:38,442 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:12:38,460 - db - INFO - Database initialization completed.
2024-10-28 15:12:38,474 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.0131 seconds, for an average of 0.0004517241379310345 seconds per hash.
2024-10-28 15:12:38,475 - utils - INFO - Checking models directory...
2024-10-28 15:12:38,475 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:12:38,476 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:12:38,476 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:12:38,476 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:12:38,477 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:12:38,477 - utils - INFO - Model downloads completed.
2024-10-28 15:12:38,477 - functions - ERROR - Error in get_or_compute_embedding: name 'hashlib' is not defined
2024-10-28 15:12:38,481 - functions - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 423, in get_or_compute_embedding
    # Create cache key
NameError: name 'hashlib' is not defined

2024-10-28 15:12:38,481 - worker - ERROR - Error processing embeddings: name 'hashlib' is not defined
2024-10-28 15:12:38,481 - worker - ERROR - Error in scan_document_task: name 'hashlib' is not defined
2024-10-28 15:12:38,483 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 326, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, None)
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 423, in get_or_compute_embedding
    # Create cache key
NameError: name 'hashlib' is not defined

2024-10-28 15:12:38,487 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (58cc6a1a721bc3ef7d05dce6b80a77c1)
2024-10-28 15:12:38,487 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:13:37,168 - main - INFO - Application shutdown initiated
2024-10-28 15:13:39,994 - main - INFO - Starting application initialization
2024-10-28 15:13:39,996 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:13:40,002 - db - INFO - Database initialization completed.
2024-10-28 15:13:40,015 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.013067 seconds, for an average of 0.00045058620689655176 seconds per hash.
2024-10-28 15:13:40,015 - utils - INFO - Checking models directory...
2024-10-28 15:13:40,015 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:13:40,016 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:13:40,016 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:13:40,016 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:13:40,016 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:13:40,016 - utils - INFO - Model downloads completed.
2024-10-28 15:13:40,022 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:13:40,023 - main - INFO - Application initialization complete
2024-10-28 15:13:43,286 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:13:43,286 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:13:43,326 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 15:13:43,328 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:13:43,332 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 15:13:43,333 - main - INFO - Processed 1 semantic data types
2024-10-28 15:13:51,607 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:13:51,607 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:13:51,648 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:13:51,653 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:13:51,653 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:13:51,687 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:13:53,956 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 15:13:53,962 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 15:13:59,599 - main - INFO - Scan job enqueued successfully. Job ID: fa789a0a1fbec29660ed81d0e57a9de2
2024-10-28 15:13:59,601 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task("RS\n\n\uf1b2 hnsw-0.11.0  ▼\n\n\uf085 Platform  ▼\n\n🏴 Feature ﬂags\n\nRus..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (fa789a0a1fbec29660ed81d0e57a9de2)
2024-10-28 15:13:59,638 - worker - INFO - Starting scan task with parameters: query_text_length=8664, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:13:59,639 - worker - INFO - Successfully built FAISS indexes
2024-10-28 15:13:59,640 - worker - INFO - Retrieved FAISS index for nomic-embed-text-v1.5.Q6_K and mean
2024-10-28 15:13:59,640 - worker - INFO - Created embedding request
2024-10-28 15:13:59,642 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:13:59,660 - db - INFO - Database initialization completed.
2024-10-28 15:13:59,672 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01229 seconds, for an average of 0.0004237931034482759 seconds per hash.
2024-10-28 15:13:59,673 - utils - INFO - Checking models directory...
2024-10-28 15:13:59,674 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:13:59,675 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:13:59,675 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:13:59,675 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:13:59,676 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:13:59,676 - utils - INFO - Model downloads completed.
2024-10-28 15:13:59,676 - functions - ERROR - Error in get_or_compute_embedding: name 'hashlib' is not defined
2024-10-28 15:13:59,681 - functions - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 423, in get_or_compute_embedding
NameError: name 'hashlib' is not defined

2024-10-28 15:13:59,681 - worker - ERROR - Error processing embeddings: name 'hashlib' is not defined
2024-10-28 15:13:59,682 - worker - ERROR - Error in scan_document_task: name 'hashlib' is not defined
2024-10-28 15:13:59,685 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 326, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, None)
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 423, in get_or_compute_embedding
NameError: name 'hashlib' is not defined

2024-10-28 15:13:59,704 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (fa789a0a1fbec29660ed81d0e57a9de2)
2024-10-28 15:13:59,704 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:14:05,913 - rq.worker - INFO - Worker worker-55558 [PID 55558]: warm shut down requested
2024-10-28 15:14:05,914 - __main__ - INFO - Worker stopped by user
2024-10-28 15:14:06,603 - rq.scheduler - INFO - Scheduler stopping, releasing locks for file_uploads, model_downloads, document_scans...
2024-10-28 15:14:06,609 - rq.scheduler - INFO - Scheduler with PID 55562 has stopped
2024-10-28 15:14:07,178 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-55558
2024-10-28 15:14:13,016 - __main__ - INFO - Initializing worker...
2024-10-28 15:14:13,017 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 15:14:13,026 - __main__ - INFO - Worker started successfully with PID: 56297
2024-10-28 15:14:15,450 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 15:14:15,454 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:14:15,469 - db - INFO - Database initialization completed.
2024-10-28 15:14:15,489 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.020022 seconds, for an average of 0.0006904137931034483 seconds per hash.
2024-10-28 15:14:15,489 - utils - INFO - Checking models directory...
2024-10-28 15:14:15,490 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:14:15,490 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:14:15,490 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:14:15,490 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:14:15,490 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:14:15,490 - utils - INFO - Model downloads completed.
2024-10-28 15:14:15,500 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:14:15,523 - rq.worker - INFO - Worker rq:worker:worker-56297 started with PID 56297, version 1.16.2
2024-10-28 15:14:15,524 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-56297
2024-10-28 15:14:15,525 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 15:14:15,533 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 15:14:15,535 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 15:14:15,536 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 15:14:18,065 - rq.scheduler - INFO - Scheduler for model_downloads, file_uploads, document_scans started with PID 56305
2024-10-28 15:14:22,103 - rq.worker - INFO - Worker worker-56297 [PID 56297]: warm shut down requested
2024-10-28 15:14:22,103 - __main__ - INFO - Worker stopped by user
2024-10-28 15:14:23,109 - rq.scheduler - INFO - Scheduler stopping, releasing locks for model_downloads, file_uploads, document_scans...
2024-10-28 15:14:23,112 - rq.scheduler - INFO - Scheduler with PID 56305 has stopped
2024-10-28 15:14:23,543 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-56297
2024-10-28 15:14:40,886 - __main__ - INFO - Initializing worker...
2024-10-28 15:14:40,886 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 15:14:40,894 - __main__ - INFO - Worker started successfully with PID: 56441
2024-10-28 15:14:43,032 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 15:14:43,033 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:14:43,039 - db - INFO - Database initialization completed.
2024-10-28 15:14:43,050 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01121 seconds, for an average of 0.000386551724137931 seconds per hash.
2024-10-28 15:14:43,050 - utils - INFO - Checking models directory...
2024-10-28 15:14:43,050 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:14:43,050 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:14:43,050 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:14:43,051 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:14:43,051 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:14:43,051 - utils - INFO - Model downloads completed.
2024-10-28 15:14:43,056 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:14:43,060 - rq.worker - INFO - Worker rq:worker:worker-56441 started with PID 56441, version 1.16.2
2024-10-28 15:14:43,060 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-56441
2024-10-28 15:14:43,067 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 15:14:43,077 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 15:14:43,079 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 15:14:43,080 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 15:14:45,457 - rq.scheduler - INFO - Scheduler for file_uploads, document_scans, model_downloads started with PID 56443
2024-10-28 15:14:58,206 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:14:58,206 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:14:58,288 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:14:58,291 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:14:58,291 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:14:58,321 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:15:00,543 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:15:00,559 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:15:05,606 - main - INFO - Scan job enqueued successfully. Job ID: 767e096dd9f03d7afeebf5d439f69b0e
2024-10-28 15:15:05,613 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (767e096dd9f03d7afeebf5d439f69b0e)
2024-10-28 15:15:05,654 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:15:05,655 - worker - INFO - Successfully built FAISS indexes
2024-10-28 15:15:05,655 - worker - INFO - Retrieved FAISS index for nomic-embed-text-v1.5.Q6_K and mean
2024-10-28 15:15:05,656 - worker - INFO - Created embedding request
2024-10-28 15:15:05,657 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:15:05,665 - db - INFO - Database initialization completed.
2024-10-28 15:15:05,672 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.006176 seconds, for an average of 0.0002129655172413793 seconds per hash.
2024-10-28 15:15:05,673 - utils - INFO - Checking models directory...
2024-10-28 15:15:05,673 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:15:05,674 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:15:05,675 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:15:05,675 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:15:05,675 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:15:05,676 - utils - INFO - Model downloads completed.
2024-10-28 15:15:05,676 - functions - ERROR - Error in get_or_compute_embedding: name 'hashlib' is not defined
2024-10-28 15:15:05,680 - functions - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 425, in get_or_compute_embedding
    cache_key = f"embedding:{embedding_request.llm_model_name}:{embedding_request.embedding_pooling_method}:{hashlib.md5(embedding_request.text.encode()).hexdigest()}"
NameError: name 'hashlib' is not defined

2024-10-28 15:15:05,680 - worker - ERROR - Error processing embeddings: name 'hashlib' is not defined
2024-10-28 15:15:05,680 - worker - ERROR - Error in scan_document_task: name 'hashlib' is not defined
2024-10-28 15:15:05,682 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 326, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, None)
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 425, in get_or_compute_embedding
    cache_key = f"embedding:{embedding_request.llm_model_name}:{embedding_request.embedding_pooling_method}:{hashlib.md5(embedding_request.text.encode()).hexdigest()}"
NameError: name 'hashlib' is not defined

2024-10-28 15:15:05,691 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (767e096dd9f03d7afeebf5d439f69b0e)
2024-10-28 15:15:05,694 - asyncio - ERROR - Task was destroyed but it is pending!
task: <Task pending name='Task-11' coro=<DatabaseWriter.dedicated_db_writer() done, defined at /Users/sidmohan/Desktop/codexify/backend/src/db.py:190> wait_for=<Future cancelled>>
2024-10-28 15:15:05,692 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:16:21,931 - main - INFO - Application shutdown initiated
2024-10-28 15:16:24,723 - main - INFO - Starting application initialization
2024-10-28 15:16:24,724 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:16:24,733 - db - INFO - Database initialization completed.
2024-10-28 15:16:24,748 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.014733 seconds, for an average of 0.0005080344827586207 seconds per hash.
2024-10-28 15:16:24,748 - utils - INFO - Checking models directory...
2024-10-28 15:16:24,748 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:16:24,749 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:16:24,749 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:16:24,749 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:16:24,749 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:16:24,749 - utils - INFO - Model downloads completed.
2024-10-28 15:16:24,755 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:16:24,757 - main - INFO - Application initialization complete
2024-10-28 15:16:33,655 - main - INFO - Application shutdown initiated
2024-10-28 15:16:36,381 - main - INFO - Starting application initialization
2024-10-28 15:16:36,383 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:16:36,387 - db - INFO - Database initialization completed.
2024-10-28 15:16:36,400 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012472 seconds, for an average of 0.0004300689655172414 seconds per hash.
2024-10-28 15:16:36,400 - utils - INFO - Checking models directory...
2024-10-28 15:16:36,400 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:16:36,400 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:16:36,400 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:16:36,401 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:16:36,401 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:16:36,401 - utils - INFO - Model downloads completed.
2024-10-28 15:16:36,406 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:16:36,407 - main - INFO - Application initialization complete
2024-10-28 15:16:42,779 - main - INFO - Application shutdown initiated
2024-10-28 15:16:45,460 - main - INFO - Starting application initialization
2024-10-28 15:16:45,461 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:16:45,466 - db - INFO - Database initialization completed.
2024-10-28 15:16:45,478 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01149 seconds, for an average of 0.00039620689655172416 seconds per hash.
2024-10-28 15:16:45,478 - utils - INFO - Checking models directory...
2024-10-28 15:16:45,478 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:16:45,478 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:16:45,479 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:16:45,479 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:16:45,479 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:16:45,479 - utils - INFO - Model downloads completed.
2024-10-28 15:16:45,484 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:16:45,485 - main - INFO - Application initialization complete
2024-10-28 15:17:07,036 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:17:07,036 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:17:07,103 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:17:07,106 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:17:07,106 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:17:07,138 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:17:11,081 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:17:11,096 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:17:15,096 - main - INFO - Scan job enqueued successfully. Job ID: 233604ce4714811479824c61677fe0c0
2024-10-28 15:17:15,099 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (233604ce4714811479824c61677fe0c0)
2024-10-28 15:17:15,160 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:17:15,161 - worker - INFO - Successfully built FAISS indexes
2024-10-28 15:17:15,162 - worker - INFO - Retrieved FAISS index for nomic-embed-text-v1.5.Q6_K and mean
2024-10-28 15:17:15,162 - worker - INFO - Created embedding request
2024-10-28 15:17:15,163 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:17:15,171 - db - INFO - Database initialization completed.
2024-10-28 15:17:15,177 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.0061 seconds, for an average of 0.0002103448275862069 seconds per hash.
2024-10-28 15:17:15,178 - utils - INFO - Checking models directory...
2024-10-28 15:17:15,179 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:17:15,179 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:17:15,180 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:17:15,180 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:17:15,181 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:17:15,181 - utils - INFO - Model downloads completed.
2024-10-28 15:17:15,181 - functions - ERROR - Error in get_or_compute_embedding: name 'hashlib' is not defined
2024-10-28 15:17:15,185 - functions - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 425, in get_or_compute_embedding
NameError: name 'hashlib' is not defined

2024-10-28 15:17:15,186 - worker - ERROR - Error processing embeddings: name 'hashlib' is not defined
2024-10-28 15:17:15,186 - worker - ERROR - Error in scan_document_task: name 'hashlib' is not defined
2024-10-28 15:17:15,189 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 326, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, None)
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 425, in get_or_compute_embedding
NameError: name 'hashlib' is not defined

2024-10-28 15:17:15,197 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (233604ce4714811479824c61677fe0c0)
2024-10-28 15:17:15,198 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:17:41,029 - rq.worker - INFO - Worker worker-56441 [PID 56441]: warm shut down requested
2024-10-28 15:17:41,029 - __main__ - INFO - Worker stopped by user
2024-10-28 15:17:41,625 - rq.scheduler - INFO - Scheduler stopping, releasing locks for file_uploads, document_scans, model_downloads...
2024-10-28 15:17:41,629 - rq.scheduler - INFO - Scheduler with PID 56443 has stopped
2024-10-28 15:17:42,084 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-56441
2024-10-28 15:17:49,342 - __main__ - INFO - Initializing worker...
2024-10-28 15:17:49,342 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 15:17:49,350 - __main__ - INFO - Worker started successfully with PID: 56869
2024-10-28 15:17:51,586 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 15:17:51,588 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:17:51,595 - db - INFO - Database initialization completed.
2024-10-28 15:17:51,608 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012426 seconds, for an average of 0.00042848275862068965 seconds per hash.
2024-10-28 15:17:51,608 - utils - INFO - Checking models directory...
2024-10-28 15:17:51,608 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:17:51,608 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:17:51,609 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:17:51,609 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:17:51,609 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:17:51,609 - utils - INFO - Model downloads completed.
2024-10-28 15:17:51,615 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:17:51,620 - rq.worker - INFO - Worker rq:worker:worker-56869 started with PID 56869, version 1.16.2
2024-10-28 15:17:51,620 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-56869
2024-10-28 15:17:51,623 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 15:17:51,631 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 15:17:51,634 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 15:17:51,635 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 15:17:54,221 - rq.scheduler - INFO - Scheduler for file_uploads, model_downloads, document_scans started with PID 56871
2024-10-28 15:17:54,744 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:17:54,744 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:17:54,796 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:17:54,800 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:17:54,800 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:17:54,826 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:17:57,933 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 15:17:57,937 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 15:18:02,479 - main - INFO - Scan job enqueued successfully. Job ID: 4ce58167b3c3b9b3b942200edf31f628
2024-10-28 15:18:02,483 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task("RS\n\n\uf1b2 hnsw-0.11.0  ▼\n\n\uf085 Platform  ▼\n\n🏴 Feature ﬂags\n\nRus..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (4ce58167b3c3b9b3b942200edf31f628)
2024-10-28 15:18:02,520 - worker - INFO - Starting scan task with parameters: query_text_length=8664, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:18:02,522 - worker - INFO - Successfully built FAISS indexes
2024-10-28 15:18:02,522 - worker - INFO - Retrieved FAISS index for nomic-embed-text-v1.5.Q6_K and mean
2024-10-28 15:18:02,523 - worker - INFO - Created embedding request
2024-10-28 15:18:02,524 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:18:02,530 - db - INFO - Database initialization completed.
2024-10-28 15:18:02,537 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.005725 seconds, for an average of 0.00019741379310344827 seconds per hash.
2024-10-28 15:18:02,538 - utils - INFO - Checking models directory...
2024-10-28 15:18:02,538 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:18:02,540 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:18:02,540 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:18:02,541 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:18:02,541 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:18:02,542 - utils - INFO - Model downloads completed.
2024-10-28 15:18:02,555 - functions - ERROR - Error computing embedding: 'str' object has no attribute 'create_embedding'
2024-10-28 15:18:02,556 - functions - ERROR - Error in get_or_compute_embedding: 'str' object has no attribute 'create_embedding'
2024-10-28 15:18:02,558 - functions - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 436, in get_or_compute_embedding
    embedding_instance = await calculate_sentence_embeddings_list(
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 482, in calculate_sentence_embeddings_list
    sentence_embeddings_object = llama.create_embedding(texts)
AttributeError: 'str' object has no attribute 'create_embedding'

2024-10-28 15:18:02,558 - worker - ERROR - Error processing embeddings: 'str' object has no attribute 'create_embedding'
2024-10-28 15:18:02,559 - worker - ERROR - Error in scan_document_task: 'str' object has no attribute 'create_embedding'
2024-10-28 15:18:02,560 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 326, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, None)
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 436, in get_or_compute_embedding
    embedding_instance = await calculate_sentence_embeddings_list(
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 482, in calculate_sentence_embeddings_list
    sentence_embeddings_object = llama.create_embedding(texts)
AttributeError: 'str' object has no attribute 'create_embedding'

2024-10-28 15:18:02,564 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (4ce58167b3c3b9b3b942200edf31f628)
2024-10-28 15:18:02,564 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:21:44,332 - main - INFO - Application shutdown initiated
2024-10-28 15:21:47,345 - main - INFO - Starting application initialization
2024-10-28 15:21:47,347 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:21:47,352 - db - INFO - Database initialization completed.
2024-10-28 15:21:47,364 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011843 seconds, for an average of 0.0004083793103448276 seconds per hash.
2024-10-28 15:21:47,364 - utils - INFO - Checking models directory...
2024-10-28 15:21:47,364 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:21:47,365 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:21:47,365 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:21:47,365 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:21:47,365 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:21:47,365 - utils - INFO - Model downloads completed.
2024-10-28 15:21:47,372 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:21:47,373 - main - INFO - Application initialization complete
2024-10-28 15:21:51,420 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:21:51,420 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:21:51,471 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:21:51,482 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:21:51,482 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:21:51,508 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:21:53,319 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 15:21:53,325 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 15:21:57,484 - main - INFO - Scan job enqueued successfully. Job ID: 642ad4adb969defb6908748c72f52883
2024-10-28 15:21:57,486 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task("RS\n\n\uf1b2 hnsw-0.11.0  ▼\n\n\uf085 Platform  ▼\n\n🏴 Feature ﬂags\n\nRus..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (642ad4adb969defb6908748c72f52883)
2024-10-28 15:21:57,529 - worker - INFO - Starting scan task with parameters: query_text_length=8664, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:21:57,530 - worker - INFO - Successfully built FAISS indexes
2024-10-28 15:21:57,530 - worker - INFO - Retrieved FAISS index for nomic-embed-text-v1.5.Q6_K and mean
2024-10-28 15:21:57,531 - worker - INFO - Created embedding request
2024-10-28 15:21:57,532 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:21:57,550 - db - INFO - Database initialization completed.
2024-10-28 15:21:57,559 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.007009 seconds, for an average of 0.00024168965517241378 seconds per hash.
2024-10-28 15:21:57,560 - utils - INFO - Checking models directory...
2024-10-28 15:21:57,560 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:21:57,561 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:21:57,561 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:21:57,562 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:21:57,562 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:21:57,563 - utils - INFO - Model downloads completed.
2024-10-28 15:21:57,565 - functions - ERROR - Error computing embedding: 'str' object has no attribute 'create_embedding'
2024-10-28 15:21:57,566 - functions - ERROR - Error in get_or_compute_embedding: 'str' object has no attribute 'create_embedding'
2024-10-28 15:21:57,569 - functions - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 436, in get_or_compute_embedding
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 482, in calculate_sentence_embeddings_list
    start_time = datetime.utcnow()
AttributeError: 'str' object has no attribute 'create_embedding'

2024-10-28 15:21:57,570 - worker - ERROR - Error processing embeddings: 'str' object has no attribute 'create_embedding'
2024-10-28 15:21:57,570 - worker - ERROR - Error in scan_document_task: 'str' object has no attribute 'create_embedding'
2024-10-28 15:21:57,571 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 326, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, None)
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 436, in get_or_compute_embedding
  File "/Users/sidmohan/Desktop/codexify/backend/src/functions.py", line 482, in calculate_sentence_embeddings_list
    start_time = datetime.utcnow()
AttributeError: 'str' object has no attribute 'create_embedding'

2024-10-28 15:21:57,574 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (642ad4adb969defb6908748c72f52883)
2024-10-28 15:21:57,574 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:22:58,841 - main - INFO - Application shutdown initiated
2024-10-28 15:23:01,951 - main - INFO - Starting application initialization
2024-10-28 15:23:01,952 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:23:01,957 - db - INFO - Database initialization completed.
2024-10-28 15:23:01,969 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011565 seconds, for an average of 0.00039879310344827587 seconds per hash.
2024-10-28 15:23:01,969 - utils - INFO - Checking models directory...
2024-10-28 15:23:01,969 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:23:01,969 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:23:01,970 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:23:01,970 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:23:01,970 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:23:01,970 - utils - INFO - Model downloads completed.
2024-10-28 15:23:01,975 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:23:01,977 - main - INFO - Application initialization complete
2024-10-28 15:23:15,522 - main - INFO - Application shutdown initiated
2024-10-28 15:23:21,512 - main - INFO - Starting application initialization
2024-10-28 15:23:21,513 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:23:21,518 - db - INFO - Database initialization completed.
2024-10-28 15:23:21,531 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01212 seconds, for an average of 0.00041793103448275863 seconds per hash.
2024-10-28 15:23:21,531 - utils - INFO - Checking models directory...
2024-10-28 15:23:21,531 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:23:21,531 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:23:21,531 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:23:21,531 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:23:21,532 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:23:21,532 - utils - INFO - Model downloads completed.
2024-10-28 15:23:21,537 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:23:21,538 - main - INFO - Application initialization complete
2024-10-28 15:23:25,115 - __main__ - INFO - Worker stopped by user
2024-10-28 15:23:25,115 - rq.worker - INFO - Worker worker-56869 [PID 56869]: warm shut down requested
2024-10-28 15:23:25,124 - rq.scheduler - INFO - Scheduler stopping, releasing locks for file_uploads, model_downloads, document_scans...
2024-10-28 15:23:25,127 - rq.scheduler - INFO - Scheduler with PID 56871 has stopped
2024-10-28 15:23:25,553 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-56869
2024-10-28 15:23:30,940 - __main__ - INFO - Initializing worker...
2024-10-28 15:23:30,940 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 15:23:30,948 - __main__ - INFO - Worker started successfully with PID: 57408
2024-10-28 15:23:33,089 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 15:23:33,091 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:23:33,096 - db - INFO - Database initialization completed.
2024-10-28 15:23:33,108 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012495 seconds, for an average of 0.0004308620689655172 seconds per hash.
2024-10-28 15:23:33,109 - utils - INFO - Checking models directory...
2024-10-28 15:23:33,109 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:23:33,109 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:23:33,109 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:23:33,109 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:23:33,109 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:23:33,109 - utils - INFO - Model downloads completed.
2024-10-28 15:23:33,116 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:23:33,124 - rq.worker - INFO - Worker rq:worker:worker-57408 started with PID 57408, version 1.16.2
2024-10-28 15:23:33,124 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-57408
2024-10-28 15:23:33,127 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 15:23:33,158 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 15:23:33,168 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 15:23:33,172 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 15:23:35,493 - rq.scheduler - INFO - Scheduler for file_uploads, model_downloads, document_scans started with PID 57413
2024-10-28 15:23:44,865 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 15:23:44,872 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 15:23:44,873 - main - INFO - Processed 1 semantic data types
2024-10-28 15:23:44,875 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 15:23:44,876 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 15:23:44,876 - main - INFO - Processed 1 semantic data types
2024-10-28 15:23:45,731 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:23:45,731 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:23:45,782 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:23:45,785 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:23:45,785 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:23:45,807 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:23:50,964 - main - INFO - Retrieving content for document with document_hash: string
2024-10-28 15:23:50,971 - main - INFO - Retrieved content for document with document_hash: string
2024-10-28 15:23:57,540 - main - INFO - Scan job enqueued successfully. Job ID: e727c324c1563f2d1bfadc1d5518c8a3
2024-10-28 15:23:57,543 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Inspird\nNon-Disclosure Agreement (NDA)\n\nTHIS AGREEMENT IS DATED: Octobe..., 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'spearman_rho')[39;49;00m (e727c324c1563f2d1bfadc1d5518c8a3)
2024-10-28 15:23:57,589 - worker - INFO - Starting scan task with parameters: query_text_length=18369, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:23:57,590 - worker - INFO - Successfully built FAISS indexes
2024-10-28 15:23:57,590 - worker - INFO - Retrieved FAISS index for nomic-embed-text-v1.5.Q6_K and mean
2024-10-28 15:23:57,591 - worker - INFO - Created embedding request
2024-10-28 15:23:57,592 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:23:57,600 - db - INFO - Database initialization completed.
2024-10-28 15:23:57,607 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.00682 seconds, for an average of 0.00023517241379310342 seconds per hash.
2024-10-28 15:23:57,609 - utils - INFO - Checking models directory...
2024-10-28 15:23:57,610 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:23:57,611 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:23:57,611 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:23:57,612 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:23:57,612 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:23:57,612 - utils - INFO - Model downloads completed.
2024-10-28 15:23:57,849 - rq.worker - WARNING - Moving job to FailedJobRegistry (Work-horse terminated unexpectedly; waitpid returned 11 (signal 11); )
2024-10-28 15:27:25,553 - main - INFO - Application shutdown initiated
2024-10-28 15:27:28,537 - main - INFO - Starting application initialization
2024-10-28 15:27:28,540 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:27:28,546 - db - INFO - Database initialization completed.
2024-10-28 15:27:28,559 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012622 seconds, for an average of 0.0004352413793103448 seconds per hash.
2024-10-28 15:27:28,559 - utils - INFO - Checking models directory...
2024-10-28 15:27:28,559 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:27:28,560 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:27:28,560 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:27:28,560 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:27:28,560 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:27:28,560 - utils - INFO - Model downloads completed.
2024-10-28 15:27:28,575 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:27:28,581 - main - INFO - Application initialization complete
2024-10-28 15:27:30,866 - main - INFO - Starting application initialization
2024-10-28 15:27:30,868 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:27:30,873 - db - INFO - Database initialization completed.
2024-10-28 15:27:30,888 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.015437 seconds, for an average of 0.0005323103448275862 seconds per hash.
2024-10-28 15:27:30,888 - utils - INFO - Checking models directory...
2024-10-28 15:27:30,888 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:27:30,889 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:27:30,889 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:27:30,889 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:27:30,889 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:27:30,889 - utils - INFO - Model downloads completed.
2024-10-28 15:27:30,894 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:27:30,895 - main - INFO - Application initialization complete
2024-10-28 15:27:35,508 - rq.worker - INFO - Worker worker-57408 [PID 57408]: warm shut down requested
2024-10-28 15:27:35,508 - __main__ - INFO - Worker stopped by user
2024-10-28 15:27:35,949 - rq.scheduler - INFO - Scheduler stopping, releasing locks for file_uploads, model_downloads, document_scans...
2024-10-28 15:27:35,957 - rq.scheduler - INFO - Scheduler with PID 57413 has stopped
2024-10-28 15:27:36,411 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-57408
2024-10-28 15:28:21,751 - __main__ - INFO - Initializing worker...
2024-10-28 15:28:21,751 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 15:28:21,759 - __main__ - INFO - Worker started successfully with PID: 57987
2024-10-28 15:28:23,976 - __mp_main__ - ERROR - Redis connection error: current limit exceeds maximum limit
2024-10-28 15:29:35,025 - main - INFO - Application shutdown initiated
2024-10-28 15:29:37,693 - main - INFO - Starting application initialization
2024-10-28 15:29:37,695 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:29:37,699 - db - INFO - Database initialization completed.
2024-10-28 15:29:37,710 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011671 seconds, for an average of 0.000402448275862069 seconds per hash.
2024-10-28 15:29:37,711 - utils - INFO - Checking models directory...
2024-10-28 15:29:37,711 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:29:37,711 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:29:37,711 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:29:37,711 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:29:37,712 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:29:37,712 - utils - INFO - Model downloads completed.
2024-10-28 15:29:37,717 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:29:37,718 - main - INFO - Application initialization complete
2024-10-28 15:29:41,294 - __main__ - INFO - Initializing worker...
2024-10-28 15:29:41,294 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 15:29:41,303 - __main__ - INFO - Worker started successfully with PID: 58118
2024-10-28 15:29:43,657 - __mp_main__ - ERROR - Redis connection error: current limit exceeds maximum limit
2024-10-28 15:29:45,767 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:29:45,767 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:29:45,807 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:29:45,810 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:29:45,810 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:29:45,833 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:30:15,346 - main - INFO - Application shutdown initiated
2024-10-28 15:30:17,997 - main - INFO - Starting application initialization
2024-10-28 15:30:18,003 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:30:18,008 - db - INFO - Database initialization completed.
2024-10-28 15:30:18,020 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011904 seconds, for an average of 0.00041048275862068964 seconds per hash.
2024-10-28 15:30:18,020 - utils - INFO - Checking models directory...
2024-10-28 15:30:18,020 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:30:18,020 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:30:18,020 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:30:18,020 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:30:18,020 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:30:18,021 - utils - INFO - Model downloads completed.
2024-10-28 15:30:18,026 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:30:18,027 - main - INFO - Application initialization complete
2024-10-28 15:30:20,927 - __main__ - INFO - Initializing worker...
2024-10-28 15:30:20,927 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 15:30:20,935 - __main__ - INFO - Worker started successfully with PID: 58254
2024-10-28 15:30:23,106 - __mp_main__ - WARNING - Failed to set memory limit: current limit exceeds maximum limit. Continuing without limit.
2024-10-28 15:30:23,106 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 15:30:23,107 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:30:23,111 - db - INFO - Database initialization completed.
2024-10-28 15:30:23,123 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011951 seconds, for an average of 0.00041210344827586205 seconds per hash.
2024-10-28 15:30:23,123 - utils - INFO - Checking models directory...
2024-10-28 15:30:23,123 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:30:23,123 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:30:23,124 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:30:23,124 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:30:23,124 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:30:23,124 - utils - INFO - Model downloads completed.
2024-10-28 15:30:23,128 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:30:23,144 - rq.worker - INFO - Worker rq:worker:worker-58254 started with PID 58254, version 1.16.2
2024-10-28 15:30:23,144 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-58254
2024-10-28 15:30:23,145 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 15:30:23,153 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 15:30:23,155 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 15:30:23,156 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 15:30:25,332 - rq.scheduler - INFO - Scheduler for model_downloads, document_scans, file_uploads started with PID 58263
2024-10-28 15:30:28,203 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:30:28,236 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:30:33,809 - main - INFO - Scan job enqueued successfully. Job ID: ce4487be970460e33e29c262feaf7bfa
2024-10-28 15:30:33,814 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (ce4487be970460e33e29c262feaf7bfa)
2024-10-28 15:30:33,847 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:30:33,847 - worker - ERROR - Error in scan_document_task: name 'load_model' is not defined
2024-10-28 15:30:33,852 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 318, in scan_document_task
    model = load_model(llm_model_name, raise_http_exception=False)
NameError: name 'load_model' is not defined

2024-10-28 15:30:33,861 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (ce4487be970460e33e29c262feaf7bfa)
2024-10-28 15:30:33,861 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:31:19,858 - main - INFO - Application shutdown initiated
2024-10-28 15:31:22,660 - main - INFO - Starting application initialization
2024-10-28 15:31:22,661 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 15:31:22,665 - db - INFO - Database initialization completed.
2024-10-28 15:31:22,676 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011057 seconds, for an average of 0.00038127586206896557 seconds per hash.
2024-10-28 15:31:22,677 - utils - INFO - Checking models directory...
2024-10-28 15:31:22,677 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 15:31:22,677 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 15:31:22,677 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 15:31:22,677 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 15:31:22,677 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 15:31:22,677 - utils - INFO - Model downloads completed.
2024-10-28 15:31:22,683 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 15:31:22,684 - main - INFO - Application initialization complete
2024-10-28 15:31:24,497 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:31:24,497 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:31:24,539 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:31:24,542 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:31:24,542 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:31:24,587 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:31:26,728 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:31:26,743 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 15:31:31,854 - main - INFO - Scan job enqueued successfully. Job ID: f72f3d76aa9ca92c4a564520c6775ff7
2024-10-28 15:31:31,856 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (f72f3d76aa9ca92c4a564520c6775ff7)
2024-10-28 15:31:31,904 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:31:32,349 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (f72f3d76aa9ca92c4a564520c6775ff7)
2024-10-28 15:31:32,350 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:33:09,526 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:33:09,526 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:33:09,574 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:33:09,577 - main - INFO - Received request to retrieve all stored documents
2024-10-28 15:33:09,578 - main - INFO - Retrieving all stored documents from the database
2024-10-28 15:33:09,604 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 15:33:12,107 - main - INFO - Retrieving content for document with document_hash: 87820c4fa5799edf350b5f232677877e4366b2f278d10754e0c601f8af5198f9
2024-10-28 15:33:12,112 - main - INFO - Retrieved content for document with document_hash: 87820c4fa5799edf350b5f232677877e4366b2f278d10754e0c601f8af5198f9
2024-10-28 15:33:17,050 - main - INFO - Scan job enqueued successfully. Job ID: 473f065fdd5073c781c8c995bb81c262
2024-10-28 15:33:17,053 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('1 Ella Mason - A marine biologist known for her research on coral reef res..., 'nomic-embed-text-v1.5.Q6_K', 'mean', '0ad4c770d64581dccd53a1399b3a3393c27d8f0efb091408b4818991b32f6d07', 0.01, 10, 'spearman_rho')[39;49;00m (473f065fdd5073c781c8c995bb81c262)
2024-10-28 15:33:17,097 - worker - INFO - Starting scan task with parameters: query_text_length=3741, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 15:33:17,159 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (473f065fdd5073c781c8c995bb81c262)
2024-10-28 15:33:17,159 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 15:33:53,011 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 15:33:53,015 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 15:33:53,015 - main - INFO - Processed 1 semantic data types
2024-10-28 15:33:53,017 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 15:33:53,019 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 15:33:53,019 - main - INFO - Processed 1 semantic data types
2024-10-28 15:34:01,240 - main - ERROR - An error occurred while processing the request: get_or_compute_embedding() got an unexpected keyword argument 'request'
2024-10-28 15:34:01,241 - main - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/main.py", line 596, in create_new_semantic_data_type
    embedding_response = await get_or_compute_embedding(request=embedding_request, req=req, use_verbose=False)
TypeError: get_or_compute_embedding() got an unexpected keyword argument 'request'

2024-10-28 15:34:12,967 - rq.worker - INFO - Worker worker-58254 [PID 58254]: warm shut down requested
2024-10-28 15:34:12,968 - __main__ - INFO - Worker stopped by user
2024-10-28 15:34:13,773 - rq.scheduler - INFO - Scheduler stopping, releasing locks for model_downloads, document_scans, file_uploads...
2024-10-28 15:34:13,777 - rq.scheduler - INFO - Scheduler with PID 58263 has stopped
2024-10-28 15:34:13,891 - main - INFO - Application shutdown initiated
2024-10-28 15:34:14,301 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-58254
2024-10-28 21:04:51,793 - main - INFO - Starting application initialization
2024-10-28 21:04:51,802 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 21:04:51,807 - db - INFO - Database initialization completed.
2024-10-28 21:04:51,820 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012624 seconds, for an average of 0.0004353103448275862 seconds per hash.
2024-10-28 21:04:51,821 - utils - INFO - Checking models directory...
2024-10-28 21:04:51,821 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 21:04:51,821 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 21:04:51,822 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 21:04:51,822 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 21:04:51,822 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 21:04:51,822 - utils - INFO - Model downloads completed.
2024-10-28 21:04:51,828 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 21:04:51,830 - main - INFO - Application initialization complete
2024-10-28 21:05:14,551 - __main__ - INFO - Initializing worker...
2024-10-28 21:05:14,552 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 21:05:14,560 - __main__ - INFO - Worker started successfully with PID: 62105
2024-10-28 21:05:17,101 - __mp_main__ - WARNING - Failed to set memory limit: current limit exceeds maximum limit. Continuing without limit.
2024-10-28 21:05:17,101 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 21:05:17,103 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 21:05:17,120 - db - INFO - Database initialization completed.
2024-10-28 21:05:17,134 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.013453 seconds, for an average of 0.00046389655172413794 seconds per hash.
2024-10-28 21:05:17,134 - utils - INFO - Checking models directory...
2024-10-28 21:05:17,134 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 21:05:17,134 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 21:05:17,134 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 21:05:17,134 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 21:05:17,135 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 21:05:17,135 - utils - INFO - Model downloads completed.
2024-10-28 21:05:17,146 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 21:05:17,171 - rq.worker - INFO - Worker rq:worker:worker-62105 started with PID 62105, version 1.16.2
2024-10-28 21:05:17,171 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-62105
2024-10-28 21:05:17,173 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 21:05:17,181 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 21:05:17,183 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 21:05:17,184 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 21:05:19,646 - rq.scheduler - INFO - Scheduler for document_scans, model_downloads, file_uploads started with PID 62152
2024-10-28 21:05:55,538 - main - INFO - Received request to retrieve all stored documents
2024-10-28 21:05:55,538 - main - INFO - Retrieving all stored documents from the database
2024-10-28 21:05:55,553 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 21:05:55,557 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 21:05:55,558 - main - INFO - Processed 1 semantic data types
2024-10-28 21:05:55,620 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 21:06:02,317 - main - INFO - Received request to retrieve all stored documents
2024-10-28 21:06:02,318 - main - INFO - Retrieving all stored documents from the database
2024-10-28 21:06:02,354 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 21:06:02,356 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 21:06:02,357 - main - INFO - Processed 1 semantic data types
2024-10-28 21:06:02,390 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 21:06:10,775 - main - INFO - Received request to retrieve all stored documents
2024-10-28 21:06:10,776 - main - INFO - Retrieving all stored documents from the database
2024-10-28 21:06:10,806 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 21:06:10,809 - main - INFO - Received request to retrieve all stored documents
2024-10-28 21:06:10,809 - main - INFO - Retrieving all stored documents from the database
2024-10-28 21:06:10,833 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 21:06:16,040 - main - INFO - Retrieving content for document with document_hash: string
2024-10-28 21:06:16,050 - main - INFO - Retrieved content for document with document_hash: string
2024-10-28 21:06:21,088 - main - INFO - Scan job enqueued successfully. Job ID: 411afafe72d2255faa3c253c25490ab8
2024-10-28 21:06:21,093 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Inspird\nNon-Disclosure Agreement (NDA)\n\nTHIS AGREEMENT IS DATED: Octobe..., 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'spearman_rho')[39;49;00m (411afafe72d2255faa3c253c25490ab8)
2024-10-28 21:06:21,161 - worker - INFO - Starting scan task with parameters: query_text_length=18369, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 21:06:21,245 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (411afafe72d2255faa3c253c25490ab8)
2024-10-28 21:06:21,245 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 21:07:11,797 - main - INFO - Received request to retrieve all stored documents
2024-10-28 21:07:11,797 - main - INFO - Retrieving all stored documents from the database
2024-10-28 21:07:11,852 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 21:07:11,902 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 21:07:11,902 - main - INFO - Processed 1 semantic data types
2024-10-28 21:07:11,938 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 21:07:36,839 - main - INFO - Received request to retrieve all stored documents
2024-10-28 21:07:36,840 - main - INFO - Retrieving all stored documents from the database
2024-10-28 21:07:36,939 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 21:07:36,944 - main - INFO - Received request to retrieve all stored documents
2024-10-28 21:07:36,944 - main - INFO - Retrieving all stored documents from the database
2024-10-28 21:07:36,973 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 21:19:51,378 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 21:19:51,384 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 21:19:51,386 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 21:33:21,484 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 21:33:21,502 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 21:33:21,504 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 21:46:51,584 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 21:46:51,590 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 21:46:51,591 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 21:59:56,699 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 21:59:56,723 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:00:02,028 - main - INFO - Scan job enqueued successfully. Job ID: 5e5f9b4c362f7199f1f25ec07d231a4d
2024-10-28 22:00:02,036 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (5e5f9b4c362f7199f1f25ec07d231a4d)
2024-10-28 22:00:02,084 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:00:02,139 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (5e5f9b4c362f7199f1f25ec07d231a4d)
2024-10-28 22:00:02,139 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 22:00:02,146 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 22:00:02,148 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 22:00:02,149 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 22:12:22,947 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 22:12:22,961 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 22:12:22,962 - main - INFO - Processed 1 semantic data types
2024-10-28 22:12:22,966 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 22:12:22,968 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 22:12:22,968 - main - INFO - Processed 1 semantic data types
2024-10-28 22:12:54,179 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:12:54,180 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:12:54,209 - main - INFO - Retrieving all semantic data types from the database
2024-10-28 22:12:54,211 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-28 22:12:54,213 - main - INFO - Processed 1 semantic data types
2024-10-28 22:12:54,222 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:13:32,345 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 22:13:32,352 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 22:13:32,354 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 22:13:46,233 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:13:46,233 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:13:46,262 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:13:46,265 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:13:46,265 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:13:46,290 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:13:50,306 - main - INFO - Retrieving content for document with document_hash: string
2024-10-28 22:13:50,311 - main - INFO - Retrieved content for document with document_hash: string
2024-10-28 22:13:57,241 - main - INFO - Scan job enqueued successfully. Job ID: eeecebe720dd64d8f2a707f7e1f2014f
2024-10-28 22:13:57,241 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Inspird\nNon-Disclosure Agreement (NDA)\n\nTHIS AGREEMENT IS DATED: Octobe..., 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'spearman_rho')[39;49;00m (eeecebe720dd64d8f2a707f7e1f2014f)
2024-10-28 22:13:57,345 - worker - INFO - Starting scan task with parameters: query_text_length=18369, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:13:57,384 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (eeecebe720dd64d8f2a707f7e1f2014f)
2024-10-28 22:13:57,385 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 22:16:00,696 - main - INFO - Application shutdown initiated
2024-10-28 22:16:04,191 - main - INFO - Starting application initialization
2024-10-28 22:16:04,192 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:16:04,197 - db - INFO - Database initialization completed.
2024-10-28 22:16:04,208 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.010988 seconds, for an average of 0.00037889655172413793 seconds per hash.
2024-10-28 22:16:04,208 - utils - INFO - Checking models directory...
2024-10-28 22:16:04,208 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:16:04,209 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:16:04,209 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:16:04,209 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:16:04,209 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:16:04,209 - utils - INFO - Model downloads completed.
2024-10-28 22:16:04,214 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:16:04,216 - main - INFO - Application initialization complete
2024-10-28 22:16:15,327 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:16:15,327 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:16:15,372 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:16:15,374 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:16:15,374 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:16:15,398 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:16:20,288 - main - INFO - Retrieving content for document with document_hash: string
2024-10-28 22:16:20,296 - main - INFO - Retrieved content for document with document_hash: string
2024-10-28 22:16:24,894 - main - INFO - Scan job enqueued successfully. Job ID: a493d4cb4fede4b216bd35ea659ad38d
2024-10-28 22:16:24,896 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Inspird\nNon-Disclosure Agreement (NDA)\n\nTHIS AGREEMENT IS DATED: Octobe..., 'nomic-embed-text-v1.5.Q6_K', 'mean', '', 0.01, 10, 'spearman_rho')[39;49;00m (a493d4cb4fede4b216bd35ea659ad38d)
2024-10-28 22:16:24,928 - worker - INFO - Starting scan task with parameters: query_text_length=18369, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:16:24,929 - worker - ERROR - Error in scan_document_task: get_or_compute_embedding() got an unexpected keyword argument 'request'
2024-10-28 22:16:24,932 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 337, in scan_document_task
    embedding_response = await get_or_compute_embedding(request=embedding_request, use_verbose=False)
TypeError: get_or_compute_embedding() got an unexpected keyword argument 'request'

2024-10-28 22:16:24,935 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (a493d4cb4fede4b216bd35ea659ad38d)
2024-10-28 22:16:24,936 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 22:17:37,357 - main - INFO - Application shutdown initiated
2024-10-28 22:17:40,326 - main - INFO - Starting application initialization
2024-10-28 22:17:40,328 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:17:40,335 - db - INFO - Database initialization completed.
2024-10-28 22:17:40,353 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.017662 seconds, for an average of 0.0006090344827586207 seconds per hash.
2024-10-28 22:17:40,353 - utils - INFO - Checking models directory...
2024-10-28 22:17:40,353 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:17:40,354 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:17:40,354 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:17:40,354 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:17:40,354 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:17:40,354 - utils - INFO - Model downloads completed.
2024-10-28 22:17:40,361 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:17:40,363 - main - INFO - Application initialization complete
2024-10-28 22:17:41,353 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:17:41,353 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:17:41,412 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:17:41,415 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:17:41,415 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:17:41,440 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:17:43,876 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:17:43,890 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:17:48,019 - main - INFO - Scan job enqueued successfully. Job ID: 362ee0a6481bbc3fbbd022407aaa400e
2024-10-28 22:17:48,020 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (362ee0a6481bbc3fbbd022407aaa400e)
2024-10-28 22:17:48,049 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:17:48,080 - worker - ERROR - Error in scan_document_task: get_or_compute_embedding() got an unexpected keyword argument 'use_verbose'
2024-10-28 22:17:48,082 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 328, in scan_document_task
    embedding_response = await get_or_compute_embedding(embedding_request, use_verbose=False)
TypeError: get_or_compute_embedding() got an unexpected keyword argument 'use_verbose'

2024-10-28 22:17:48,091 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (362ee0a6481bbc3fbbd022407aaa400e)
2024-10-28 22:17:48,091 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 22:19:42,951 - main - INFO - Application shutdown initiated
2024-10-28 22:19:45,650 - main - INFO - Starting application initialization
2024-10-28 22:19:45,651 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:19:45,656 - db - INFO - Database initialization completed.
2024-10-28 22:19:45,668 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012056 seconds, for an average of 0.0004157241379310345 seconds per hash.
2024-10-28 22:19:45,669 - utils - INFO - Checking models directory...
2024-10-28 22:19:45,669 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:19:45,669 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:19:45,669 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:19:45,669 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:19:45,669 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:19:45,669 - utils - INFO - Model downloads completed.
2024-10-28 22:19:45,675 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:19:45,676 - main - INFO - Application initialization complete
2024-10-28 22:19:55,918 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:19:55,918 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:19:55,973 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:19:55,976 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:19:55,976 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:19:56,014 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:19:57,758 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:19:57,775 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:20:03,235 - main - INFO - Scan job enqueued successfully. Job ID: a692cd1099cdb180f2d272086c414795
2024-10-28 22:20:03,237 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (a692cd1099cdb180f2d272086c414795)
2024-10-28 22:20:03,274 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:20:03,305 - worker - ERROR - Error in scan_document_task: DatabaseWriter.__init__() missing 1 required positional argument: 'queue'
2024-10-28 22:20:03,308 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 330, in scan_document_task
    db_writer = DatabaseWriter()
TypeError: DatabaseWriter.__init__() missing 1 required positional argument: 'queue'

2024-10-28 22:20:03,316 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mJob OK[39;49;00m (a692cd1099cdb180f2d272086c414795)
2024-10-28 22:20:03,317 - rq.worker - INFO - Result is kept for 86400 seconds
2024-10-28 22:20:50,820 - main - INFO - Application shutdown initiated
2024-10-28 22:20:53,722 - main - INFO - Starting application initialization
2024-10-28 22:20:53,724 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:20:53,728 - db - INFO - Database initialization completed.
2024-10-28 22:20:53,739 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011145 seconds, for an average of 0.0003843103448275862 seconds per hash.
2024-10-28 22:20:53,740 - utils - INFO - Checking models directory...
2024-10-28 22:20:53,740 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:20:53,740 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:20:53,740 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:20:53,741 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:20:53,741 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:20:53,741 - utils - INFO - Model downloads completed.
2024-10-28 22:20:53,746 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:20:53,747 - main - INFO - Application initialization complete
2024-10-28 22:20:53,750 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:20:53,750 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:20:53,784 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:20:53,787 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:20:53,787 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:20:53,812 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:20:56,207 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:20:56,222 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:21:01,857 - main - INFO - Scan job enqueued successfully. Job ID: 3db8683e80c93b03604deee025904ae8
2024-10-28 22:21:01,859 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (3db8683e80c93b03604deee025904ae8)
2024-10-28 22:21:01,885 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:21:01,917 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:21:01,926 - db - INFO - Database initialization completed.
2024-10-28 22:21:01,927 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:21:01,931 - db - INFO - Database initialization completed.
2024-10-28 22:21:01,936 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.005161 seconds, for an average of 0.0001779655172413793 seconds per hash.
2024-10-28 22:21:01,937 - utils - INFO - Checking models directory...
2024-10-28 22:21:01,937 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:21:01,938 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:21:01,939 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:21:01,939 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:21:01,939 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:21:01,940 - utils - INFO - Model downloads completed.
2024-10-28 22:21:02,138 - rq.worker - WARNING - Moving job to FailedJobRegistry (Work-horse terminated unexpectedly; waitpid returned 11 (signal 11); )
2024-10-28 22:23:08,213 - main - INFO - Application shutdown initiated
2024-10-28 22:23:11,283 - main - INFO - Starting application initialization
2024-10-28 22:23:11,284 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:23:11,290 - db - INFO - Database initialization completed.
2024-10-28 22:23:11,302 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01184 seconds, for an average of 0.0004082758620689655 seconds per hash.
2024-10-28 22:23:11,302 - utils - INFO - Checking models directory...
2024-10-28 22:23:11,302 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:23:11,303 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:23:11,303 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:23:11,303 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:23:11,303 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:23:11,303 - utils - INFO - Model downloads completed.
2024-10-28 22:23:11,309 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:23:11,310 - main - INFO - Application initialization complete
2024-10-28 22:23:11,313 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:23:11,313 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:23:11,354 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:23:11,359 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:23:11,359 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:23:11,383 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:23:13,165 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:23:13,181 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:23:17,695 - main - INFO - Scan job enqueued successfully. Job ID: 6684417d015a7c4794815423fb18ad87
2024-10-28 22:23:17,697 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (6684417d015a7c4794815423fb18ad87)
2024-10-28 22:23:17,728 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:23:17,729 - worker - ERROR - Error in scan_document_task: 'DatabaseWriter' object has no attribute 'initialize'
2024-10-28 22:23:17,731 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 324, in scan_document_task
    await db_writer.initialize()
AttributeError: 'DatabaseWriter' object has no attribute 'initialize'

2024-10-28 22:23:17,742 - rq.worker - ERROR - [Job 6684417d015a7c4794815423fb18ad87]: exception raised while executing (worker.scan_document_task)
Traceback (most recent call last):
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/worker.py", line 1430, in perform_job
    rv = job.perform()
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/job.py", line 1280, in perform
    self._result = self._execute()
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/job.py", line 1320, in _execute
    coro_result = loop.run_until_complete(result)
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 435, in scan_document_task
    await db_writer.close()
AttributeError: 'DatabaseWriter' object has no attribute 'close'

2024-10-28 22:26:18,820 - main - INFO - Application shutdown initiated
2024-10-28 22:26:21,718 - main - INFO - Starting application initialization
2024-10-28 22:26:21,720 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:26:21,725 - db - INFO - Database initialization completed.
2024-10-28 22:26:21,736 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011822 seconds, for an average of 0.0004076551724137931 seconds per hash.
2024-10-28 22:26:21,737 - utils - INFO - Checking models directory...
2024-10-28 22:26:21,737 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:26:21,737 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:26:21,737 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:26:21,737 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:26:21,737 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:26:21,737 - utils - INFO - Model downloads completed.
2024-10-28 22:26:21,743 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:26:21,744 - main - INFO - Application initialization complete
2024-10-28 22:26:54,420 - main - INFO - Application shutdown initiated
2024-10-28 22:26:57,086 - main - INFO - Starting application initialization
2024-10-28 22:26:57,087 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:26:57,092 - db - INFO - Database initialization completed.
2024-10-28 22:26:57,103 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011332 seconds, for an average of 0.00039075862068965517 seconds per hash.
2024-10-28 22:26:57,103 - utils - INFO - Checking models directory...
2024-10-28 22:26:57,103 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:26:57,104 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:26:57,104 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:26:57,104 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:26:57,104 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:26:57,104 - utils - INFO - Model downloads completed.
2024-10-28 22:26:57,109 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:26:57,110 - main - INFO - Application initialization complete
2024-10-28 22:26:59,588 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:26:59,588 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:26:59,637 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:26:59,639 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:26:59,640 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:26:59,662 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:29:18,898 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:29:18,898 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:29:18,931 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:29:18,935 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:29:18,935 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:29:18,965 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:29:21,504 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:29:21,520 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-28 22:29:27,999 - main - INFO - Scan job enqueued successfully. Job ID: 8a5b6285f964276c41008a23106a2a28
2024-10-28 22:29:28,001 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (8a5b6285f964276c41008a23106a2a28)
2024-10-28 22:29:28,030 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:29:28,030 - worker - ERROR - Error in scan_document_task: 'DatabaseWriter' object has no attribute 'initialize'
2024-10-28 22:29:28,033 - worker - ERROR - Traceback (most recent call last):
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 324, in scan_document_task
    await db_writer.initialize()
AttributeError: 'DatabaseWriter' object has no attribute 'initialize'

2024-10-28 22:29:28,044 - rq.worker - ERROR - [Job 8a5b6285f964276c41008a23106a2a28]: exception raised while executing (worker.scan_document_task)
Traceback (most recent call last):
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/worker.py", line 1430, in perform_job
    rv = job.perform()
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/job.py", line 1280, in perform
    self._result = self._execute()
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/site-packages/rq/job.py", line 1320, in _execute
    coro_result = loop.run_until_complete(result)
  File "/Users/sidmohan/.pyenv/versions/3.10.13/lib/python3.10/asyncio/base_events.py", line 649, in run_until_complete
    return future.result()
  File "/Users/sidmohan/Desktop/codexify/backend/src/worker.py", line 435, in scan_document_task
    await db_writer.close()
AttributeError: 'DatabaseWriter' object has no attribute 'close'

2024-10-28 22:29:28,048 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 22:29:28,050 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 22:29:28,052 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 22:32:28,570 - main - INFO - Application shutdown initiated
2024-10-28 22:32:31,626 - main - INFO - Starting application initialization
2024-10-28 22:32:31,627 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:32:31,633 - db - INFO - Database initialization completed.
2024-10-28 22:32:31,645 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012278 seconds, for an average of 0.0004233793103448276 seconds per hash.
2024-10-28 22:32:31,646 - utils - INFO - Checking models directory...
2024-10-28 22:32:31,646 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:32:31,646 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:32:31,646 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:32:31,646 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:32:31,646 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:32:31,646 - utils - INFO - Model downloads completed.
2024-10-28 22:32:31,652 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:32:31,654 - main - INFO - Application initialization complete
2024-10-28 22:32:38,532 - main - INFO - Application shutdown initiated
2024-10-28 22:33:08,347 - main - INFO - Starting application initialization
2024-10-28 22:33:08,349 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:33:08,353 - db - INFO - Database initialization completed.
2024-10-28 22:33:08,365 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011754 seconds, for an average of 0.0004053103448275862 seconds per hash.
2024-10-28 22:33:08,366 - utils - INFO - Checking models directory...
2024-10-28 22:33:08,366 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:33:08,366 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:33:08,366 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:33:08,366 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:33:08,366 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:33:08,366 - utils - INFO - Model downloads completed.
2024-10-28 22:33:08,372 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:33:08,373 - main - INFO - Application initialization complete
2024-10-28 22:33:10,470 - rq.worker - INFO - Worker worker-62105 [PID 62105]: warm shut down requested
2024-10-28 22:33:10,476 - __main__ - INFO - Worker stopped by user
2024-10-28 22:33:10,509 - rq.scheduler - INFO - Scheduler stopping, releasing locks for document_scans, model_downloads, file_uploads...
2024-10-28 22:33:10,512 - rq.scheduler - INFO - Scheduler with PID 62152 has stopped
2024-10-28 22:33:10,950 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-62105
2024-10-28 22:33:16,127 - __main__ - INFO - Initializing worker...
2024-10-28 22:33:16,127 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-28 22:33:16,135 - __main__ - INFO - Worker started successfully with PID: 68380
2024-10-28 22:33:18,641 - __mp_main__ - WARNING - Failed to set memory limit: current limit exceeds maximum limit. Continuing without limit.
2024-10-28 22:33:18,641 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-28 22:33:18,649 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:33:18,658 - db - INFO - Database initialization completed.
2024-10-28 22:33:18,674 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01437 seconds, for an average of 0.0004955172413793104 seconds per hash.
2024-10-28 22:33:18,674 - utils - INFO - Checking models directory...
2024-10-28 22:33:18,674 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:33:18,674 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:33:18,674 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:33:18,675 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:33:18,675 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:33:18,675 - utils - INFO - Model downloads completed.
2024-10-28 22:33:18,681 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:33:18,686 - rq.worker - INFO - Worker rq:worker:worker-68380 started with PID 68380, version 1.16.2
2024-10-28 22:33:18,686 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-68380
2024-10-28 22:33:18,688 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-28 22:33:18,695 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 22:33:18,696 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 22:33:18,697 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 22:33:19,248 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:33:19,248 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:33:19,301 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:33:19,304 - main - INFO - Received request to retrieve all stored documents
2024-10-28 22:33:19,304 - main - INFO - Retrieving all stored documents from the database
2024-10-28 22:33:19,339 - main - INFO - Retrieved 8 stored documents from the database
2024-10-28 22:33:21,134 - rq.scheduler - INFO - Scheduler for file_uploads, model_downloads, document_scans started with PID 68391
2024-10-28 22:33:26,151 - main - INFO - Retrieving content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 22:33:26,158 - main - INFO - Retrieved content for document with document_hash: 0dba43f224b96d8e46c521810477b5bc6debb22fc44d803e804f215887615d71
2024-10-28 22:33:32,130 - main - INFO - Scan job enqueued successfully. Job ID: f6d15d53ce165c49d7343bddf09ee56f
2024-10-28 22:33:32,132 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task("RS\n\n\uf1b2 hnsw-0.11.0  ▼\n\n\uf085 Platform  ▼\n\n🏴 Feature ﬂags\n\nRus..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (f6d15d53ce165c49d7343bddf09ee56f)
2024-10-28 22:33:32,160 - worker - INFO - Starting scan task with parameters: query_text_length=8664, model=nomic-embed-text-v1.5.Q6_K
2024-10-28 22:33:32,193 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:33:32,201 - db - INFO - Database initialization completed.
2024-10-28 22:33:32,207 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.004721 seconds, for an average of 0.00016279310344827588 seconds per hash.
2024-10-28 22:33:32,207 - utils - INFO - Checking models directory...
2024-10-28 22:33:32,208 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:33:32,209 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:33:32,209 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:33:32,209 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:33:32,210 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:33:32,210 - utils - INFO - Model downloads completed.
2024-10-28 22:33:32,374 - rq.worker - WARNING - Moving job to FailedJobRegistry (Work-horse terminated unexpectedly; waitpid returned 11 (signal 11); )
2024-10-28 22:34:56,762 - main - INFO - Application shutdown initiated
2024-10-28 22:34:59,519 - main - INFO - Starting application initialization
2024-10-28 22:34:59,520 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-28 22:34:59,525 - db - INFO - Database initialization completed.
2024-10-28 22:34:59,537 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011514 seconds, for an average of 0.00039703448275862067 seconds per hash.
2024-10-28 22:34:59,537 - utils - INFO - Checking models directory...
2024-10-28 22:34:59,537 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-28 22:34:59,537 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-28 22:34:59,537 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-28 22:34:59,538 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-28 22:34:59,538 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-28 22:34:59,538 - utils - INFO - Model downloads completed.
2024-10-28 22:34:59,543 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-28 22:34:59,544 - main - INFO - Application initialization complete
2024-10-28 22:47:02,533 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 22:47:02,536 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 22:47:02,538 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 23:00:32,750 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 23:00:32,754 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 23:00:32,755 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 23:14:02,868 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 23:14:02,874 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 23:14:02,880 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 23:27:32,946 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 23:27:32,951 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 23:27:32,953 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 23:41:03,138 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 23:41:03,141 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 23:41:03,143 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-28 23:54:33,296 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-28 23:54:33,299 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-28 23:54:33,302 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 00:08:03,375 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 00:08:03,380 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 00:08:03,382 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 00:21:33,499 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 00:21:33,503 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 00:21:33,505 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 00:35:03,611 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 00:35:03,616 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 00:35:03,618 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 00:48:33,748 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 00:48:33,753 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 00:48:33,756 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 01:02:03,965 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 01:02:03,970 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 01:02:03,972 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 01:15:34,120 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 01:15:34,127 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 01:15:34,133 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 01:29:04,223 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 01:29:04,228 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 01:29:04,230 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 01:42:34,300 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 01:42:34,305 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 01:42:34,306 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 01:56:04,398 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 01:56:04,403 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 01:56:04,405 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 02:09:34,557 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 02:09:34,561 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 02:09:34,563 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 02:23:04,720 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 02:23:04,726 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 02:23:04,728 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 02:36:34,853 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 02:36:34,858 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 02:36:34,860 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 02:50:04,946 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 02:50:04,950 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 02:50:04,952 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 03:03:35,059 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 03:03:35,064 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 03:03:35,065 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 03:17:05,250 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 03:17:05,253 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 03:17:05,255 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 03:30:35,309 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 03:30:35,315 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 03:30:35,317 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 03:44:05,503 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 03:44:05,508 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 03:44:05,509 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 03:57:35,695 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 03:57:35,700 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 03:57:35,702 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 04:11:05,845 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 04:11:05,849 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 04:11:05,851 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 04:24:35,905 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 04:24:35,909 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 04:24:35,910 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 04:38:06,006 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 04:38:06,013 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 04:38:06,015 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 04:51:36,083 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 04:51:36,087 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 04:51:36,089 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 05:05:06,165 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 05:05:06,169 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 05:05:06,173 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 05:18:36,274 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 05:18:36,280 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 05:18:36,282 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 05:32:06,435 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 05:32:06,438 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 05:32:06,440 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 05:45:36,521 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 05:45:36,525 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 05:45:36,528 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 05:59:06,611 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 05:59:06,617 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 05:59:06,619 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 06:12:36,707 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 06:12:36,711 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 06:12:36,713 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 06:26:06,879 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 06:26:06,886 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 06:26:06,888 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 06:39:37,027 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 06:39:37,032 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 06:39:37,033 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 06:51:00,709 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 06:51:00,711 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 06:51:00,776 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 07:09:20,265 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 07:09:20,422 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 07:09:20,426 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 07:22:50,622 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 07:22:50,627 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 07:22:50,629 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 07:36:20,754 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 07:36:20,759 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 07:36:20,762 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 07:49:50,860 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 07:49:50,867 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 07:49:50,869 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 07:53:07,084 - main - INFO - Received request to retrieve all stored documents
2024-10-29 07:53:07,085 - main - INFO - Retrieving all stored documents from the database
2024-10-29 07:53:07,107 - main - INFO - Retrieving all semantic data types from the database
2024-10-29 07:53:07,113 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-29 07:53:07,114 - main - INFO - Processed 1 semantic data types
2024-10-29 07:53:07,189 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 07:56:06,719 - main - INFO - Application shutdown initiated
2024-10-29 07:56:10,297 - main - INFO - Starting application initialization
2024-10-29 07:56:10,298 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-29 07:56:10,304 - db - INFO - Database initialization completed.
2024-10-29 07:56:10,316 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.01271 seconds, for an average of 0.00043827586206896554 seconds per hash.
2024-10-29 07:56:10,317 - utils - INFO - Checking models directory...
2024-10-29 07:56:10,317 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-29 07:56:10,317 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-29 07:56:10,318 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-29 07:56:10,318 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-29 07:56:10,318 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-29 07:56:10,318 - utils - INFO - Model downloads completed.
2024-10-29 07:56:10,324 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-29 07:56:10,325 - main - INFO - Application initialization complete
2024-10-29 07:56:45,824 - main - INFO - Received request to retrieve all stored documents
2024-10-29 07:56:45,824 - main - INFO - Retrieving all stored documents from the database
2024-10-29 07:56:45,871 - main - INFO - Retrieving all semantic data types from the database
2024-10-29 07:56:45,878 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 07:56:45,880 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-29 07:56:45,881 - main - INFO - Processed 1 semantic data types
2024-10-29 08:03:21,038 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 08:03:21,045 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 08:03:21,048 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 08:10:23,298 - main - INFO - Received request to retrieve all stored documents
2024-10-29 08:10:23,301 - main - INFO - Retrieving all stored documents from the database
2024-10-29 08:10:23,419 - main - INFO - Retrieving all semantic data types from the database
2024-10-29 08:10:23,470 - main - INFO - Retrieved 1 semantic data types from the database
2024-10-29 08:10:23,473 - main - INFO - Processed 1 semantic data types
2024-10-29 08:10:23,535 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 08:10:43,087 - main - INFO - Received request to retrieve all stored documents
2024-10-29 08:10:43,088 - main - INFO - Retrieving all stored documents from the database
2024-10-29 08:10:43,131 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 08:10:43,134 - main - INFO - Received request to retrieve all stored documents
2024-10-29 08:10:43,134 - main - INFO - Retrieving all stored documents from the database
2024-10-29 08:10:43,158 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 08:10:45,807 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-29 08:10:45,825 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-29 08:10:53,501 - main - INFO - Scan job enqueued successfully. Job ID: be7bddf0e56c33846c75d8c26e721c7e
2024-10-29 08:10:53,505 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (be7bddf0e56c33846c75d8c26e721c7e)
2024-10-29 08:10:53,578 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-29 08:10:53,578 - worker - INFO - Loading model...
2024-10-29 08:10:53,630 - worker - INFO - Model loaded successfully
2024-10-29 08:10:53,630 - worker - INFO - Creating embedding request...
2024-10-29 08:10:53,631 - worker - INFO - Computing embedding...
2024-10-29 08:10:53,638 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-29 08:10:53,648 - db - INFO - Database initialization completed.
2024-10-29 08:10:53,654 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.005742 seconds, for an average of 0.000198 seconds per hash.
2024-10-29 08:10:53,655 - utils - INFO - Checking models directory...
2024-10-29 08:10:53,655 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-29 08:10:53,656 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-29 08:10:53,657 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-29 08:10:53,657 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-29 08:10:53,657 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-29 08:10:53,658 - utils - INFO - Model downloads completed.
2024-10-29 08:10:53,848 - rq.worker - WARNING - Moving job to FailedJobRegistry (Work-horse terminated unexpectedly; waitpid returned 11 (signal 11); )
2024-10-29 08:11:55,717 - rq.worker - INFO - Worker worker-68380 [PID 68380]: warm shut down requested
2024-10-29 08:11:55,718 - __main__ - INFO - Worker stopped by user
2024-10-29 08:11:56,120 - rq.scheduler - INFO - Scheduler stopping, releasing locks for file_uploads, model_downloads, document_scans...
2024-10-29 08:11:56,129 - rq.scheduler - INFO - Scheduler with PID 68391 has stopped
2024-10-29 08:11:56,664 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-68380
2024-10-29 08:12:09,406 - __main__ - INFO - Initializing worker...
2024-10-29 08:12:09,406 - __main__ - INFO - Worker listening to queues: ['model_downloads', 'file_uploads', 'document_scans']
2024-10-29 08:12:09,417 - __main__ - INFO - Worker started successfully with PID: 79318
2024-10-29 08:12:11,633 - __mp_main__ - WARNING - Failed to set memory limit: current limit exceeds maximum limit. Continuing without limit.
2024-10-29 08:12:11,633 - __mp_main__ - INFO - Connecting to Redis at localhost:6379
2024-10-29 08:12:11,635 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-29 08:12:11,641 - db - INFO - Database initialization completed.
2024-10-29 08:12:11,654 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.012247 seconds, for an average of 0.0004223103448275862 seconds per hash.
2024-10-29 08:12:11,654 - utils - INFO - Checking models directory...
2024-10-29 08:12:11,654 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-29 08:12:11,654 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-29 08:12:11,654 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-29 08:12:11,655 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-29 08:12:11,655 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-29 08:12:11,655 - utils - INFO - Model downloads completed.
2024-10-29 08:12:11,661 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-29 08:12:11,665 - rq.worker - INFO - Worker rq:worker:worker-79318 started with PID 79318, version 1.16.2
2024-10-29 08:12:11,665 - rq.worker - INFO - Subscribing to channel rq:pubsub:worker-79318
2024-10-29 08:12:11,666 - rq.worker - INFO - *** Listening on [32mmodel_downloads, file_uploads, document_scans[39;49;00m...
2024-10-29 08:12:11,672 - rq.worker - INFO - Cleaning registries for queue: model_downloads
2024-10-29 08:12:11,674 - rq.worker - INFO - Cleaning registries for queue: file_uploads
2024-10-29 08:12:11,675 - rq.worker - INFO - Cleaning registries for queue: document_scans
2024-10-29 08:12:13,986 - rq.scheduler - INFO - Scheduler for file_uploads, model_downloads, document_scans started with PID 79324
2024-10-29 08:12:32,617 - main - INFO - Received request to retrieve all stored documents
2024-10-29 08:12:32,618 - main - INFO - Retrieving all stored documents from the database
2024-10-29 08:12:32,684 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 08:12:32,687 - main - INFO - Received request to retrieve all stored documents
2024-10-29 08:12:32,687 - main - INFO - Retrieving all stored documents from the database
2024-10-29 08:12:32,710 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 08:12:34,932 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-29 08:12:34,942 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-29 08:12:39,168 - main - INFO - Scan job enqueued successfully. Job ID: bfd6cbdeb9bb469170afd4f0b78e7b43
2024-10-29 08:12:39,171 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (bfd6cbdeb9bb469170afd4f0b78e7b43)
2024-10-29 08:12:39,205 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-29 08:12:39,205 - worker - INFO - Loading model...
2024-10-29 08:12:39,236 - worker - INFO - Model loaded successfully
2024-10-29 08:12:39,237 - worker - INFO - Creating embedding request...
2024-10-29 08:12:39,238 - worker - INFO - Computing embedding...
2024-10-29 08:12:39,239 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-29 08:12:39,246 - db - INFO - Database initialization completed.
2024-10-29 08:12:39,252 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.004988 seconds, for an average of 0.000172 seconds per hash.
2024-10-29 08:12:39,253 - utils - INFO - Checking models directory...
2024-10-29 08:12:39,253 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-29 08:12:39,254 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-29 08:12:39,254 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-29 08:12:39,255 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-29 08:12:39,255 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-29 08:12:39,255 - utils - INFO - Model downloads completed.
2024-10-29 08:12:39,442 - rq.worker - WARNING - Moving job to FailedJobRegistry (Work-horse terminated unexpectedly; waitpid returned 11 (signal 11); )
2024-10-29 08:13:02,569 - main - INFO - Application shutdown initiated
2024-10-29 08:13:22,534 - main - INFO - Starting application initialization
2024-10-29 08:13:22,536 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-29 08:13:22,540 - db - INFO - Database initialization completed.
2024-10-29 08:13:22,552 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.011536 seconds, for an average of 0.00039779310344827584 seconds per hash.
2024-10-29 08:13:22,552 - utils - INFO - Checking models directory...
2024-10-29 08:13:22,552 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-29 08:13:22,553 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-29 08:13:22,553 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-29 08:13:22,553 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-29 08:13:22,553 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-29 08:13:22,553 - utils - INFO - Model downloads completed.
2024-10-29 08:13:22,559 - utils - INFO - Building Faiss index over embeddings for model nomic-embed-text-v1.5.Q6_K with pooling method mean...
2024-10-29 08:13:22,560 - main - INFO - Application initialization complete
2024-10-29 08:13:28,248 - main - INFO - Received request to retrieve all stored documents
2024-10-29 08:13:28,249 - main - INFO - Retrieving all stored documents from the database
2024-10-29 08:13:28,310 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 08:13:28,315 - main - INFO - Received request to retrieve all stored documents
2024-10-29 08:13:28,315 - main - INFO - Retrieving all stored documents from the database
2024-10-29 08:13:28,346 - main - INFO - Retrieved 8 stored documents from the database
2024-10-29 08:13:39,161 - main - INFO - Retrieving content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-29 08:13:39,175 - main - INFO - Retrieved content for document with document_hash: 53b0db0c450998a83a7e219c62ed812fd6d307803b932e3333e2d9ad745fee08
2024-10-29 08:13:44,766 - main - INFO - Scan job enqueued successfully. Job ID: 0af17e1fadfdcfb1e8698ef08fbee847
2024-10-29 08:13:44,768 - rq.worker - INFO - [32mdocument_scans[39;49;00m: [34mworker.scan_document_task('Sherlock: A Deep Learning Approach to\nSemantic Data Type Detection\n\nMad..., 'nomic-embed-text-v1.5.Q6_K', 'mean', 'default_codex', 0.01, 10, 'spearman_rho')[39;49;00m (0af17e1fadfdcfb1e8698ef08fbee847)
2024-10-29 08:13:44,796 - worker - INFO - Starting scan task with parameters: query_text_length=153599, model=nomic-embed-text-v1.5.Q6_K
2024-10-29 08:13:44,797 - worker - INFO - Loading model...
2024-10-29 08:13:44,828 - worker - INFO - Model loaded successfully
2024-10-29 08:13:44,828 - worker - INFO - Creating embedding request...
2024-10-29 08:13:44,829 - worker - INFO - Computing embedding...
2024-10-29 08:13:44,830 - db - INFO - Initializing database, creating tables, and setting SQLite PRAGMAs...
2024-10-29 08:13:44,836 - db - INFO - Database initialization completed.
2024-10-29 08:13:44,842 - db - INFO - Finished initializing set of input hash/llm_model_name combinations that are either currently being processed or have already been processed. Set size: 29; Took 0.005305 seconds, for an average of 0.0001829310344827586 seconds per hash.
2024-10-29 08:13:44,843 - utils - INFO - Checking models directory...
2024-10-29 08:13:44,843 - utils - INFO - Models directory exists: /Users/sidmohan/Desktop/codexify/backend/src/models
2024-10-29 08:13:44,844 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/nomic-embed-text-v1.5.Q6_K.gguf
2024-10-29 08:13:44,845 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/Hermes-3-Llama-3.1-8B.Q4_K_M.gguf
2024-10-29 08:13:44,845 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/bge-m3-q8_0.gguf
2024-10-29 08:13:44,845 - utils - INFO - File already exists: /Users/sidmohan/Desktop/codexify/backend/src/models/llava-llama-3-8b-v1_1-int4.gguf
2024-10-29 08:13:44,847 - utils - INFO - Model downloads completed.
2024-10-29 08:13:45,043 - rq.worker - WARNING - Moving job to FailedJobRegistry (Work-horse terminated unexpectedly; waitpid returned 11 (signal 11); )
2024-10-29 08:15:58,320 - rq.worker - INFO - Worker worker-79318 [PID 79318]: warm shut down requested
2024-10-29 08:15:58,320 - __main__ - INFO - Worker stopped by user
2024-10-29 08:15:58,440 - rq.scheduler - INFO - Scheduler stopping, releasing locks for file_uploads, model_downloads, document_scans...
2024-10-29 08:15:58,445 - rq.scheduler - INFO - Scheduler with PID 79324 has stopped
2024-10-29 08:15:58,906 - rq.worker - INFO - Unsubscribing from channel rq:pubsub:worker-79318
2024-10-29 08:15:59,382 - main - INFO - Application shutdown initiated
